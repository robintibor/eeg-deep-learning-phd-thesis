{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "847ddbb5",
   "metadata": {},
   "source": [
    "(understanding-pathology)=\n",
    "# Understanding Pathology Decoding With Invertible Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687fe09a-ac1e-48c0-a766-68df30393949",
   "metadata": {},
   "source": [
    "Move somewhere?\n",
    "We chose pathology decoding as  (i)n the question which learned features the networks uses to diagnose pathology seemed especially fascinating to us (ii) the TUH dataset is especially large."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a30cc7-1afa-431a-a469-d3940ee3e3d7",
   "metadata": {},
   "source": [
    "We then present the results of invertible networks trained to decode pathology from EEG, showing competitive results with regular deep convolutional networks. Our visualizations show the network uses slowing, i.e. increased amplitudes in the lower frequencies as well as spikes/burts as markers that indicate pathology and a strong stable alpha rhythm, especially on the posterior electrodes as markers for normal EEG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00edfd8-d828-4886-bf92-9d6feddcafa9",
   "metadata": {},
   "source": [
    "After our initial work on pathology decoding, we wanted to gain a deeper understanding of the features deep networks learn to distinguish healthy from pathological recordings. For that, we used invertible networks as generative classifiers since they offer more ways to visualize their learned prediction function in input space. We visualize prototypes of the two classes as well as individual electrode signals predictive of a certain class independent of the signals at other electrodes. These visualizations revealed both well-known features as well as surprising patterns in the very low frequencies. To gain an even better understanding, we distilled the invertible network's knowledge into a very small network that is interpretable by design. These visualizations showed regular patterns in the alpha and beta range associated with healthy recordings and a diverse set of more irregular waveforms associated with pathology. All work presented here is novel unpublished work performed in the context of this thesis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d1e411-f3f8-45a2-b5f0-cfa29b812f34",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset, Training Details and Performance of Invertible Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ff79b4-03bf-4efb-992d-822d92533e4e",
   "metadata": {},
   "source": [
    "```{table} Accuracy of Invertible Network in comparison with accuracies of regular ConvNets.\n",
    ":name: table-tuh-invertible-accuracy\n",
    "\n",
    "|Deep|Shallow|TCN|EEGNet|EEG-InvNet|\n",
    "|-|-|-|-|-|\n",
    "|84.6|84.1|86.2|83.4|85.5|\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e811b5b2-6926-42bd-8867-a18551ccedbb",
   "metadata": {
    "tags": []
   },
   "source": [
    "We apply our EEG-InvNet to pathology decoding on the same TUH dataset as in {ref}`pathology`. We use only 2 minutes of each recording at 64 Hz, and input 2 seconds as one example to the invertible network. This reduced dataset allows fast experimentation while still yielding good decoding performance. We used AdamW as our optimizer and cosine annealing with restarts every 25 epochs as our learning rate schedule.  We emphasize these details were not heavily optimized for maximum decoding performance, but rather chosen to obtain a robustly performing model worth investigating more deeply. Results in {numref}`table-tuh-invertible-accuracy` show that our EEG-InvNet compares similar than regular ConvNets, even better in some cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed8675a-2190-4581-a6a5-3b54d86509e8",
   "metadata": {},
   "source": [
    "## Class Prototypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85aa67ea-7c5f-4728-a838-1fe3f3889b6e",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "![title](images/net-disc-prototypes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8c4d2c-4d19-4b74-a661-764ba3a32e23",
   "metadata": {},
   "source": [
    "```{figure} images/net-disc-prototypes.png\n",
    "---\n",
    "name: disc-invnet-prototypes\n",
    "---\n",
    "Learned Class Prototypes from Invertible Network. Obtained by inverting learned means of class-conditional gaussian distributions from latent space to input space through the invertible network trained for pathology decoding.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5836707b-6118-47e1-9aa6-75dbc669dabb",
   "metadata": {},
   "source": [
    "Class prototypes reveal known oscillatory features and surprisingly hint at the use of very-low-frequency information by the invertible network. We inverted the learned latent means of the healthy and the pathological class distributions back to the input space to visualize the most likely healthy and most likely pathological examples under the learned distribution. We differences in the alpha rhythm like a stronger alpha rhythm at O1 in the healthy example. We also see further differences with a variety of different oscillatory patterns present for both classes. Surprisingly, there are also differences in the very low frequencies like substantially different mean values for FP1 and FP2 for the two class prototypes, which we will further investigate later. One challenge of this visualization is that one has to look at each prototype as one complete example and cannot interpret signals at individiual electrodes independently. This is what we tackle in our next visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89aaf8a-2abb-4257-90e3-298103924302",
   "metadata": {},
   "source": [
    "## Per-Channel Prototypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c954e27-bc6e-4c0e-b338-800aed14d8f9",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "![title](images/marginal-chan-6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1f1257-3a0e-4d67-8636-5471ec938b37",
   "metadata": {},
   "source": [
    "```{figure} images/marginal-chan-6.png\n",
    "---\n",
    "name: marginal-chan\n",
    "---\n",
    "Learned Per-Channel Prototypes from Invertible Network. Each channels' input is optimized independently to increase the invertible networks prediction for the respective class. During that optimization, signals for the other non-optimized channels are sampled from the training data.  Color indicates average softmax prediction over 10000 samples for the other channels. Very prominent slowing patterns appear for the pathological class at mjultiple electrodes.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe32aa2-5035-48da-b167-3d2be069acd2",
   "metadata": {},
   "source": [
    "The per-channel prototypes reveal interesting learned features for the two classes. The pathological prototypes show strong low-frequency activity, for example at T3 and T4, consistent with slowing as a biomarker for pathology. The healthy signal shows alpha activity, for example at C4 and T6.  Besides these patterns, a lot of other interesting patterns may be interesting to further investigate. One of them, the differences in the very low frequencies will be further explored below. Note that it was not possible to synthesize a signal that is clearly indicative of one class independent of the other electrodes for all electrodes. This is to be expected if the network may for example use the degree of synchrony between signals at different  electrodes as a feature. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd466e2-9049-4d8f-9ab1-65836cb0e83b",
   "metadata": {},
   "source": [
    "### EEG-CosNet Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263d0194-3867-414c-bbe9-0dc3bcb89d82",
   "metadata": {},
   "source": [
    "```{table} Accuracy of small interpretable network on invertible network predictions and original labels.\n",
    ":name: table-tuh-cos-net-accuracy\n",
    "\n",
    "||EEG-InvNet Predictions|Original Labels|\n",
    "|-|-|-|\n",
    "|Train|92.5|89.1|\n",
    "|Test|88.8|82.6|\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb311ba-4978-4d3f-9c21-8c7a09290bac",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "![cos-pattern](images/cos-sim-net-pattern-with-hspace.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcf9231-d1fd-4aed-84b2-edb8825a2531",
   "metadata": {},
   "source": [
    "```{figure} images/cos-sim-net-pattern-with-hspace.png\n",
    "---\n",
    "name: cos-sim-net-pattern-fig\n",
    "---\n",
    "Visualization of small interpretable EEG-CosNet trained to mimic the EEG-InvNet. Scalp Plots are spatial filter weights transformed to patterns, signals below each scalp plot show corresponding convolutional filter. Signal colors represent the weights of the linear classification layer, transformed to patterns (see TODO for explanation). Plots are sorted by these colors. Note that polarities of the scalp plots and temporal waveforms are arbitrary as absolute cosine similarities are computed on the spatially filtered and temporally convolved signals. \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c813472-c602-44dc-94e6-cbb307382ba1",
   "metadata": {},
   "source": [
    "Results for the EEG-CosNet show that a large fraction of the predictions of the invertible network can be predicted from a relatively small number of mostly neurophysiologically plausible spatio-temporal patterns. EEG-CosNet predicts 88.8% of the recordings in the same way as the EEG-InvNet and retains a test set label accuracy of 82.6% (see {numref}`table-tuh-cos-net-accuracy`. This shows that from just 64 spatiotemporal features, the EEG-CosNet is able to predict the vast majority of the EEG-InvNet predictions. Still, the remaining gap indicates that the EEG-InvNet has learned some features that the EEG-CosNet cannot represent.\n",
    "\n",
    "Visualizations in {numref}`cos-sim-net-pattern-fig` show more regular waveforms in the alpha and beta-frequency ranges with higher association for the healthy class and more waveforms in other frequency ranges and less regular waveforms with higher association for the pathological class. As examples for the healthy class, plots 1 and 3 show oscillations with a strong alpha component and plots 15-17 show oscillations with strong beta components. For the pathological class, we see slower oscillations, e.g., in plots 53 and 60, and also more irregular waveforms in, e.g., plots 49 and 52."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603c1e0c-70e6-4e14-98e7-08e2f36af309",
   "metadata": {},
   "source": [
    "## Investigation of Very Low Frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209f9464-2987-482f-9950-d4fd8035953b",
   "metadata": {},
   "source": [
    "One surprising observation from the visualizations are differences in the very low frequencies (<0.5 Hz) between the two class prototypes. For example, the very different mean values in the class prototypes for FP1 and FP2 suggest very low frequency information differs between the two classes on those electrodes. These kinds of differences motivated us to more deeply investigate in how far  very low frequency information is predictive of pathology."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5f7c35-44f9-4da3-8542-fe6f733819d4",
   "metadata": {},
   "source": [
    "```{table} Accuracy on data lowpassed below 0.5 H.\n",
    ":name: table-tuh-low-freq-accuracy\n",
    "\n",
    "|EEG-InvNet|EEG-CosNet|Fourier-GMM|\n",
    "|-|-|-|\n",
    "|75.4|75.0|75.4|\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b9dd48-169f-4358-b702-b368ae723fa1",
   "metadata": {},
   "source": [
    "For this, we first trained an invertible network on data lowpassed to be below 0.5 Hz via first removing all Fourier components above 0.5 Hz for both each recording and also each 2-second input window for the network. This retain 75.4% accuracy, indicating even these very low frequencies remain fairly informative about the pathologicality of the recording. We additionally trained the EEG_CosNet with a temporal filter spanning the entire input window length of 2 seconds and found it to retain 75% test accuracy. Finally, we also directly trained a 8-component gaussian mixture model Fourier-GMM in Fourier space as only 3 dimensions per electrode remain (summed value of the input window as well as real and imaginary value of the 0.5-Hz Fourier component). Each of the 8 mixture components also had learnable class weights. The Fourier-GMM also retains 75.4% test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039a328c-1fe1-4ce7-9930-4de1f596daf6",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "![cos-pattern](images/net-lowfreq-prototypes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8470cf7-104e-4a15-9ea5-e5a0adc02645",
   "metadata": {},
   "source": [
    "```{figure} images/net-lowfreq-prototypes.png\n",
    "---\n",
    "name: net-low-freq-prototypes-fig\n",
    "---\n",
    "Class prototypes for the EEG-InvNet trained on data lowpassed to be below 0.5 Hz.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c46f933-e7b5-4896-bc77-05e6af1ac00a",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "![cos-pattern](images/marginal-chan-low-freq.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568f8dc1-e8e0-4097-afbb-5ee578ebbb79",
   "metadata": {},
   "source": [
    "```{figure} images/marginal-chan-low-freq.png\n",
    "---\n",
    "name: marginal-chan-low-freq-fig\n",
    "---\n",
    "Per-Electrode Prototypes for EEG-InvNet trained on data lowpassed below 0.5 Hz. Note strongly predictive signals at T3,T4,T6.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68040f6-ac68-4571-a856-f4300156931e",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "![cos-pattern](images/cos-sim-net-low-freq-pattern-with-hspace.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4674001-682a-4b6f-a9f0-12c959213c4b",
   "metadata": {},
   "source": [
    "```{figure} images/cos-sim-net-low-freq-pattern-with-hspace.png\n",
    "---\n",
    "name: cos-sim-net-low-freq-pattern-fig\n",
    "---\n",
    "Spatiotemporal patterns for EEG-CosNet trained on lowpassed data below 0.5 Hz.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa4bb9c-ea41-443d-9f98-d5b903d5e210",
   "metadata": {},
   "source": [
    "::::{subfigure} ABCD|EFGH\n",
    ":gap: 0px\n",
    ":name: low-freq-input-space-prototypes-fig\n",
    ":class-grid: outline\n",
    ":subcaptions: below\n",
    "\n",
    ":::{image} images/low-freq-prototypes-0.png\n",
    ":::\n",
    "\n",
    ":::{image} images/low-freq-prototypes-1.png\n",
    ":::\n",
    "\n",
    ":::{image} images/low-freq-prototypes-2.png\n",
    ":::\n",
    "\n",
    ":::{image} images/low-freq-prototypes-3.png\n",
    ":::\n",
    "\n",
    ":::{image} images/low-freq-prototypes-4.png\n",
    ":::\n",
    "\n",
    ":::{image} images/low-freq-prototypes-5.png\n",
    ":::\n",
    "\n",
    ":::{image} images/low-freq-prototypes-6.png\n",
    ":::\n",
    "\n",
    ":::{image} images/low-freq-prototypes-7.png\n",
    ":::\n",
    "\n",
    "\n",
    "Means of the Fourier-GMM  mixture components shown after inversion into input space.\n",
    "::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005dabe4-071e-43d6-9039-2e229a86ba19",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "![cos-pattern](images/low-freq-gmm-prototypes-scaled-per-freq-with-class-color-and-bar.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f10a1c-2e90-43fb-8424-fa37f71ea98a",
   "metadata": {},
   "source": [
    "```{figure} images/low-freq-gmm-prototypes-scaled-per-freq-with-class-color-and-bar.png\n",
    "---\n",
    "name: fourier-gmm-low-freq-fig\n",
    "---\n",
    "Means of the Fourier-GMM mixture components in Fourier space. Scal plots for 0-Hz bin, real and imaginary values of 0.5-Hz bin. Components sorted by pathological class weight, also shown as colored text in top right of each component. Colormaps scaled per plot. Note strong frontal components for mixture components associated with healthy class.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9426507-7134-4bce-a88e-6efcb9ce39bc",
   "metadata": {},
   "source": [
    "Overall, class prototypes show A1 and A1 play a role, per-electrode prototypes show temporal electrodes are relevant, EEG-CosNet and GMM show importance of frontal electrodes. All together, one can hopefully understand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82600ab2-aa1c-45ed-9267-3f3987925c97",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2501bddc-58dd-449e-acf5-097a23301ebf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5c1c18-7ec6-4bb6-bec7-1b216ed3919c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e70d734-5547-4f74-b646-52c58bd4b59c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401c7685-4ab4-44d8-a061-97cae7974b67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4b7fef-876a-411c-ae39-01c16f4f0faa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2ceb97-4da8-4117-863e-4e7b8d142000",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e85ed6c-79ec-4cb6-9970-8e788c3ae865",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45bd947-13d4-4710-9bf9-f228d04bcb5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6031363-0124-4521-9cd5-3f63ad962c79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870c7ebe-189c-4c4e-a462-8c314e6fdb6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac89b892-57bd-4912-a491-eb7bfe11e846",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
