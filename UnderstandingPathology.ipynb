{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "847ddbb5",
   "metadata": {},
   "source": [
    "(understanding-pathology)=\n",
    "# Understanding Pathology Decoding With Invertible Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f7cda1-59c2-4027-ad22-041823dd6268",
   "metadata": {},
   "source": [
    "```{admonition} EEG-InvNet and EEG-CosNet can reveal learned features for EEG pathology decoding\n",
    "* EEG-InvNet can reach 85.5% accuracy, competitive with regular ConvNets\n",
    "* Visualizations show networks learn well-known features like temporal slowing or occipital alpha\n",
    "* Visualizations also reveal surprising learned features in the very low frequencies up to 0.5 Hz\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00edfd8-d828-4886-bf92-9d6feddcafa9",
   "metadata": {},
   "source": [
    "After our initial work on pathology decoding, we wanted to gain a deeper understanding of the features deep networks learn to distinguish healthy from pathological recordings. For that, we used invertible networks as generative classifiers since they offer more ways to visualize their learned prediction function in input space. Our EEG-InvNet reached competitive accuracies on the pathology decoding task. We visualize prototypes of the two classes as well as individual electrode signals predictive of a certain class independent of the signals at other electrodes. These visualizations revealed both well-known features like temporal slowing or occipital alpha as well as surprising patterns in the very low frequencies (<= 0.5 Hz). To gain an even better understanding, we distilled the invertible network's knowledge into a very small network called EEG-CosNet that is interpretable by design. These visualizations showed regular patterns in the alpha and beta range associated with healthy recordings and a diverse set of more irregular waveforms associated with pathology. For the very low frequencies, visualizations revealed a frontal component predicting the healthy class and other components with spatial topographies including the temporal areas predicting the pathological class.\n",
    "\n",
    "All work presented in this chapter is novel unpublished work performed by me in the context of this thesis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d1e411-f3f8-45a2-b5f0-cfa29b812f34",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset, Training Details and Decoding Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ff79b4-03bf-4efb-992d-822d92533e4e",
   "metadata": {},
   "source": [
    "```{table} Accuracy of Invertible Network in comparison with accuracies of regular ConvNets.\n",
    ":name: table-tuh-invertible-accuracy\n",
    "\n",
    "|Deep|Shallow|TCN|EEGNet|EEG-InvNet|\n",
    "|-|-|-|-|-|\n",
    "|84.6|84.1|86.2|83.4|85.5|\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e811b5b2-6926-42bd-8867-a18551ccedbb",
   "metadata": {
    "tags": []
   },
   "source": [
    "We apply our EEG-InvNet to pathology decoding on the same TUH dataset as in {ref}`pathology`. We use only 2 minutes of each recording at 64 Hz, and input 2 seconds as one example to the invertible network. This reduced dataset allows fast experimentation while still yielding good decoding performance. We used AdamW {cite:p}`DBLP:conf/iclr/LoshchilovH19` as our optimizer and cosine annealing with restarts {cite:p}`DBLP:conf/iclr/LoshchilovH17` every 25 epochs as our learning rate schedule.  We emphasize these details were not heavily optimized for maximum decoding performance, but rather chosen to obtain a robustly performing model worth investigating more deeply. Results in {numref}`table-tuh-invertible-accuracy` show that our EEG-InvNet compares similar than regular ConvNets, even better than some ConvNets, therefore motivating a deeper investigation into its learned features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed8675a-2190-4581-a6a5-3b54d86509e8",
   "metadata": {},
   "source": [
    "## Class Prototypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85aa67ea-7c5f-4728-a838-1fe3f3889b6e",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "![title](images/net-disc-prototypes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8c4d2c-4d19-4b74-a661-764ba3a32e23",
   "metadata": {},
   "source": [
    "```{figure} images/net-disc-prototypes.png\n",
    "---\n",
    "name: disc-invnet-prototypes\n",
    "---\n",
    "**Learned class prototypes from EEG-InvNet.** Obtained by inverting learned means of class-conditional gaussian distributions from latent space to input space through the invertible network trained for pathology decoding.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5836707b-6118-47e1-9aa6-75dbc669dabb",
   "metadata": {},
   "source": [
    "Class prototypes reveal known oscillatory features and surprisingly hint at the use of very-low-frequency information by the invertible network. We inverted the learned latent means of the healthy and the pathological class distributions back to the input space to visualize the most likely healthy and most likely pathological examples under the learned distribution, see also {ref}`methods-class-prototypes`. Visualizations in {numref}`disc-invnet-prototypes` show differences in the alpha rhythm like a stronger alpha rhythm at O1 in the healthy example. We also see further differences with a variety of different oscillatory patterns present for both classes. Surprisingly, there are also differences in the very low frequencies like substantially different mean values for FP1 and FP2 for the two class prototypes, which we will further investigate later. One challenge of this visualization is that one has to look at each prototype as one complete example and cannot interpret signals at individiual electrodes independently. This is what we tackle in our next visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89aaf8a-2abb-4257-90e3-298103924302",
   "metadata": {},
   "source": [
    "## Per-Channel Prototypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c954e27-bc6e-4c0e-b338-800aed14d8f9",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "![title](images/marginal-chan-6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1f1257-3a0e-4d67-8636-5471ec938b37",
   "metadata": {},
   "source": [
    "```{figure} images/marginal-chan-6.png\n",
    "---\n",
    "name: marginal-chan\n",
    "---\n",
    "**Learned per-channel prototypes from EEG-InvNet.** Each channels' input is optimized independently to increase the invertible networks prediction for the respective class. During that optimization, signals for the other non-optimized channels are sampled from the training data.  Color indicates average softmax prediction over 10000 samples for the other channels. Very prominent slowing patterns appear for the pathological class at mjultiple electrodes.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe32aa2-5035-48da-b167-3d2be069acd2",
   "metadata": {},
   "source": [
    "The per-channel prototypes reveal interesting learned features for the two classes (see {numref}`marginal-chan`). The pathological prototypes show strong low-frequency activity, for example at T3 and T4, consistent with slowing as a biomarker for pathology. The healthy signal shows alpha activity, for example at C4 and T6.  Besides these patterns, a lot of other interesting patterns may be interesting to further investigate. One of them, the differences in the very low frequencies will be further explored below. Note that it was not possible to synthesize a signal that is clearly indicative of one class independent of the other electrodes for all electrodes. This is to be expected if the EEG-InvNet uses a feature inherently impossible to recreate within a single electrode like the degree of synchrony between signals at different  electrodes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd466e2-9049-4d8f-9ab1-65836cb0e83b",
   "metadata": {},
   "source": [
    "## EEG-CosNet Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263d0194-3867-414c-bbe9-0dc3bcb89d82",
   "metadata": {},
   "source": [
    "```{table} **Accuracy of small interpretable network on invertible network predictions and original labels.**\n",
    ":name: table-tuh-cos-net-accuracy\n",
    "\n",
    "||EEG-InvNet Predictions|Original Labels|\n",
    "|-|-|-|\n",
    "|Train|92.5|89.1|\n",
    "|Test|88.8|82.6|\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb311ba-4978-4d3f-9c21-8c7a09290bac",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "![cos-pattern](images/cos-sim-net-pattern-with-hspace.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcf9231-d1fd-4aed-84b2-edb8825a2531",
   "metadata": {},
   "source": [
    "```{figure} images/cos-sim-net-pattern-with-hspace.png\n",
    "---\n",
    "name: cos-sim-net-pattern-fig\n",
    "---\n",
    "**Visualization of small interpretable EEG-CosNet trained to mimic the EEG-InvNet.** Scalp Plots are spatial filter weights transformed to patterns, signals below each scalp plot show corresponding convolutional filter. Signal colors represent the weights of the linear classification layer, transformed to patterns (see {ref}`methods-eeg-cosnet` for an explanation). Plots are sorted by these colors. Note that polarities of the scalp plots and temporal waveforms are arbitrary as absolute cosine similarities are computed on the spatially filtered and temporally convolved signals. \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c813472-c602-44dc-94e6-cbb307382ba1",
   "metadata": {},
   "source": [
    "Results for the EEG-CosNet show that a large fraction of the predictions of the invertible network can be predicted from a relatively small number of mostly neurophysiologically plausible spatio-temporal patterns. EEG-CosNet predicts 88.8% of the recordings in the same way as the EEG-InvNet and retains a test set label accuracy of 82.6% (see {numref}`table-tuh-cos-net-accuracy`. This shows that from just 64 spatiotemporal features, the EEG-CosNet is able to predict the vast majority of the EEG-InvNet predictions. Still, the remaining gap indicates that the EEG-InvNet has learned some features that the EEG-CosNet cannot represent.\n",
    "\n",
    "Visualizations in {numref}`cos-sim-net-pattern-fig` show more regular waveforms in the alpha and beta-frequency ranges with higher association for the healthy class and more waveforms in other frequency ranges as well as less regular waveforms with higher association for the pathological class. As examples for the healthy class, plots 1 and 3 show oscillations with a strong alpha component and plots 15-17 show oscillations with strong beta components. For the pathological class, we see slower oscillations, e.g., in plots 53 and 60, and also more irregular waveforms in, e.g., plots 49 and 52."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603c1e0c-70e6-4e14-98e7-08e2f36af309",
   "metadata": {},
   "source": [
    "## Investigation of Very Low Frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209f9464-2987-482f-9950-d4fd8035953b",
   "metadata": {},
   "source": [
    "One surprising observation from the visualizations are differences in the very low frequencies (<=0.5 Hz) between the two class prototypes. For example, the very different mean values in the class prototypes for FP1 and FP2 suggest very low frequency information differs between the two classes on those electrodes. These kinds of differences motivated us to more deeply investigate in how far  very low frequency information is predictive of pathology."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5f7c35-44f9-4da3-8542-fe6f733819d4",
   "metadata": {},
   "source": [
    "```{table} **Accuracy on data lowpassed below 0.5 Hz.**\n",
    ":name: table-tuh-low-freq-accuracy\n",
    "\n",
    "|EEG-InvNet|EEG-CosNet|Fourier-GMM|\n",
    "|-|-|-|\n",
    "|75.4|75.0|75.4|\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b9dd48-169f-4358-b702-b368ae723fa1",
   "metadata": {},
   "source": [
    "For this, we trained an EEG-InvNet on data lowpassed to be below 0.5 Hz. For the lowpass, we first removed all Fourier components above 0.5 Hz for each recording and also for each 2-second input window for the network. This retained 75.4% accuracy with the EEG-InvNet, indicating even these very low frequencies remain fairly informative about the pathologicality of the recording. We additionally trained the EEG-CosNet with a temporal filter spanning the entire input window length of 2 seconds and found it to retain 75% test accuracy. Finally, we also directly trained a 8-component gaussian mixture model Fourier-GMM in Fourier space. Only 3 dimensions per electrode remain: real value of the 0-Hz component ( summed values of the input window) and real and imaginary value of the 0.5-Hz Fourier component). Each of the 8 mixture components had learnable class weights, how much each mixture component contributed to that classes learned distribution. The Fourier-GMM also retains 75.4% test accuracy. All results are shown in {numref}`table-tuh-low-freq-accuracy`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0c6f46-7e0b-4ec9-944b-c8d090f3ebc9",
   "metadata": {},
   "source": [
    "### EEG-InvNet Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039a328c-1fe1-4ce7-9930-4de1f596daf6",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "![cos-pattern](images/net-lowfreq-prototypes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8470cf7-104e-4a15-9ea5-e5a0adc02645",
   "metadata": {},
   "source": [
    "```{figure} images/net-lowfreq-prototypes.png\n",
    "---\n",
    "name: net-low-freq-prototypes-fig\n",
    "---\n",
    "**Class prototypes for the EEG-InvNet trained on data lowpassed to be below 0.5 Hz.** Note large differences at A1 and A2.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c46f933-e7b5-4896-bc77-05e6af1ac00a",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "![cos-pattern](images/marginal-chan-low-freq.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568f8dc1-e8e0-4097-afbb-5ee578ebbb79",
   "metadata": {},
   "source": [
    "```{figure} images/marginal-chan-low-freq.png\n",
    "---\n",
    "name: marginal-chan-low-freq-fig\n",
    "---\n",
    "**Per-electrode prototypes for EEG-InvNet trained on data lowpassed below 0.5 Hz.** Note strongly predictive signals at T3,T4,T6.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f36ff31-01cf-4dbc-84a2-e25262f24c54",
   "metadata": {},
   "source": [
    "The visualizations of the EEG-InvNet show several interesting features. The class prototypes in {numref}`net-low-freq-prototypes-fig` show differences at most electrodes, especially pronounced for A1 and A2. The per-electrode prototypes in   {numref}`net-low-freq-prototypes-fig` show predictive information in the T3,T4 and T6 electrodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c2564d-5f28-4919-831c-fbbddb02792a",
   "metadata": {},
   "source": [
    "### EEG-CosNet Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68040f6-ac68-4571-a856-f4300156931e",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "![cos-pattern](images/cos-sim-net-low-freq-pattern-with-hspace.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4674001-682a-4b6f-a9f0-12c959213c4b",
   "metadata": {},
   "source": [
    "```{figure} images/cos-sim-net-low-freq-pattern-with-hspace.png\n",
    "---\n",
    "name: cos-sim-net-low-freq-pattern-fig\n",
    "---\n",
    "**Spatiotemporal patterns for EEG-CosNet trained on lowpassed data below 0.5 Hz.** Note large frontal components associated with healthy class.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d593c734-aede-46ae-9ca7-dd647d9da021",
   "metadata": {},
   "source": [
    "Visualization of the EEG-CosNet in {numref}`cos-sim-net-low-freq-pattern-fig` contain strong frontally components associated with the healthy class and  components in temporal areas associated with the pathological class. The temporal components are in line with the per-electrode visualization, and the frontal components were already visible as differences in mean signal values in the class prototypes on the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6347b16c-bcb4-4d76-86b4-241665e16ae5",
   "metadata": {},
   "source": [
    "### Fourier-GMM Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa4bb9c-ea41-443d-9f98-d5b903d5e210",
   "metadata": {},
   "source": [
    "::::{subfigure} ABCD|EFGH\n",
    ":gap: 0px\n",
    ":name: low-freq-input-space-prototypes-fig\n",
    ":class-grid: outline\n",
    ":subcaptions: below\n",
    "\n",
    ":::{image} images/low-freq-prototypes-0.png\n",
    ":::\n",
    "\n",
    ":::{image} images/low-freq-prototypes-1.png\n",
    ":::\n",
    "\n",
    ":::{image} images/low-freq-prototypes-2.png\n",
    ":::\n",
    "\n",
    ":::{image} images/low-freq-prototypes-3.png\n",
    ":::\n",
    "\n",
    ":::{image} images/low-freq-prototypes-4.png\n",
    ":::\n",
    "\n",
    ":::{image} images/low-freq-prototypes-5.png\n",
    ":::\n",
    "\n",
    ":::{image} images/low-freq-prototypes-6.png\n",
    ":::\n",
    "\n",
    ":::{image} images/low-freq-prototypes-7.png\n",
    ":::\n",
    "\n",
    "\n",
    "**Means of the Fourier-GMM  mixture components shown after inversion into input space.** Note clearly visible frontal signals in the components for the healthy class.\n",
    "::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc346276-2d81-4293-9660-05655b1099b8",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "![cos-pattern](images/low-freq-gmm-prototypes-scaled-per-freq-with-class-color-and-bar.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f10a1c-2e90-43fb-8424-fa37f71ea98a",
   "metadata": {},
   "source": [
    "```{figure} images/low-freq-gmm-prototypes-scaled-per-freq-with-class-color-and-bar.png\n",
    "---\n",
    "name: fourier-gmm-low-freq-fig\n",
    "---\n",
    "**Means of the Fourier-GMM mixture components in Fourier space.** Scalp plots for 0-Hz bin, real and imaginary values of 0.5-Hz bin. Components sorted by pathological class weight, also shown as colored text in top right of each component. Colormaps scaled per frequency bin. Note strong frontal components for mixture components associated with healthy class.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7eb814-7db7-4925-acdd-a1869e0ac04a",
   "metadata": {},
   "source": [
    "Visualizations of the Fourier-GMM in {numref}`fourier-gmm-low-freq-fig` again show frontal components associated with the healthy class and other components, with spatial topographies that include temporal areas, associated with the pathological class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82600ab2-aa1c-45ed-9267-3f3987925c97",
   "metadata": {},
   "source": [
    "Overall, the visualizations consistently indicate a frontal component predictive for the healthy class and other components, with a spatial topography that often includes temporal and nearby areas, predictive for the pathological class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a9847f-2a1b-45ec-8624-2fe5c1d26d06",
   "metadata": {},
   "source": [
    "```{admonition}  Open Questions\n",
    ":class: tip\n",
    "* What other methods may in the future help understand discriminative features in the EEG signal?\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
