
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Deep Learning for Brain-Signal Decoding from Electroencephalography (EEG)</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script async="async" kind="hypothesis" src="https://hypothes.is/embed.js"></script>
    <script kind="utterances">

    var commentsRunWhenDOMLoaded = cb => {
    if (document.readyState != 'loading') {
        cb()
    } else if (document.addEventListener) {
        document.addEventListener('DOMContentLoaded', cb)
    } else {
        document.attachEvent('onreadystatechange', function() {
        if (document.readyState == 'complete') cb()
        })
    }
}

var addUtterances = () => {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src = "https://utteranc.es/client.js";
    script.async = "async";

    script.setAttribute("repo", "robintibor/eeg-deep-learning-phd-thesis");
    script.setAttribute("issue-term", "pathname");
    script.setAttribute("theme", "github-light");
    script.setAttribute("label", "💬 comment");
    script.setAttribute("crossorigin", "anonymous");

    sections = document.querySelectorAll("div.section");
    if (sections !== null) {
        section = sections[sections.length-1];
        section.appendChild(script);
    }
}
commentsRunWhenDOMLoaded(addUtterances);
</script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint single-page" id="site-navigation">
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/Abstract.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toctree-l1 toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="Abstract.html#document-Introduction">
   Introduction
  </a>
 </li>
 <li class="toctree-l1 toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="Abstract.html#document-PriorWork">
   Prior Work
  </a>
 </li>
 <li class="toctree-l1 toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="Abstract.html#document-FBCSPAndFBCSPNet">
   Filterbank Common Spatial Patterns and Filterbank Network
  </a>
 </li>
 <li class="toctree-l1 toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="Abstract.html#document-DeepArchitectures">
   Neural Network Architectures for EEG-Decoding
  </a>
 </li>
 <li class="toctree-l1 toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="Abstract.html#document-CroppedTraining">
   Cropped Training
  </a>
 </li>
 <li class="toctree-l1 toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="Abstract.html#document-PerturbationVisualization">
   Perturbation Visualization
  </a>
 </li>
 <li class="toctree-l1 toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="Abstract.html#document-MovementDecoding">
   Decoding Movement-Related Brain Activity
  </a>
 </li>
 <li class="toctree-l1 toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="Abstract.html#document-TaskDecoding">
   Generalization to Other Tasks
  </a>
 </li>
 <li class="toctree-l1 toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="Abstract.html#document-Pathology">
   Decoding Pathology
  </a>
 </li>
 <li class="toctree-l1 toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="Abstract.html#document-Invertible">
   Invertible Networks
  </a>
 </li>
 <li class="toctree-l1 toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="Abstract.html#document-FutureWork">
   Future Work
  </a>
 </li>
 <li class="toctree-l1 toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="Abstract.html#document-References">
   References
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Deep Learning for Brain-Signal Decoding from Electroencephalography</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toctree-l1 toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="Abstract.html#document-Introduction">
   Introduction
  </a>
 </li>
 <li class="toctree-l1 toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="Abstract.html#document-PriorWork">
   Prior Work
  </a>
 </li>
 <li class="toctree-l1 toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="Abstract.html#document-FBCSPAndFBCSPNet">
   Filterbank Common Spatial Patterns and Filterbank Network
  </a>
 </li>
 <li class="toctree-l1 toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="Abstract.html#document-DeepArchitectures">
   Neural Network Architectures for EEG-Decoding
  </a>
 </li>
 <li class="toctree-l1 toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="Abstract.html#document-CroppedTraining">
   Cropped Training
  </a>
 </li>
 <li class="toctree-l1 toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="Abstract.html#document-PerturbationVisualization">
   Perturbation Visualization
  </a>
 </li>
 <li class="toctree-l1 toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="Abstract.html#document-MovementDecoding">
   Decoding Movement-Related Brain Activity
  </a>
 </li>
 <li class="toctree-l1 toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="Abstract.html#document-TaskDecoding">
   Generalization to Other Tasks
  </a>
 </li>
 <li class="toctree-l1 toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="Abstract.html#document-Pathology">
   Decoding Pathology
  </a>
 </li>
 <li class="toctree-l1 toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="Abstract.html#document-Invertible">
   Invertible Networks
  </a>
 </li>
 <li class="toctree-l1 toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="Abstract.html#document-FutureWork">
   Future Work
  </a>
 </li>
 <li class="toctree-l1 toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="Abstract.html#document-References">
   References
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="deep-learning-for-brain-signal-decoding-from-electroencephalography">
<h1>Deep Learning for Brain-Signal Decoding from Electroencephalography<a class="headerlink" href="#deep-learning-for-brain-signal-decoding-from-electroencephalography" title="Permalink to this headline">#</a></h1>
<p>Machine learning has the potential to improve medical applications by processing larger amounts of data and extracting different information from it than medical doctors. In particular, brain-signal decoding from electroencephalographic (EEG) recordings is a promising area for machine learning due to the relative ease of acquiring large amounts of EEG recordings and the difficulty of interpreting them manually. Deep neural networks (DNNs) have been successful at a variety of natural-signal decoding tasks like object recognition from images or speech recognition from audio and thus may be well-suited for decoding EEG signals. However, prior to the work in this thesis, it was still unclear how well DNNs perform on EEG decoding compared to hand-engineered, feature-based approaches, and more research was needed to determine the optimal approaches for using deep learning in this context. This thesis describes constructing and training EEG-decoding deep learning networks that perform as well as feature-based approaches and developing visualizations that suggest they extract physiologically meaningful features.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="text-align:left head"><p>Chapter</p></th>
<th class="text-align:right head"><p>Summary</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:left"><p><strong>Introduction and Background</strong></p></td>
<td class="text-align:right"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p><a class="reference internal" href="Abstract.html#introduction"><span class="std std-ref">Introduction</span></a></p></td>
<td class="text-align:right"><p>Deep learning on EEG is a very promising approach for brain-signal-based medical applications</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p><a class="reference internal" href="Abstract.html#prior-work"><span class="std std-ref">Prior Work</span></a></p></td>
<td class="text-align:right"><p>Prior to 2017, research did not clearly show how competitive deep learning is compared with well-optimized feature baselines</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p><a class="reference internal" href="Abstract.html#fbscp-and-filterbank-net"><span class="std std-ref">Filterbank Common Spatial Patterns and Filterbank Network</span></a></p></td>
<td class="text-align:right"><p>Filter Bank Common Spatial Patterns was as an inspiration for initial network architectures</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p><strong>Methods</strong></p></td>
<td class="text-align:right"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p><a class="reference internal" href="Abstract.html#network-architectures"><span class="std std-ref">Neural Network Architectures for EEG-Decoding</span></a></p></td>
<td class="text-align:right"><p>Progressively more generic neural network architectures for EEG decoding were created</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p><a class="reference internal" href="Abstract.html#cropped-training"><span class="std std-ref">Cropped Training</span></a></p></td>
<td class="text-align:right"><p>A training strategy to use many sliding windows was implemented in a computationally efficient manner</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p><a class="reference internal" href="Abstract.html#perturbation-visualization"><span class="std std-ref">Perturbation Visualization</span></a></p></td>
<td class="text-align:right"><p>A visualization of how frequency features affect the trained network was developed</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p><strong>Applications and Results</strong></p></td>
<td class="text-align:right"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p><a class="reference internal" href="Abstract.html#movement-related"><span class="std std-ref">Movement-Related Decoding</span></a></p></td>
<td class="text-align:right"><p>Deep learning can be at least as good as feature-based baselines for movement-related decoding; deep networks also learn to extract known hand-engineered features</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p><a class="reference internal" href="Abstract.html#task-related"><span class="std std-ref">Generalization to Other Tasks</span></a></p></td>
<td class="text-align:right"><p>Our deep networks generalize well to decoding other decoding tasks</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p><a class="reference internal" href="Abstract.html#pathology"><span class="std std-ref">Decoding Pathology</span></a></p></td>
<td class="text-align:right"><p>Deep networks designed for task-related decoding can also decode pathology well</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p><a class="reference internal" href="Abstract.html#invertible-networks"><span class="std std-ref">Invertible Networks</span></a></p></td>
<td class="text-align:right"><p>Better Understanding and larger data through invertible networks</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p><a class="reference internal" href="Abstract.html#future-work"><span class="std std-ref">Future Work</span></a></p></td>
<td class="text-align:right"><p>Newer DL architectures such as transformers may allow better performance and better interpretability</p></td>
</tr>
</tbody>
</table>
<div class="toctree-wrapper compound">
<span id="document-Introduction"></span><div class="tex2jax_ignore mathjax_ignore section" id="introduction">
<span id="id1"></span><h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">#</a></h2>
<div class="admonition-deep-learning-dl-is-a-very-promising-method-to-decode-brain-signals-from-eeg admonition">
<p class="admonition-title">Deep Learning (DL) is a very promising method to decode brain signals from EEG</p>
<ul class="simple">
<li><p>Deep learning may extract more information from EEG signals that humans can</p></li>
<li><p>Deep learning may improve EEG-based diagnosis, enable new assistive technologies and advance scientific understanding of the EEG signal</p></li>
<li><p>Our EEG decoding deep learning models perform as good or better than feature-based methods</p></li>
</ul>
</div>
<p>Machine learning (ML), i.e., using data to learn programs that can solve specific tasks, has the potential to benefit medical applications. Compared to humans, machine-learning programs can process larger amounts of data and may extract different information. For example, machine-learning algorithms have processed the large amounts of signals recorded in an intensive-care unit to predict kidney failure <span id="id2">[<a class="reference internal" href="Abstract.html#id10" title="Nina Rank, Boris Pfahringer, Jörg Kempfert, Christof Stamm, Titus Kühne, Felix Schoenrath, Volkmar Falk, Carsten Eickhoff, and Alexander Meyer. Deep-learning-based real-time prediction of acute kidney injury outperforms human predictive performance. NPJ digital medicine, 3(1):1–12, 2020.">Rank <em>et al.</em>, 2020</a>, <a class="reference internal" href="Abstract.html#id9" title="Nenad Tomašev, Xavier Glorot, Jack W Rae, Michal Zielinski, Harry Askham, Andre Saraiva, Anne Mottram, Clemens Meyer, Suman Ravuri, Ivan Protsyuk, and others. A clinically applicable approach to continuous prediction of future acute kidney injury. Nature, 572(7767):116–119, 2019.">Tomašev <em>et al.</em>, 2019</a>]</span>, diagnosed breast cancer using high-frequency features usually ignored by doctors <span id="id3">[<a class="reference internal" href="Abstract.html#id6" title="Taro Makino, Stanisław Jastrzębski, Witold Oleszkiewicz, Celin Chacko, Robin Ehrenpreis, Naziya Samreen, Chloe Chhor, Eric Kim, Jiyon Lee, Kristine Pysarenko, and others. Differences between human and machine perception in medical diagnosis. Scientific reports, 12(1):1–13, 2022.">Makino <em>et al.</em>, 2022</a>]</span>, translated brain signals into control signals for external devices in real-time <span id="id4">[<a class="reference internal" href="Abstract.html#id5" title="Reza Abiri, Soheil Borhani, Eric W Sellers, Yang Jiang, and Xiaopeng Zhao. A comprehensive review of eeg-based brain–computer interface paradigms. Journal of neural engineering, 16(1):011001, 2019.">Abiri <em>et al.</em>, 2019</a>]</span> and detected pathology from long brain signal recordings <span id="id5">[<a class="reference internal" href="Abstract.html#id4" title="Lukas AW Gemein, Robin T Schirrmeister, Patryk Chrabąszcz, Daniel Wilson, Joschka Boedecker, Andreas Schulze-Bonhage, Frank Hutter, and Tonio Ball. Machine-learning-based diagnostics of eeg pathology. NeuroImage, 220:117021, 2020.">Gemein <em>et al.</em>, 2020</a>, <a class="reference internal" href="Abstract.html#id31" title="R. Schirrmeister, L. Gemein, K. Eggensperger, F. Hutter, and T. Ball. Deep learning with convolutional neural networks for decoding and visualization of eeg pathology. In 2017 IEEE Signal Processing in Medicine and Biology Symposium (SPMB), volume, 1-7. 2017. doi:10.1109/SPMB.2017.8257015.">Schirrmeister <em>et al.</em>, 2017</a>]</span>. Used in this way, machine learning may improve medical interventions, enable new assistive devices and advance scientific understanding.</p>
<p>Brain-signal decoding is an especially interesting problem to tackle with machine learning. Brain signals contain a lot of information, yet are hard to interpret for humans. Additionally, more brain signals can be recorded than humans could possibly process. Machine-learning algorithms can help doctors triage patients by quickly detecting stroke biomarkers from computed tomography (CT) <span id="id6">[<a class="reference internal" href="Abstract.html#id3" title="Isha R Chavva, Anna L Crawford, Mercy H Mazurek, Matthew M Yuen, Anjali M Prabhat, Sam Payabvash, Gordon Sze, Guido J Falcone, Charles C Matouk, Adam de Havenon, and others. Deep learning applications for acute stroke management. Annals of Neurology, 92(4):574–587, 2022.">Chavva <em>et al.</em>, 2022</a>]</span> and enable brain-computer interfaces by recognizing people’s intentions from electroencephalographic (EEG) in real time  <span id="id7">[<a class="reference internal" href="Abstract.html#id5" title="Reza Abiri, Soheil Borhani, Eric W Sellers, Yang Jiang, and Xiaopeng Zhao. A comprehensive review of eeg-based brain–computer interface paradigms. Journal of neural engineering, 16(1):011001, 2019.">Abiri <em>et al.</em>, 2019</a>]</span>. Also, as brain signals are far from being fully understood, machine-learning algorithms have the potential to advance scientific understanding by finding new brain-signal biomarkers for different pathologies <span id="id8">[<a class="reference internal" href="Abstract.html#id8" title="Maithra Raghu and Eric Schmidt. A survey of deep learning for scientific discovery. arXiv preprint arXiv:2003.11755, 2020.">Raghu and Schmidt, 2020</a>]</span>.</p>
<p>Electroencephalographic (EEG) brain-signal recordings are well-suited for machine learning due to the ease of acquiring them and the difficulty of interpreting them manually. Large EEG datasets can be created relatively easy compared to other medical recordings as EEG recordings are fairly cheap and lack substantial side effects.  Furthermore, EEG recordings are particularly challenging for humans to interpret, making them a promising target for information extraction through machine learning. Some clinical applications of EEG such as pathology diagnosis may be improved by machine learning, while others such as brain-computer interfaces are even only possible because of machine learning. Finally, since the information contained in the EEG signal is far from being fully understood, machine learning may even help understand the EEG signal itself better.</p>
<p>Deep learning is a very promising approach for brain-signal decoding from EEG. The term deep learning describes machine-learning models with multiple computational stages, where the computational stages are typically trained jointly to solve a given task <span id="id9">[<a class="reference internal" href="Abstract.html#id105" title="Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436–444, May 2015. URL: http://www.nature.com/nature/journal/v521/n7553/full/nature14539.html (visited on 2016-05-11), doi:10.1038/nature14539.">LeCun <em>et al.</em>, 2015</a>, <a class="reference internal" href="Abstract.html#id78" title="Juergen Schmidhuber. Deep Learning in Neural Networks: An Overview. Neural Networks, 61:85–117, January 2015. arXiv: 1404.7828. URL: http://arxiv.org/abs/1404.7828 (visited on 2015-08-12), doi:10.1016/j.neunet.2014.09.003.">Schmidhuber, 2015</a>]</span>. Convolutional neural networks (ConvNets) are deep learning models that are  inspired by computational structures in the visual cortex of the brain and have in recent times become very successful models for a wide variety of tasks, including object detection in images, speech recognition from audio or machine translation.  ConvNets only have very general assumptions about the properties of their training signals embedded into them (such as smoothness and hierarchical structure) and have shown great success on a variety of natural signals. Therefore, ConvNets are very promising to apply to hard-to-understand natural signals like EEG signals.</p>
<p>Prior to the work presented in this thesis, it was unclear how well ConvNet architectures can decode EEG signals compared to hand-engineered, feature-based approaches. The high dimensionality, low signal-to-noise ratio and large signal variability (e.g., from person to person or even session to session for the same person) of EEG data present challenges that may  be better addressed by feature-based approaches that exploit more specific assumptions about the EEG signal. While there have been previous efforts to apply deep learning to EEG, a systematic study of the performance of modern ConvNets on EEG decoding compared with a strong feature-based baseline and including the impact of network architecture and training hyperparameters, was lacking. Furthermore, research into understanding what features the ConvNets extract from the EEG signal had been limited.</p>
<p>We therefore created several ConvNet architectures to thoroughly evaluate on EEG decoding. We first evaluated our ConvNet’s decoding performance under a range of different hyperparameter choices on  widely studied movement-related decoding tasks and found they perform at least as good as a strong feature-based baseline. The ConvNets also generalized well to a range of other decoding tasks, including other task-related decoding tasks as well as pathology diagnosis. We also developed visualizations to understand the features the ConvNets extract from the EEG signal, finding that their predictions are sensitive to plausible neurophysiological features.</p>
<p>In this thesis, I will first describe the research on deep learning EEG decoding prior to our work, then proceed to describe the deep network architectures and training methods we developed to rival or surpass feature-based EEG decoding approaches on movement- and other task-related EEG decoding tasks as well as pathology diagnosis. Furthermore, I also describe the visualization methods we developed that suggest the networks are using plausible neurophysiological patterns to solve their tasks.</p>
</div>
<span id="document-PriorWork"></span><div class="tex2jax_ignore mathjax_ignore section" id="prior-work">
<span id="id1"></span><h2>Prior Work<a class="headerlink" href="#prior-work" title="Permalink to this headline">#</a></h2>
<div class="admonition-prior-to-our-work-research-on-deep-learning-based-eeg-decoding-was-limited admonition">
<p class="admonition-title">Prior to our work, research on deep-learning-based EEG decoding was limited</p>
<ul class="simple">
<li><p>Few studies compared to published well-tuned feature-based decoding results</p></li>
<li><p>Most EEG DL architectures had only 1-3 convolutional layers and included fully-connected layers</p></li>
<li><p>Most work only considered very restricted frequency ranges</p></li>
<li><p>Most studies only compared few design choices and training strategies</p></li>
</ul>
</div>
<div class="section" id="decoding-problems-and-baselines">
<h3>Decoding Problems and Baselines<a class="headerlink" href="#decoding-problems-and-baselines" title="Permalink to this headline">#</a></h3>
<table class="colwidths-auto docutils align-default" id="prior-work-tasks-table">
<caption><span class="caption-number">Table 1 </span><span class="caption-text">Decoding problems in deep-learning EEG decoding studies prior to our work. Studies with external baseline compared their decoding results to an external baseline result by other authors.</span><a class="headerlink" href="#prior-work-tasks-table" title="Permalink to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="text-align:left head"><p>Decoding problem</p></th>
<th class="text-align:left head"><p>Number of studies</p></th>
<th class="text-align:left head"><p>With external baseline</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:left"><p>Imagined or Executed Movement</p></td>
<td class="text-align:left"><p>6</p></td>
<td class="text-align:left"><p>2</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Oddball/P300</p></td>
<td class="text-align:left"><p>5</p></td>
<td class="text-align:left"><p>1</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Epilepsy-related</p></td>
<td class="text-align:left"><p>4</p></td>
<td class="text-align:left"><p>2</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Music Rhythm</p></td>
<td class="text-align:left"><p>2</p></td>
<td class="text-align:left"><p>0</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Memory Performance/Cognitive Load</p></td>
<td class="text-align:left"><p>2</p></td>
<td class="text-align:left"><p>0</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Driver Performance</p></td>
<td class="text-align:left"><p>1</p></td>
<td class="text-align:left"><p>0</p></td>
</tr>
</tbody>
</table>
<p>Prior to 2017, when the first work presented in this thesis was published, there was only limited literature on EEG decoding with deep learning. From 19 studies we identified at the time, only 5 compared their decoding results to an external baseline result, limiting the evaluation of the decoding results.  The most widely studied decoding problems were mvoement-related decoding problems such as decoding which body part (hand, feet etc.) a person is moving or imagining to move (see <a class="reference internal" href="#prior-work-tasks-table"><span class="std std-numref">Table 1</span></a>). To advance the understanding of EEG deep learning decoding, we therefore decided to first focus on widely researched movement-related decoding tasks and compare to a strong feature-based baseline (see <a class="reference internal" href="Abstract.html#fbscp-and-filterbank-net"><span class="std std-ref">Filterbank Common Spatial Patterns and Filterbank Network</span></a>).</p>
</div>
<div class="section" id="input-domains-and-frequency-ranges">
<h3>Input Domains and Frequency Ranges<a class="headerlink" href="#input-domains-and-frequency-ranges" title="Permalink to this headline">#</a></h3>
<div class="cell tag_hide-input tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">cm</span>
<span class="kn">import</span> <span class="nn">seaborn</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">from</span> <span class="nn">myst_nb</span> <span class="kn">import</span> <span class="n">glue</span>
<span class="n">seaborn</span><span class="o">.</span><span class="n">set_palette</span><span class="p">(</span><span class="s1">&#39;colorblind&#39;</span><span class="p">)</span>
<span class="n">seaborn</span><span class="o">.</span><span class="n">set_style</span><span class="p">(</span><span class="s1">&#39;darkgrid&#39;</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">re</span>

<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="o">%</span><span class="k">config</span> InlineBackend.figure_format = &#39;png&#39;
<span class="c1">#matplotlib.rcParams[&#39;figure.figsize&#39;] = (12.0, 1.0)</span>
<span class="n">matplotlib</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;font.size&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">14</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="s1">&#39;Time,  8–30 Hz &#39;</span><span class="p">,</span> <span class="s1">&#39;Time, 0.1–40 Hz &#39;</span><span class="p">,</span> <span class="s1">&#39;Time, 0.05–15 Hz &#39;</span><span class="p">,</span>
       <span class="s1">&#39;Time, 0.3–20 Hz &#39;</span><span class="p">,</span> <span class="s1">&#39;Frequency, 6–30 Hz &#39;</span><span class="p">,</span> <span class="s1">&#39; Frequency, 0–200 Hz &#39;</span><span class="p">,</span>
       <span class="s1">&#39;Time,  1–50 Hz &#39;</span><span class="p">,</span> <span class="s1">&#39; Time,  0–100 HZ &#39;</span><span class="p">,</span>
       <span class="s1">&#39;Frequency, mean amplitude for 0–7 Hz, 7–14 Hz, 14–49 Hz &#39;</span><span class="p">,</span>
       <span class="s1">&#39;Time, 0.5–50 Hz &#39;</span><span class="p">,</span> <span class="s1">&#39;Time,  0–128 Hz &#39;</span><span class="p">,</span>
       <span class="s1">&#39; Frequency, mean power for 4–7 Hz, 8–13 Hz, 13–30 Hz &#39;</span><span class="p">,</span>
       <span class="s1">&#39;Time, 0.5–30Hz &#39;</span><span class="p">,</span> <span class="s1">&#39;Time, 0.1–50 Hz &#39;</span><span class="p">,</span>
       <span class="s1">&#39;Frequency, 4–40 Hz, using FBCSP &#39;</span><span class="p">,</span>
       <span class="s1">&#39; Time and frequency evaluated, 0-200 Hz &#39;</span><span class="p">,</span> <span class="s1">&#39;Frequency, 8–30 Hz &#39;</span><span class="p">,</span>
       <span class="s1">&#39;Time, 0.15–200 Hz &#39;</span><span class="p">,</span> <span class="s1">&#39; Time, 0.1-20 Hz &#39;</span><span class="p">])</span>
<span class="n">domain_strings</span> <span class="o">=</span> <span class="p">[</span><span class="n">s</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;,&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">a</span><span class="p">]</span>
<span class="n">start_fs</span> <span class="o">=</span> <span class="p">[</span><span class="nb">float</span><span class="p">(</span><span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;[a-z ]+&#39;</span><span class="p">,</span><span class="sa">r</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">re</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;[–-–-]&#39;</span><span class="p">,</span><span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;,&#39;</span><span class="p">)[</span><span class="mi">1</span><span class="p">:]))[</span><span class="mi">0</span><span class="p">]))</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">a</span><span class="p">]</span>
<span class="n">end_fs</span> <span class="o">=</span> <span class="p">[</span><span class="nb">float</span><span class="p">(</span><span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;[a-z HZFBCSP]+&#39;</span><span class="p">,</span><span class="sa">r</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">re</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;[–-–-]&#39;</span><span class="p">,</span><span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;,&#39;</span><span class="p">)[</span><span class="mi">1</span><span class="p">:]))[</span><span class="mi">1</span><span class="p">]))</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">a</span><span class="p">]</span>
<span class="n">domain_strings</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">domain_strings</span><span class="p">)</span>
<span class="n">start_fs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">start_fs</span><span class="p">)</span>
<span class="n">end_fs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">end_fs</span><span class="p">)</span>

<span class="n">freq_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="s1">&#39;freq&#39;</span> <span class="ow">in</span> <span class="n">s</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">domain_strings</span><span class="p">])</span>
<span class="n">time_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="s1">&#39;time&#39;</span> <span class="ow">in</span> <span class="n">s</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">domain_strings</span><span class="p">])</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">98349384</span><span class="p">)</span>
<span class="n">color</span> <span class="o">=</span> <span class="n">seaborn</span><span class="o">.</span><span class="n">color_palette</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">i_sort</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">flatnonzero</span><span class="p">(</span><span class="n">time_mask</span><span class="p">)[</span><span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">end_fs</span><span class="p">[</span><span class="n">time_mask</span><span class="p">])]</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">d</span><span class="p">,</span><span class="n">s</span><span class="p">,</span><span class="n">e</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span>
        <span class="n">domain_strings</span><span class="p">[</span><span class="n">i_sort</span><span class="p">],</span> <span class="n">start_fs</span><span class="p">[</span><span class="n">i_sort</span><span class="p">],</span> <span class="n">end_fs</span><span class="p">[</span><span class="n">i_sort</span><span class="p">])):</span>
    <span class="n">offset</span> <span class="o">=</span> <span class="mf">0.6</span><span class="o">*</span><span class="n">i</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">i_sort</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.3</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">offset</span><span class="p">,</span><span class="n">offset</span><span class="p">]</span> <span class="p">,</span> <span class="p">[</span><span class="n">s</span><span class="p">,</span> <span class="n">e</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">)</span>
<span class="n">i_sort</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">flatnonzero</span><span class="p">(</span><span class="n">freq_mask</span><span class="p">)[</span><span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">end_fs</span><span class="p">[</span><span class="n">freq_mask</span><span class="p">])]</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">d</span><span class="p">,</span><span class="n">s</span><span class="p">,</span><span class="n">e</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span>
        <span class="n">domain_strings</span><span class="p">[</span><span class="n">i_sort</span><span class="p">],</span> <span class="n">start_fs</span><span class="p">[</span><span class="n">i_sort</span><span class="p">],</span> <span class="n">end_fs</span><span class="p">[</span><span class="n">i_sort</span><span class="p">])):</span>
    <span class="n">offset</span> <span class="o">=</span> <span class="mf">0.6</span><span class="o">*</span><span class="n">i</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">i_sort</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.7</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">offset</span><span class="p">,</span><span class="n">offset</span><span class="p">]</span> <span class="p">,</span> <span class="p">[</span><span class="n">s</span><span class="p">,</span> <span class="n">e</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">1.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Input domain&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Frequency [Hz]&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;Time&quot;</span><span class="p">,</span> <span class="s2">&quot;Frequency&quot;</span><span class="p">],</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">45</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Input domains and frequency ranges in prior work&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.05</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">25</span><span class="p">,</span><span class="mi">50</span><span class="p">,</span><span class="mi">75</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span><span class="mi">150</span><span class="p">,</span><span class="mi">200</span><span class="p">])</span>
<span class="n">glue</span><span class="p">(</span><span class="s1">&#39;input_domain_fig&#39;</span><span class="p">,</span> <span class="n">fig</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">(</span><span class="n">fig</span><span class="p">)</span>
<span class="kc">None</span>
</pre></div>
</div>
</div>
</div>
<div class="figure align-default" id="id28">
<div class="cell_output docutils container">
<img alt="_images/PriorWork_6_0.png" src="_images/PriorWork_6_0.png" />
</div>
<p class="caption"><span class="caption-number">Fig. 1 </span><span class="caption-text"><em>Input domains and frequency ranges in prior work</em>. Grey lines represent frequency ranges of individual studies. Note that many studies only include frequencies below 50 Hz, some use very restricted ranges (alpha/beta band).</span><a class="headerlink" href="#id28" title="Permalink to this image">#</a></p>
</div>
<p>Deep networks can either decode directly from the time-domain EEG or process the data in the frequency domain, for example after a Fourier transformation. 12 of the prior studies used time-domain inputs, 6 used frequency-domain inputs and one used both. We decided to work directly in the time domain, as the deep networks should in principle be able to learn how to extract any needed spectral information from the time-domain input.</p>
<p>Most prior studies that were working in the time domain only used frequencies below 50 Hz. We were interested in how well deep networks can also extract lesser-used higher-frequency components of the EEG signal. For that, we used a sampling rate of 250 Hz, which means we were able to analyze frequencies up to the Nyquist frequency of 125 Hz. As a suitable dataset where high-frequency information may help decoding, we included our high-gamma dataset in our study, since it was recorded specifically to allow extraction of higher-frequency (&gt;50 Hz) information from scalp EEG <span id="id2">[<a class="reference internal" href="Abstract.html#id34" title="Robin Tibor Schirrmeister, Jost Tobias Springenberg, Lukas Dominique Josef Fiederer, Martin Glasstetter, Katharina Eggensperger, Michael Tangermann, Frank Hutter, Wolfram Burgard, and Tonio Ball. Deep learning with convolutional neural networks for eeg decoding and visualization. Human Brain Mapping, aug 2017. URL: http://dx.doi.org/10.1002/hbm.23730, doi:10.1002/hbm.23730.">Schirrmeister <em>et al.</em>, 2017</a>]</span>.</p>
</div>
<div class="section" id="network-architectures">
<h3>Network Architectures<a class="headerlink" href="#network-architectures" title="Permalink to this headline">#</a></h3>
<div class="cell tag_hide-input tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ls</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="s1">&#39; 2/2 &#39;</span><span class="p">,</span> <span class="s1">&#39; 3/1 &#39;</span><span class="p">,</span> <span class="s1">&#39; 2/2 &#39;</span><span class="p">,</span> <span class="s1">&#39; 3/2 &#39;</span><span class="p">,</span> <span class="s1">&#39; 1/1 &#39;</span><span class="p">,</span> <span class="s1">&#39; 1/2 &#39;</span><span class="p">,</span> <span class="s1">&#39; 1/3 &#39;</span><span class="p">,</span>
       <span class="s1">&#39; 1–2/2 &#39;</span><span class="p">,</span> <span class="s1">&#39; 3/1 (+ LSTM as postprocessor) &#39;</span><span class="p">,</span> <span class="s1">&#39; 4/3 &#39;</span><span class="p">,</span> <span class="s1">&#39; 1-3/1-3 &#39;</span><span class="p">,</span>
       <span class="s1">&#39; 3–7/2 (+ LSTM or other temporal post-processing (see design choices)) &#39;</span><span class="p">,</span>
       <span class="s1">&#39; 2/1 &#39;</span><span class="p">,</span> <span class="s1">&#39; 3/3 (Spatio-temporal regularization) &#39;</span><span class="p">,</span>
       <span class="s1">&#39; 2/2 (Final fully connected layer uses concatenated output by convolutionaland fully connected layers) &#39;</span><span class="p">,</span>
       <span class="s1">&#39; 1-2/1 &#39;</span><span class="p">,</span>
       <span class="s1">&#39;2/0 (Convolutional deep belief network, separately trained RBF-SVM classifier) &#39;</span><span class="p">,</span>
       <span class="s1">&#39; 3/1 (Convolutional layers trained as convolutional stacked autoencoder with target prior) &#39;</span><span class="p">,</span>
       <span class="s1">&#39; 2/2 &#39;</span><span class="p">])</span>

<span class="n">conv_ls</span> <span class="o">=</span> <span class="p">[</span><span class="n">l</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;/&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">ls</span><span class="p">]</span>
<span class="n">low_conv_ls</span> <span class="o">=</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">re</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;[–-]&#39;</span><span class="p">,</span> <span class="n">c</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span><span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">conv_ls</span><span class="p">]</span>
<span class="n">high_conv_ls</span> <span class="o">=</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">re</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;[–-]&#39;</span><span class="p">,</span> <span class="n">c</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span><span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">conv_ls</span><span class="p">]</span>
<span class="n">dense_ls</span> <span class="o">=</span> <span class="p">[</span><span class="n">l</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;/&#39;</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">ls</span><span class="p">]</span>
<span class="n">low_dense_ls</span> <span class="o">=</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">re</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;[–-]&#39;</span><span class="p">,</span> <span class="n">c</span><span class="p">[:</span><span class="mi">8</span><span class="p">])[</span><span class="mi">0</span><span class="p">][:</span><span class="mi">2</span><span class="p">])</span><span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">dense_ls</span><span class="p">]</span>
<span class="n">high_dense_ls</span> <span class="o">=</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">re</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;[–-]&#39;</span><span class="p">,</span> <span class="n">c</span><span class="p">[:</span><span class="mi">8</span><span class="p">])[</span><span class="o">-</span><span class="mi">1</span><span class="p">][:</span><span class="mi">2</span><span class="p">])</span><span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">dense_ls</span><span class="p">]</span>

<span class="n">all_conv_ls</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">low_c</span><span class="p">,</span> <span class="n">high_c</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">low_c</span><span class="p">,</span> <span class="n">high_c</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">low_conv_ls</span><span class="p">,</span> <span class="n">high_conv_ls</span><span class="p">)])</span>
<span class="n">all_dense_ls</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">low_c</span><span class="p">,</span> <span class="n">high_c</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">low_c</span><span class="p">,</span> <span class="n">high_c</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">low_dense_ls</span><span class="p">,</span> <span class="n">high_dense_ls</span><span class="p">)])</span>
<span class="n">bincount_conv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">all_conv_ls</span><span class="p">)</span>
<span class="n">bincount_dense</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">all_dense_ls</span><span class="p">)</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">98349384</span><span class="p">)</span>
<span class="n">color</span> <span class="o">=</span> <span class="n">seaborn</span><span class="o">.</span><span class="n">color_palette</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="k">for</span> <span class="n">low_c</span><span class="p">,</span> <span class="n">high_c</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">low_conv_ls</span><span class="p">,</span> <span class="n">high_conv_ls</span><span class="p">):</span>
    <span class="n">offset</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.1</span>
    <span class="n">tried_cs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">low_c</span><span class="p">,</span> <span class="n">high_c</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">offset</span><span class="p">,]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">tried_cs</span><span class="p">),</span> <span class="n">tried_cs</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">)</span>
    
<span class="k">for</span> <span class="n">i_c</span><span class="p">,</span> <span class="n">n_c</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">bincount_conv</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">i_c</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="n">n_c</span><span class="o">*</span><span class="mi">40</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.535</span><span class="p">,</span> <span class="n">i_c</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">n_c</span><span class="p">)</span><span class="o">+</span> <span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;left&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">low_c</span><span class="p">,</span> <span class="n">high_c</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">low_dense_ls</span><span class="p">,</span> <span class="n">high_dense_ls</span><span class="p">):</span>
    <span class="n">offset</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">rng</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.1</span>
    <span class="n">tried_cs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">low_c</span><span class="p">,</span> <span class="n">high_c</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">offset</span><span class="p">,]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">tried_cs</span><span class="p">),</span> <span class="n">tried_cs</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">)</span>
    
<span class="k">for</span> <span class="n">i_c</span><span class="p">,</span> <span class="n">n_c</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">bincount_dense</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="mf">1.4</span><span class="p">,</span> <span class="n">i_c</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="n">n_c</span><span class="o">*</span><span class="mi">40</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">1.535</span><span class="p">,</span> <span class="n">i_c</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">n_c</span><span class="p">)</span><span class="o">+</span> <span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;left&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Type of layer&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Number of layers&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;Convolutional&quot;</span><span class="p">,</span> <span class="s2">&quot;Dense&quot;</span><span class="p">],</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">45</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">7</span><span class="p">]);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Number of layers in prior works&#39; architectures&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.05</span><span class="p">)</span>
<span class="n">glue</span><span class="p">(</span><span class="s1">&#39;layernum_fig&#39;</span><span class="p">,</span> <span class="n">fig</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">(</span><span class="n">fig</span><span class="p">)</span>
<span class="kc">None</span>
</pre></div>
</div>
</div>
</div>
<div class="figure align-default" id="id29">
<div class="cell_output docutils container">
<img alt="_images/PriorWork_10_0.png" src="_images/PriorWork_10_0.png" />
</div>
<p class="caption"><span class="caption-number">Fig. 2 </span><span class="caption-text"><em>Number of layers in prior work</em>. Small grey markers represent individual architectures. Dashed lines indicate different number of layers investigated in a single study (e.g., a single study investigated 3-7 convolutional layers). Larger grey markers indicate sum of occurences of that layer number over all studies (e.g., 9 architectures used 2 convolutional layers). Note most architectures use only 1-3 convolutional layers.</span><a class="headerlink" href="#id29" title="Permalink to this image">#</a></p>
</div>
<p>The architectures used in prior work typically only included up to 3 layers, with only 2 studies considering more layers. As network architectures in other domains tend to be a lot deeper, we also evaluated architectures with a larger number of layers in our work. Several architectures from prior work also included fully-connected layers with larger number of parameters which had fallen out of favor in computer-vision deep-learning architectures due to their large compute and memory requirements with little accuracy benefit. Our architectures do not include traditional fully-connected layers with large number of parameters.</p>
</div>
<div class="section" id="hyperparameter-evaluations">
<h3>Hyperparameter Evaluations<a class="headerlink" href="#hyperparameter-evaluations" title="Permalink to this headline">#</a></h3>
<table class="colwidths-auto docutils align-default" id="prior-work-design-choices-table">
<caption><span class="caption-number">Table 2 </span><span class="caption-text">Design choices and training strategies that prior deep-learning EEG decoding studies had studies.</span><a class="headerlink" href="#prior-work-design-choices-table" title="Permalink to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="text-align:left head"><p>Study</p></th>
<th class="text-align:left head"><p>Design choices</p></th>
<th class="text-align:left head"><p>Training strategies</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:left"><p><span id="id3">[<a class="reference internal" href="Abstract.html#id150" title="Vernon J. Lawhern, Amelia J. Solon, Nicholas R. Waytowich, Stephen M. Gordon, Chou P. Hung, and Brent J. Lance. EEGNet: A Compact Convolutional Network for EEG-based Brain-Computer Interfaces. arXiv:1611.08024 [cs, q-bio, stat], November 2016. arXiv: 1611.08024. URL: http://arxiv.org/abs/1611.08024 (visited on 2016-12-20).">Lawhern <em>et al.</em>, 2016</a>]</span></p></td>
<td class="text-align:left"><p>Kernel sizes</p></td>
<td class="text-align:left"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p><span id="id4">[<a class="reference internal" href="Abstract.html#id222" title="Xuyun Sun, Cunle Qian, Zhongqin Chen, Zhaohui Wu, Benyan Luo, and Gang Pan. Remembered or Forgotten?—An EEG-Based Computational Prediction Approach. PLOS ONE, 11(12):e0167497, December 2016. URL: http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0167497 (visited on 2017-02-14), doi:10.1371/journal.pone.0167497.">Sun <em>et al.</em>, 2016</a>]</span></p></td>
<td class="text-align:left"><p></p></td>
<td class="text-align:left"><p>Different time windows</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p><span id="id5">[<a class="reference internal" href="Abstract.html#id215" title="Yousef Rezaei Tabar and Ugur Halici. A novel deep learning approach for classification of EEG motor imagery signals. Journal of Neural Engineering, 14(1):016003, 2017. URL: http://stacks.iop.org/1741-2552/14/i=1/a=016003 (visited on 2017-02-14), doi:10.1088/1741-2560/14/1/016003.">Tabar and Halici, 2017</a>]</span></p></td>
<td class="text-align:left"><p>Addition of six-layer stacked autoencoder on ConvNet features <br> Kernel sizes</p></td>
<td class="text-align:left"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p><span id="id6">[<a class="reference internal" href="Abstract.html#id172" title="J. Liang, R. Lu, C. Zhang, and F. Wang. Predicting Seizures from Electroencephalography Recordings: A Knowledge Transfer Strategy. In 2016 IEEE International Conference on Healthcare Informatics (ICHI), 184–191. October 2016. doi:10.1109/ICHI.2016.27.">Liang <em>et al.</em>, 2016</a>]</span></p></td>
<td class="text-align:left"><p></p></td>
<td class="text-align:left"><p>Different subdivisions of frequency range <br>Different lengths of time crops <br>Transfer learning with auxiliary non-epilepsy datasets</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p><span id="id7">[<a class="reference internal" href="Abstract.html#id180" title="Mehdi Hajinoroozi, Zijing Mao, Tzyy-Ping Jung, Chin-Teng Lin, and Yufei Huang. EEG-based prediction of driver's cognitive performance by deep convolutional neural network. Signal Processing: Image Communication, 47:549–555, September 2016. URL: http://www.sciencedirect.com/science/article/pii/S0923596516300832 (visited on 2016-12-20), doi:10.1016/j.image.2016.05.018.">Hajinoroozi <em>et al.</em>, 2016</a>]</span></p></td>
<td class="text-align:left"><p>Replacement of convolutional layers by restricted Boltzmann machines with slightly varied network architecture}</p></td>
<td class="text-align:left"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p><span id="id8">[<a class="reference internal" href="Abstract.html#id154" title="A. Antoniades, L. Spyrou, C. C. Took, and S. Sanei. Deep learning for epileptic intracranial EEG data. In 2016 IEEE 26th International Workshop on Machine Learning for Signal Processing (MLSP), 1–6. September 2016. doi:10.1109/MLSP.2016.7738824.">Antoniades <em>et al.</em>, 2016</a>]</span></p></td>
<td class="text-align:left"><p>1 or 2 convolutional layers</p></td>
<td class="text-align:left"><p></p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p><span id="id9">[<a class="reference internal" href="Abstract.html#id204" title="A. Page, C. Shea, and T. Mohsenin. Wearable seizure detection using convolutional neural networks with transfer learning. In 2016 IEEE International Symposium on Circuits and Systems (ISCAS), 1086–1089. May 2016. doi:10.1109/ISCAS.2016.7527433.">Page <em>et al.</em>, 2016</a>]</span></p></td>
<td class="text-align:left"><p></p></td>
<td class="text-align:left"><p>Cross-subject supervised training, within-subject finetuning of fully connected layers</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p><span id="id10">[<a class="reference internal" href="Abstract.html#id197" title="Pouya Bashivan, Irina Rish, Mohammed Yeasin, and Noel Codella. Learning Representations from EEG with Deep Recurrent-Convolutional Neural Networks. In arXiv:1511.06448 [cs]. 2016. arXiv: 1511.06448. URL: http://arxiv.org/abs/1511.06448 (visited on 2016-12-20).">Bashivan <em>et al.</em>, 2016</a>]</span></p></td>
<td class="text-align:left"><p>Number of convolutional layers <br>Temporal processing of ConvNet output by max pooling, temporal convolution, LSTM or temporal convolution + LSTM</p></td>
<td class="text-align:left"><p></p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p><span id="id11">[<a class="reference internal" href="Abstract.html#id226" title="Sebastian Stober. Learning Discriminative Features from Electroencephalography Recordings by Encoding Similarity Constraints. In Bernstein Conference 2016. 2016. doi:10.12751/nncn.bc2016.0223.">Stober, 2016</a>]</span></p></td>
<td class="text-align:left"><p>Kernel sizes</p></td>
<td class="text-align:left"><p>Pretraining first layer as convolutional autoencoder with different constraints</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p><span id="id12">[<a class="reference internal" href="Abstract.html#id117" title="S. Sakhavi, C. Guan, and S. Yan. Parallel convolutional-linear neural network for motor imagery classification. In Signal Processing Conference (EUSIPCO), 2015 23rd European, 2736–2740. August 2015. doi:10.1109/EUSIPCO.2015.7362882.">Sakhavi <em>et al.</em>, 2015</a>]</span></p></td>
<td class="text-align:left"><p>Combination ConvNet and MLP (trained on different features) vs. only ConvNet vs. only MLP</p></td>
<td class="text-align:left"><p></p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p><span id="id13">[<a class="reference internal" href="Abstract.html#id184" title="Sebastian Stober, Daniel J. Cameron, and Jessica A. Grahn. Using Convolutional Neural Networks to Recognize Rhythm Stimuli from Electroencephalography Recordings. In Proceedings of the 27th International Conference on Neural Information Processing Systems, NIPS'14, 1449–1457. Cambridge, MA, USA, 2014. MIT Press. URL: http://dl.acm.org/citation.cfm?id=2968826.2968988 (visited on 2016-12-20).">Stober <em>et al.</em>, 2014</a>]</span></p></td>
<td class="text-align:left"><p>Best values from automatic hyperparameter optimization: frequency cutoff, one vs two layers, kernel sizes, number of channels, pooling width</p></td>
<td class="text-align:left"><p>Best values from automatic hyperparameter optimization: learning rate, learning rate decay, momentum, final momentum</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p><span id="id14">[<a class="reference internal" href="Abstract.html#id209" title="Zuoguan Wang, Siwei Lyu, Gerwin Schalk, and Qiang Ji. Deep Feature Learning Using Target Priors with Applications in ECoG Signal Decoding for BCI. In Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence, IJCAI '13, 1785–1791. Beijing, China, 2013. AAAI Press. URL: http://dl.acm.org/citation.cfm?id=2540128.2540384 (visited on 2017-01-16).">Wang <em>et al.</em>, 2013</a>]</span></p></td>
<td class="text-align:left"><p>Partially supervised CSA</p></td>
<td class="text-align:left"><p></p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p><span id="id15">[<a class="reference internal" href="Abstract.html#id212" title="Hubert Cecotti and Axel Graser. Convolutional Neural Networks for P300 Detection with Application to Brain-Computer Interfaces. IEEE Transactions on Pattern Analysis and Machine Intelligence, 33(3):433–445, March 2011. URL: http://dx.doi.org/10.1109/TPAMI.2010.125 (visited on 2016-12-20), doi:10.1109/TPAMI.2010.125.">Cecotti and Graser, 2011</a>]</span></p></td>
<td class="text-align:left"><p>Electrode subset (fixed or automatically determined) <br>Using only one spatial filter <br>Different ensembling strategies</p></td>
<td class="text-align:left"><p></p></td>
</tr>
</tbody>
</table>
<p>Prior work varied widely in which design choices and training strategies were compared. 6 of the studies did not compare any design choices or  training strategy hyperparamters. The other 13 studies evaluated different hyperparameters, with the most common one the kernel size (see <a class="reference internal" href="#prior-work-design-choices-table"><span class="std std-numref">Table 2</span></a>). Only one study evaluated a wider range of hyperparameters <span id="id16">[<a class="reference internal" href="Abstract.html#id184" title="Sebastian Stober, Daniel J. Cameron, and Jessica A. Grahn. Using Convolutional Neural Networks to Recognize Rhythm Stimuli from Electroencephalography Recordings. In Proceedings of the 27th International Conference on Neural Information Processing Systems, NIPS'14, 1449–1457. Cambridge, MA, USA, 2014. MIT Press. URL: http://dl.acm.org/citation.cfm?id=2968826.2968988 (visited on 2016-12-20).">Stober <em>et al.</em>, 2014</a>]</span>. To fill this gap, we compared a wider range of design choices and training strategies and specifically evaluated in how far improvements of computer vision architecture design choices and training strategies also lead to improvements in EEG decoding.</p>
</div>
<div class="section" id="visualizations">
<h3>Visualizations<a class="headerlink" href="#visualizations" title="Permalink to this headline">#</a></h3>
<table class="colwidths-auto docutils align-default" id="prior-work-visualizations-table">
<caption><span class="caption-number">Table 3 </span><span class="caption-text">Visualizations presented in prior work.</span><a class="headerlink" href="#prior-work-visualizations-table" title="Permalink to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="text-align:left head"><p>Study</p></th>
<th class="text-align:left head"><p>Visualization type(s)</p></th>
<th class="text-align:left head"><p>Visualization findings</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:left"><p><span id="id17">[<a class="reference internal" href="Abstract.html#id222" title="Xuyun Sun, Cunle Qian, Zhongqin Chen, Zhaohui Wu, Benyan Luo, and Gang Pan. Remembered or Forgotten?—An EEG-Based Computational Prediction Approach. PLOS ONE, 11(12):e0167497, December 2016. URL: http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0167497 (visited on 2017-02-14), doi:10.1371/journal.pone.0167497.">Sun <em>et al.</em>, 2016</a>]</span></p></td>
<td class="text-align:left"><p>Weights (spatial)</p></td>
<td class="text-align:left"><p>Largest weights found over prefrontal and temporal cortex</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p><span id="id18">[<a class="reference internal" href="Abstract.html#id196" title="Ran Manor, Liran Mishali, and Amir B. Geva. Multimodal Neural Network for Rapid Serial Visual Presentation Brain Computer Interface. Frontiers in Computational Neuroscience, December 2016. URL: http://www.ncbi.nlm.nih.gov/pmc/articles/PMC5168930/ (visited on 2017-02-03), doi:10.3389/fncom.2016.00130.">Manor <em>et al.</em>, 2016</a>]</span></p></td>
<td class="text-align:left"><p>Weights <br> Activations <br> Saliency maps by gradient</p></td>
<td class="text-align:left"><p>Weights showed typical P300 distribution <br>Activations were high at plausible times (300-500ms) <br>Saliency maps showed plausible spatio-temporal plots</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p><span id="id19">[<a class="reference internal" href="Abstract.html#id215" title="Yousef Rezaei Tabar and Ugur Halici. A novel deep learning approach for classification of EEG motor imagery signals. Journal of Neural Engineering, 14(1):016003, 2017. URL: http://stacks.iop.org/1741-2552/14/i=1/a=016003 (visited on 2017-02-14), doi:10.1088/1741-2560/14/1/016003.">Tabar and Halici, 2017</a>]</span></p></td>
<td class="text-align:left"><p>Weights (spatial + frequential)</p></td>
<td class="text-align:left"><p>Some weights represented difference of values of two electrodes on different sides of head</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p><span id="id20">[<a class="reference internal" href="Abstract.html#id172" title="J. Liang, R. Lu, C. Zhang, and F. Wang. Predicting Seizures from Electroencephalography Recordings: A Knowledge Transfer Strategy. In 2016 IEEE International Conference on Healthcare Informatics (ICHI), 184–191. October 2016. doi:10.1109/ICHI.2016.27.">Liang <em>et al.</em>, 2016</a>]</span></p></td>
<td class="text-align:left"><p>Weights <br> Clustering of weights</p></td>
<td class="text-align:left"><p>Clusters of weights showed typical frequency band subdivision (delta, theta, alpha, beta, gamma)</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p><span id="id21">[<a class="reference internal" href="Abstract.html#id154" title="A. Antoniades, L. Spyrou, C. C. Took, and S. Sanei. Deep learning for epileptic intracranial EEG data. In 2016 IEEE 26th International Workshop on Machine Learning for Signal Processing (MLSP), 1–6. September 2016. doi:10.1109/MLSP.2016.7738824.">Antoniades <em>et al.</em>, 2016</a>]</span></p></td>
<td class="text-align:left"><p>Weights <br>Correlation weights and interictal epileptic discharges (IED) <br>Activations</p></td>
<td class="text-align:left"><p>Weights increasingly correlated with IED waveforms with increasing number of training iterations <br>Second layer captured more complex and well-defined epileptic shapes than first layer <br>IEDs led to highly synchronized activations for neighbouring electrodes</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p><span id="id22">[<a class="reference internal" href="Abstract.html#id182" title="Pierre Thodoroff, Joelle Pineau, and Andrew Lim. Learning Robust Features using Deep Learning for Automatic Seizure Detection. In JMLR Workshop and Conference Proceedings, volume 56. 2016. URL: http://www.jmlr.org/proceedings/papers/v56/Thodoroff16.pdf (visited on 2017-02-14).">Thodoroff <em>et al.</em>, 2016</a>]</span></p></td>
<td class="text-align:left"><p>Input occlusion and effect on prediction accuracy</p></td>
<td class="text-align:left"><p>Allowed to locate areas critical for seizure</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p><span id="id23">[<a class="reference internal" href="Abstract.html#id200" title="Jared Shamwell, Hyungtae Lee, Heesung Kwon, Amar R. Marathe, Vernon Lawhern, and William Nothwang. Single-trial EEG RSVP classification using convolutional neural networks. In Thomas George, Achyut K. Dutta, and M. Saif Islam, editors, SPIE Defense+ Security, volume 9836. International Society for Optics and Photonics, May 2016. URL: http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.2224172 (visited on 2017-02-14), doi:10.1117/12.2224172.">Shamwell <em>et al.</em>, 2016</a>]</span></p></td>
<td class="text-align:left"><p>Weights (spatial)</p></td>
<td class="text-align:left"><p>Some filter weights had expected topographic distributions for P300 <br>Others filters had large weights on areas not traditionally associated with P300</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p><span id="id24">[<a class="reference internal" href="Abstract.html#id197" title="Pouya Bashivan, Irina Rish, Mohammed Yeasin, and Noel Codella. Learning Representations from EEG with Deep Recurrent-Convolutional Neural Networks. In arXiv:1511.06448 [cs]. 2016. arXiv: 1511.06448. URL: http://arxiv.org/abs/1511.06448 (visited on 2016-12-20).">Bashivan <em>et al.</em>, 2016</a>]</span></p></td>
<td class="text-align:left"><p>Inputs that maximally activate given filter <br>Activations of these inputs <br>”Deconvolution” for these inputs</p></td>
<td class="text-align:left"><p>Different filters were sensitive to different frequency bands <br>Later layers had more spatially localized activations <br>Learned features had noticeable links to well-known electrophysiological markers of cognitive load <br></p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p><span id="id25">[<a class="reference internal" href="Abstract.html#id226" title="Sebastian Stober. Learning Discriminative Features from Electroencephalography Recordings by Encoding Similarity Constraints. In Bernstein Conference 2016. 2016. doi:10.12751/nncn.bc2016.0223.">Stober, 2016</a>]</span></p></td>
<td class="text-align:left"><p>Weights (spatial+3 timesteps, pretrained as autoencoder)</p></td>
<td class="text-align:left"><p>Different constraints led to different weights, one type of constraints could enforce weights that are similar across subjects; other type of constraints led to weights that have similar spatial topographies under different architectural configurations and preprocessings</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p><span id="id26">[<a class="reference internal" href="Abstract.html#id147" title="Ran Manor and Amir B. Geva. Convolutional Neural Network for Multi-Category Rapid Serial Visual Presentation BCI. Frontiers in Computational Neuroscience, 9:146, 2015. doi:10.3389/fncom.2015.00146.">Manor and Geva, 2015</a>]</span></p></td>
<td class="text-align:left"><p>Weights <br> Mean and single-trial activations</p></td>
<td class="text-align:left"><p>Spatiotemporal regularization led to softer peaks in weights <br>Spatial weights showed typical P300 distribution <br>Activations mostly had peaks at typical times (300-400ms)</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p><span id="id27">[<a class="reference internal" href="Abstract.html#id212" title="Hubert Cecotti and Axel Graser. Convolutional Neural Networks for P300 Detection with Application to Brain-Computer Interfaces. IEEE Transactions on Pattern Analysis and Machine Intelligence, 33(3):433–445, March 2011. URL: http://dx.doi.org/10.1109/TPAMI.2010.125 (visited on 2016-12-20), doi:10.1109/TPAMI.2010.125.">Cecotti and Graser, 2011</a>]</span></p></td>
<td class="text-align:left"><p>Weights</p></td>
<td class="text-align:left"><p>Spatial filters were similar for different architectures <br>Spatial filters were different (more focal, more diffuse) for different subjects</p></td>
</tr>
</tbody>
</table>
<p>Visualizations can help understand what information the networks are extracting from the EEG signal. 11 of the prior 19 studies presented any visualizations. These studies mostly focussed on analyzing weights and activations, see <a class="reference internal" href="#prior-work-visualizations-table"><span class="std std-numref">Table 3</span></a>. In our work, we focused on investigating how far the networks extract features known to work well for movement-related decoding, see <a class="reference internal" href="Abstract.html#perturbation-visualization"><span class="std std-ref">Perturbation Visualization</span></a>.</p>
<div class="tip admonition">
<p class="admonition-title">We aimed to thoroughly evaluate deep-learning-based EEG decoding by…</p>
<ul class="simple">
<li><p>using well-researched EEG movement-related decoding tasks with strong baselines</p></li>
<li><p>using a dataset suitable to analyze extraction of higher-frequency information</p></li>
<li><p>trying shallower EEG-specific as well as deeper more generic architectures</p></li>
<li><p>evaluating many design choices and two training strategies</p></li>
<li><p>investigating in how far networks learn to extract well-known EEG features</p></li>
</ul>
</div>
</div>
</div>
<span id="document-FBCSPAndFBCSPNet"></span><div class="tex2jax_ignore mathjax_ignore section" id="filterbank-common-spatial-patterns-and-filterbank-network">
<span id="fbscp-and-filterbank-net"></span><h2>Filterbank Common Spatial Patterns and Filterbank Network<a class="headerlink" href="#filterbank-common-spatial-patterns-and-filterbank-network" title="Permalink to this headline">#</a></h2>
<p>In a prior master thesis <span id="id1">[<a class="reference internal" href="Abstract.html#id2" title="Robin Tibor Schirrmeister. Convolutional neural networks for movement decoding from eeg signals. Master's thesis, Albert-Ludwigs-Universität Freiburg, 2015.">Schirrmeister, 2015</a>]</span>, we had developed a neural network architecture closely resembling the feature-based decoding algorithm Filterbank Common Spatial Patterns. In this chapter I describe filter bank common spatial patterns as well as the corresponding filterbank network of the prior master thesis as the starting point for the network architectures developed in the context of this thesis.</p>
<div class="section" id="filter-bank-common-spatial-patterns-as-a-starting-point">
<h3>Filter Bank Common Spatial Patterns as a Starting Point<a class="headerlink" href="#filter-bank-common-spatial-patterns-as-a-starting-point" title="Permalink to this headline">#</a></h3>
<p>We selected filter bank common spatial patterns (FBSCP <span id="id2">[<a class="reference internal" href="Abstract.html#id65" title="Kai Keng Ang, Zheng Yang Chin, Haihong Zhang, and Cuntai Guan. Filter Bank Common Spatial Pattern (FBCSP) in Brain-Computer Interface. In IEEE International Joint Conference on Neural Networks, 2008. IJCNN 2008. (IEEE World Congress on Computational Intelligence), 2390–2397. June 2008. URL: http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4634130, doi:10.1109/IJCNN.2008.4634130.">Ang <em>et al.</em>, 2008</a>, <a class="reference internal" href="Abstract.html#id67" title="Zheng Yang Chin, Kai Keng Ang, Chuanchu Wang, Cuntai Guan, and Haihong Zhang. Multi-class filter bank common spatial pattern for four-class motor imagery BCI. In Annual International Conference of the IEEE Engineering in Medicine and Biology Society, 2009. EMBC 2009, 571–574. September 2009. URL: http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=5332383, doi:10.1109/IEMBS.2009.5332383.">Chin <em>et al.</em>, 2009</a>]</span>) as the feature-based EEG-decoding algorithm our initial neural network architectures were designed to mimic. FBCSP is an EEG-decoding algorithm that has been successfully used in task-related EEG-decoding competitions <span id="id3">[<a class="reference internal" href="Abstract.html#id80" title="Michael Tangermann, Klaus-Robert Müller, Ad Aertsen, Niels Birbaumer, Christoph Braun, Clemens Brunner, Robert Leeb, Carsten Mehring, Kai J. Miller, Gernot R. Müller-Putz, Guido Nolte, Gert Pfurtscheller, Hubert Preissl, Gerwin Schalk, Alois Schlögl, Carmen Vidaurre, Stephan Waldert, and Benjamin Blankertz. Review of the BCI Competition IV. Frontiers in Neuroscience, July 2012. URL: http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3396284/ (visited on 2015-08-20), doi:10.3389/fnins.2012.00055.">Tangermann <em>et al.</em>, 2012</a>]</span>. FBCSP aims to decode task-related changes in signal amplitude in different frequencies, such as a decrease in the amplitude of alpha and beta-band oscillations during movement imagination. In the following we will explain how FBCSP decodes two classes of EEG signals by finding frequency-specific spatial filters that transform the signal, such that it has relatively high variance for one class and low variance for the other class and vice versa.</p>
</div>
<div class="section" id="common-spatial-patterns">
<h3>Common Spatial Patterns<a class="headerlink" href="#common-spatial-patterns" title="Permalink to this headline">#</a></h3>
<div class="figure align-default" id="csp-figure">
<img alt="_images/Methods_Common_Spatial_Patterns_18_0.png" src="_images/Methods_Common_Spatial_Patterns_18_0.png" />
<p class="caption"><span class="caption-number">Fig. 3 </span><span class="caption-text">Common Spatial Patterns example taken from a master thesis <span id="id4">[<a class="reference internal" href="Abstract.html#id2" title="Robin Tibor Schirrmeister. Convolutional neural networks for movement decoding from eeg signals. Master's thesis, Albert-Ludwigs-Universität Freiburg, 2015.">Schirrmeister, 2015</a>]</span>. Top parts show EEG signals for three electrodes during a left hand and  a right hand movement. Bottom parts show spatially filtered signals of two CSP filters. Green parts have lower variance and red parts have higher variance. Note that this difference is strongly amplified after CSP filtering.</span><a class="headerlink" href="#csp-figure" title="Permalink to this image">#</a></p>
</div>
<p>The basic building block of FBCSP is the Common Spatial Patterns (CSP) algorithm. CSP is used to decode brain signals that lead to a change in the amplitudes of the EEG signal with a specific spatial topography <span id="id5">[<a class="reference internal" href="Abstract.html#id62" title="B. Blankertz, R. Tomioka, S. Lemm, M. Kawanabe, and K.-R. Muller. Optimizing Spatial filters for Robust EEG Single-Trial Analysis. IEEE Signal Processing Magazine, 25(1):41–56, 2008. URL: http://ieeexplore.ieee.org/xpls/icp.jsp?arnumber=4408441, doi:10.1109/MSP.2008.4408441.">Blankertz <em>et al.</em>, 2008</a>, <a class="reference internal" href="Abstract.html#id157" title="Zoltan J. Koles, Michael S. Lazar, and Steven Z. Zhou. Spatial patterns underlying population differences in the background EEG. Brain Topography, 2(4):275–284, June 1990. URL: http://link.springer.com/article/10.1007/BF01129656 (visited on 2017-01-09), doi:10.1007/BF01129656.">Koles <em>et al.</em>, 1990</a>, <a class="reference internal" href="Abstract.html#id113" title="H. Ramoser, J. Muller-Gerking, and G. Pfurtscheller. Optimal spatial filtering of single trial EEG during imagined hand movement. IEEE Transactions on Rehabilitation Engineering, 8(4):441–446, December 2000. doi:10.1109/86.895946.">Ramoser <em>et al.</em>, 2000</a>]</span>. To do that, CSP aims to maximize the ratio of the signal variance between spatially filtered signals of two classes, e.g. of the signal during two different movements. For example, the signal of a spatial filter computed by CSP may have a very large variance during movements of the left hand and a very small variance during movements of the right hand. Concretely, we are given signals <span class="math notranslate nohighlight">\(X_{1}, X_{2} \in \mathbb{R}^{n x k x t}\)</span> from <span class="math notranslate nohighlight">\(n\)</span> EEG trials (can be different for <span class="math notranslate nohighlight">\(X_1, X_2\)</span>), <span class="math notranslate nohighlight">\(k\)</span> EEG electrodes and <span class="math notranslate nohighlight">\(t\)</span> timepoints within each trial. CSP then finds a spatial filter <span class="math notranslate nohighlight">\(w\)</span> that maximize the ratio of the variances of the spatially filtered <span class="math notranslate nohighlight">\(X_1,X_2\)</span>:</p>
<p><span class="math notranslate nohighlight">\(w=\arg\!\max_w\frac{Var(w^T X_1)}{Var(w^T X_2)}= \arg\!\max_w\frac{||w^T X_1||^2}{||w^T X_2||^2}=\arg\!\max_w\frac{w^T X_1  X_1^T w}{w^T X_2  X_2^T w}\)</span></p>
<p>Rather than just finding a single spatial filter <span class="math notranslate nohighlight">\(w\)</span>, CSP is typically used to find a whole matrix of spatial filters <span class="math notranslate nohighlight">\(W^{kxk}\)</span>, with spatial filters ordered by the above variance ratio and orthogonal to each other. So the first filter <span class="math notranslate nohighlight">\(w_1\)</span> results in the largest variance ratio and the last filter <span class="math notranslate nohighlight">\(w_k\)</span> results in the smallest variance ratio. Different algorithms can then be used to subselect some set of filters to filter signals for a subsequent decoding algorithm.</p>
<p>The CSP-filtered signals can be used to construct features to train a classifier. Since the CSP-filtered signals should have very different variances for the different classes, the natural choice is to use the per-trial variances of the CSP-filtered signals as features. This results in as many features per trial as the number of CSP filters that were selected for decoding. Typically, one applies the logarithm to the variances to get more standard-normally distributed features.</p>
</div>
<div class="section" id="filterbank">
<h3>Filterbank<a class="headerlink" href="#filterbank" title="Permalink to this headline">#</a></h3>
<p>CSP is typically applied to an EEG signal that has been bandpass filtered to a specific frequency range. The filtering to a frequency range is useful as brain signals cause EEG signal amplitude changes that are temporally and spatially different for different frequencies <span id="id6">[<a class="reference internal" href="Abstract.html#id2" title="Robin Tibor Schirrmeister. Convolutional neural networks for movement decoding from eeg signals. Master's thesis, Albert-Ludwigs-Universität Freiburg, 2015.">Schirrmeister, 2015</a>]</span>. For example, during movement the alpha rhythm may be suppressed for multiple electrodes covering a fairly large region on the scalp while the high gamma rhythm would be amplified for a few electrodes covering a smaller region.</p>
<p>Filterbank Common Spatial Patterns applies CSP separately on signals bandpass-filtered to different frequency ranges <span id="id7">[<a class="reference internal" href="Abstract.html#id65" title="Kai Keng Ang, Zheng Yang Chin, Haihong Zhang, and Cuntai Guan. Filter Bank Common Spatial Pattern (FBCSP) in Brain-Computer Interface. In IEEE International Joint Conference on Neural Networks, 2008. IJCNN 2008. (IEEE World Congress on Computational Intelligence), 2390–2397. June 2008. URL: http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4634130, doi:10.1109/IJCNN.2008.4634130.">Ang <em>et al.</em>, 2008</a>, <a class="reference internal" href="Abstract.html#id67" title="Zheng Yang Chin, Kai Keng Ang, Chuanchu Wang, Cuntai Guan, and Haihong Zhang. Multi-class filter bank common spatial pattern for four-class motor imagery BCI. In Annual International Conference of the IEEE Engineering in Medicine and Biology Society, 2009. EMBC 2009, 571–574. September 2009. URL: http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=5332383, doi:10.1109/IEMBS.2009.5332383.">Chin <em>et al.</em>, 2009</a>]</span>. This allows to capture multiple frequency-specific changes in the EEG signal and can also make the decoding more robust to subject-specific signal characteristics, i.e., which frequency range is most informative for a given subject. The trial-log-variance features of each frequencyband and each CSP filter are then concatenated to form the entire trial feature vector. Typically, a feature selection procedure will select a subset of these features to train the final classifier.</p>
<p>The overall FBCSP pipeline hence looks like this:</p>
<ol class="simple">
<li><p><strong>Bandpass filtering</strong>: Different bandpass filters are applied to separate the raw EEG signal into different frequency bands.</p></li>
<li><p><strong>Epoching</strong>: The continuous EEG signal is cut into trials as explained in the section “Input and labels.”</p></li>
<li><p><strong>CSP computation</strong>: Per frequency band, the common spatial patterns (CSP) algorithm is applied to extract spatial filters. CSP aims to extract spatial filters that make the trials discriminable by the power of the spatially filtered trial signal (see Koles et al. [1990], Ramoser et al. [2000], and Blankertz et al. [2008] for more details on the computation).</p></li>
<li><p><strong>Spatial filtering</strong>: The spatial filters computed in Step 2 are applied to the EEG signal.</p></li>
<li><p><strong>Feature construction</strong>: Feature vectors are constructed from the filtered signals: Specifically, feature vectors are the log-variance of the spatially filtered trial signal for each frequency band and for each spatial filter.</p></li>
<li><p><strong>Classification</strong>: A classifier is trained to predict per-trial labels based on the feature vectors.</p></li>
</ol>
</div>
<div class="section" id="filterbank-network-architecture">
<h3>Filterbank network architecture<a class="headerlink" href="#filterbank-network-architecture" title="Permalink to this headline">#</a></h3>
<p>The first neural network architecture was developed by us in a prior master thesis <span id="id8">[<a class="reference internal" href="Abstract.html#id2" title="Robin Tibor Schirrmeister. Convolutional neural networks for movement decoding from eeg signals. Master's thesis, Albert-Ludwigs-Universität Freiburg, 2015.">Schirrmeister, 2015</a>]</span> to jointly learn the same steps that are learned separately by FBCSP. Concretely, the network simultaneously learn the spatial filters across many frequency bands and the classification weights for the trial variances of all resulting spatially filtered signals. To be able to do that, the network is fed with several signals that were bandpass-filtered to different frequency ranges. The network then performs the following steps:</p>
<ol class="simple">
<li><p>Apply learnable spatial filter weights, resulting in spatially filtered signals</p></li>
<li><p>Square the resulting signals</p></li>
<li><p>Sum the squared signals across the trial</p></li>
<li><p>Take the logarithm of the summed values</p></li>
<li><p>Apply learnable classification weights on these “log-variance” features</p></li>
<li><p>Take the softmax to produce per-class predictions.</p></li>
</ol>
<p>The spatial filter weights and the classification weights are trained jointly.</p>
<div class="figure align-default" id="filterbank-net-figure">
<img alt="_images/csp_as_a_net_explanation.png" src="_images/csp_as_a_net_explanation.png" />
<p class="caption"><span class="caption-number">Fig. 4 </span><span class="caption-text">Filterbank network architecture overview.  Input signals were bandpass filtered to different frequency ranges. Signals are first transformed by learned spatial filters, then squared, summed and the log-transformed. The resulting features are transformed into class probabilities by a classification weights followed by the softmax function. Taken from from a master thesis <span id="id9">[<a class="reference internal" href="Abstract.html#id2" title="Robin Tibor Schirrmeister. Convolutional neural networks for movement decoding from eeg signals. Master's thesis, Albert-Ludwigs-Universität Freiburg, 2015.">Schirrmeister, 2015</a>]</span>.</span><a class="headerlink" href="#filterbank-net-figure" title="Permalink to this image">#</a></p>
</div>
</div>
</div>
<span id="document-DeepArchitectures"></span><div class="tex2jax_ignore mathjax_ignore section" id="neural-network-architectures-for-eeg-decoding">
<span id="network-architectures"></span><h2>Neural Network Architectures for EEG-Decoding<a class="headerlink" href="#neural-network-architectures-for-eeg-decoding" title="Permalink to this headline">#</a></h2>
<p>We continued developing our neural network architectures with our EEG-specific development strategy of starting with networks that resemble feature-based algorithms. After the filterbank network from the master thesis, we adapted the so-called shallow network, initally also developed in the same master thesis <span id="id1">[<a class="reference internal" href="Abstract.html#id2" title="Robin Tibor Schirrmeister. Convolutional neural networks for movement decoding from eeg signals. Master's thesis, Albert-Ludwigs-Universität Freiburg, 2015.">Schirrmeister, 2015</a>]</span>. The shallow network still resembles filter bank common spatial patterns, but less closely than the filterbank network. After validating that these initial network architectures perform as well as filter bank common spatial patterns, we progressed to developing and evaluating more generic architectures.</p>
<p>In this section, I describe the architectures presented in our first publication on EEG deep learning decoding <span id="id2">[<a class="reference internal" href="Abstract.html#id34" title="Robin Tibor Schirrmeister, Jost Tobias Springenberg, Lukas Dominique Josef Fiederer, Martin Glasstetter, Katharina Eggensperger, Michael Tangermann, Frank Hutter, Wolfram Burgard, and Tonio Ball. Deep learning with convolutional neural networks for eeg decoding and visualization. Human Brain Mapping, aug 2017. URL: http://dx.doi.org/10.1002/hbm.23730, doi:10.1002/hbm.23730.">Schirrmeister <em>et al.</em>, 2017</a>]</span>. This part uses text and figures from <span id="id3">[<a class="reference internal" href="Abstract.html#id34" title="Robin Tibor Schirrmeister, Jost Tobias Springenberg, Lukas Dominique Josef Fiederer, Martin Glasstetter, Katharina Eggensperger, Michael Tangermann, Frank Hutter, Wolfram Burgard, and Tonio Ball. Deep learning with convolutional neural networks for eeg decoding and visualization. Human Brain Mapping, aug 2017. URL: http://dx.doi.org/10.1002/hbm.23730, doi:10.1002/hbm.23730.">Schirrmeister <em>et al.</em>, 2017</a>]</span> adapted for readibility in this thesis.</p>
<div class="section" id="shallow-network-architecture">
<h3>Shallow Network Architecture<a class="headerlink" href="#shallow-network-architecture" title="Permalink to this headline">#</a></h3>
<div class="figure align-default" id="shallow-net-figure">
<a class="reference internal image-reference" href="_images/3D_Diagram_MatplotLib.ipynb.0.png"><img alt="_images/3D_Diagram_MatplotLib.ipynb.0.png" src="_images/3D_Diagram_MatplotLib.ipynb.0.png" style="width: 50%;" /></a>
<p class="caption"><span class="caption-number">Fig. 5 </span><span class="caption-text">Shallow network architecture, figure from <span id="id4">[<a class="reference internal" href="Abstract.html#id34" title="Robin Tibor Schirrmeister, Jost Tobias Springenberg, Lukas Dominique Josef Fiederer, Martin Glasstetter, Katharina Eggensperger, Michael Tangermann, Frank Hutter, Wolfram Burgard, and Tonio Ball. Deep learning with convolutional neural networks for eeg decoding and visualization. Human Brain Mapping, aug 2017. URL: http://dx.doi.org/10.1002/hbm.23730, doi:10.1002/hbm.23730.">Schirrmeister <em>et al.</em>, 2017</a>]</span>.  EEG input (at the top) is progressively transformed toward the bottom, until the final classifier output. Black cuboids: inputs/feature maps; brown cuboids: convolution/pooling kernels. The corresponding sizes are indicated in black and brown, respectively. Note that the final dense layer operates only on a small remaining temporal dimension, making it similar to a regular convolutional layer.</span><a class="headerlink" href="#shallow-net-figure" title="Permalink to this image">#</a></p>
</div>
<p>We developed the shallow network architecture, a more flexible architecture than the filterbank network that also learns temporal filters on the input signal and on the later representation. Instead of bandpass-filtered signals, it is fed the raw signals as input.
The steps the architecture implements are as follows (also see <a class="reference internal" href="#shallow-net-figure"><span class="std std-numref">Fig. 5</span></a>):</p>
<ol class="simple">
<li><p><strong>Temporal Filtering</strong> Learnable temporal filters are indepedently convolved with the signals of each EEG electrode. Afterwards, the channel dimension of the network representation contains <span class="math notranslate nohighlight">\(\mathrm{electrodes} \cdot \mathrm{temporal~ filters}\)</span> channels.</p></li>
<li><p><strong>Spatial Filtering</strong> Combining spatial filtering with mixing the outputs of the temporal filters, the network-channel dimension is linearly transformed by learned weights to a smaller dimensionality for further preprocessing.</p></li>
<li><p><strong>Log Average Power</strong> The resulting feature timeseries are then squared, average-pooled and log-transformed, which allows the network to more easily learn log-power-based features. Unlike the filterbank network, the average pooling does not collapse the feature timeseries into one value per trial. So after these processing steps, still some temporal information about the timecourse of the variance throughout the trial can be preserved.</p></li>
<li><p><strong>Classifier</strong> The final classification layer transforms these feature timecourses into class probabilities using a linear transformation and a softmax function.</p></li>
</ol>
</div>
<div class="section" id="deep-network-architecture">
<h3>Deep Network Architecture<a class="headerlink" href="#deep-network-architecture" title="Permalink to this headline">#</a></h3>
<div class="figure align-default" id="deep-net-figure">
<a class="reference internal image-reference" href="_images/3D_Diagram_MatplotLib.ipynb.1.png"><img alt="_images/3D_Diagram_MatplotLib.ipynb.1.png" src="_images/3D_Diagram_MatplotLib.ipynb.1.png" style="width: 75%;" /></a>
<p class="caption"><span class="caption-number">Fig. 6 </span><span class="caption-text">Deep network architecture, figure from <span id="id5">[<a class="reference internal" href="Abstract.html#id34" title="Robin Tibor Schirrmeister, Jost Tobias Springenberg, Lukas Dominique Josef Fiederer, Martin Glasstetter, Katharina Eggensperger, Michael Tangermann, Frank Hutter, Wolfram Burgard, and Tonio Ball. Deep learning with convolutional neural networks for eeg decoding and visualization. Human Brain Mapping, aug 2017. URL: http://dx.doi.org/10.1002/hbm.23730, doi:10.1002/hbm.23730.">Schirrmeister <em>et al.</em>, 2017</a>]</span>. Conventions as in <a class="reference internal" href="#shallow-net-figure"><span class="std std-numref">Fig. 5</span></a>.</span><a class="headerlink" href="#deep-net-figure" title="Permalink to this image">#</a></p>
</div>
<p>The deep architecture is a more generic architecture, closer to network architectures used in computer vision, see  <a class="reference internal" href="#deep-net-figure"><span class="std std-numref">Fig. 6</span></a> for a schematic overview. The first two temporal convolution and spatial filtering layers are the same in the shallow network, which is followed by a ELU nonlinearity (ELUs, <span class="math notranslate nohighlight">\(f(x)=x\)</span> for <span class="math notranslate nohighlight">\(x &gt; 0\)</span> and <span class="math notranslate nohighlight">\(f(x) = e^x-1\)</span> for <span class="math notranslate nohighlight">\(x &lt;= 0\)</span> <span id="id6">[<a class="reference internal" href="Abstract.html#id217" title="Djork-Arné Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs). In ArXiv e-prints, volume 1511, arXiv:1511.07289. 2016. URL: http://adsabs.harvard.edu/abs/2015arXiv151107289C (visited on 2016-12-21).">Clevert <em>et al.</em>, 2016</a>]</span>) and max pooling. The following three blocks simply consist of a convolution, a ELU nonlinearity and a max pooling. In the end, there is again a final linear classification layer with a softmax function. Due to its less specific and more generic computational steps, the deep architecture should be able to capture a large variety of features. Hence, the learned features may also be less biased towards the amplitude features commonly used in task-related EEG decoding.</p>
</div>
<div class="section" id="residual-network">
<h3>Residual Network<a class="headerlink" href="#residual-network" title="Permalink to this headline">#</a></h3>
<div class="figure align-default" id="residual-net-figure">
<img alt="_images/residual_block.png" src="_images/residual_block.png" />
<p class="caption"><span class="caption-number">Fig. 7 </span><span class="caption-text">Residual block, Figure from <span id="id7">[<a class="reference internal" href="Abstract.html#id34" title="Robin Tibor Schirrmeister, Jost Tobias Springenberg, Lukas Dominique Josef Fiederer, Martin Glasstetter, Katharina Eggensperger, Michael Tangermann, Frank Hutter, Wolfram Burgard, and Tonio Ball. Deep learning with convolutional neural networks for eeg decoding and visualization. Human Brain Mapping, aug 2017. URL: http://dx.doi.org/10.1002/hbm.23730, doi:10.1002/hbm.23730.">Schirrmeister <em>et al.</em>, 2017</a>]</span>. “Residual block used in the ResNet architecture and as described in original paper (<span id="id8">[<a class="reference internal" href="Abstract.html#id110" title="Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image Recognition. arXiv:1512.03385 [cs], December 2015. arXiv: 1512.03385. URL: http://arxiv.org/abs/1512.03385 (visited on 2016-05-11).">He <em>et al.</em>, 2015</a>]</span>; see Fig. 2) with identity shortcut option A, except using ELU instead of ReLU nonlinearities.”</span><a class="headerlink" href="#residual-net-figure" title="Permalink to this image">#</a></p>
</div>
<table class="colwidths-auto docutils align-default" id="residual-architectures-table">
<caption><span class="caption-number">Table 4 </span><span class="caption-text">Residual network architecture hyperparameters. Number of kernels, kernel and output size for all subparts of the network. Output size is always time x height x channels. Note that channels here refers to input channels of a network layer, not to EEG channels; EEG channels are in the height dimension. Output size is only shown if it changes from the previous block. Second convolution and all residual blocks used ELU nonlinearities. Note that in the end we had seven outputs, i.e., predictions for the four classes, in the time dimension (<strong>7</strong>x1x4 final output size). In practice, when using cropped training as explained in the following chapter, we even had 424 predictions, and used the mean of these to predict the trial.</span><a class="headerlink" href="#residual-architectures-table" title="Permalink to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Layer/Block</p></th>
<th class="head"><p>Number of Kernels</p></th>
<th class="head"><p>Kernel Size</p></th>
<th class="head"><p>Output Size</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Input</p></td>
<td><p></p></td>
<td><p></p></td>
<td><p>1000x44x1</p></td>
</tr>
<tr class="row-odd"><td><p>Convolution (linear)</p></td>
<td><p>48</p></td>
<td><p>3x1</p></td>
<td><p>1000x44x48</p></td>
</tr>
<tr class="row-even"><td><p>Convolution (ELU)</p></td>
<td><p>48</p></td>
<td><p>1x44</p></td>
<td><p>1000x1x48</p></td>
</tr>
<tr class="row-odd"><td><p>ResBlock (ELU)</p></td>
<td><p>48</p></td>
<td><p>3x1</p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p>ResBlock (ELU)</p></td>
<td><p>48</p></td>
<td><p>3x1</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p>ResBlock (ELU)</p></td>
<td><p>96</p></td>
<td><p>3x1 (Stride 2x1)</p></td>
<td><p>500x1x96</p></td>
</tr>
<tr class="row-even"><td><p>ResBlock (ELU)</p></td>
<td><p>96</p></td>
<td><p>3x1</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p>ResBlock (ELU)</p></td>
<td><p>144</p></td>
<td><p>3x1 (Stride 2x1)</p></td>
<td><p>250x1x96</p></td>
</tr>
<tr class="row-even"><td><p>ResBlock (ELU)</p></td>
<td><p>144</p></td>
<td><p>3x1</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p>ResBlock (ELU)</p></td>
<td><p>144</p></td>
<td><p>3x1 (Stride 2x1)</p></td>
<td><p>125x1x96</p></td>
</tr>
<tr class="row-even"><td><p>ResBlock (ELU)</p></td>
<td><p>144</p></td>
<td><p>3x1</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p>ResBlock (ELU)</p></td>
<td><p>144</p></td>
<td><p>3x1 (Stride 2x1)</p></td>
<td><p>63x1x96</p></td>
</tr>
<tr class="row-even"><td><p>ResBlock (ELU)</p></td>
<td><p>144</p></td>
<td><p>3x1</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p>ResBlock (ELU)</p></td>
<td><p>144</p></td>
<td><p>3x1 (Stride 2x1)</p></td>
<td><p>32x1x96</p></td>
</tr>
<tr class="row-even"><td><p>ResBlock (ELU)</p></td>
<td><p>144</p></td>
<td><p>3x1</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p>ResBlock (ELU)</p></td>
<td><p>144</p></td>
<td><p>3x1 (Stride 2x1)</p></td>
<td><p>16x1x96</p></td>
</tr>
<tr class="row-even"><td><p>ResBlock (ELU)</p></td>
<td><p>144</p></td>
<td><p>3x1</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p>Mean Pooling</p></td>
<td><p></p></td>
<td><p>10x1</p></td>
<td><p>7x1x144</p></td>
</tr>
<tr class="row-even"><td><p>Convolution + Softmax</p></td>
<td><p>4</p></td>
<td><p>1x1</p></td>
<td><p>7x1x4</p></td>
</tr>
</tbody>
</table>
<p>We also developed a residual network (ResNet <span id="id9">[<a class="reference internal" href="Abstract.html#id110" title="Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image Recognition. arXiv:1512.03385 [cs], December 2015. arXiv: 1512.03385. URL: http://arxiv.org/abs/1512.03385 (visited on 2016-05-11).">He <em>et al.</em>, 2015</a>]</span>) for EEG decoding. Residual networks add the input of a residual computational block back to its output, and empirically this allows to stably train much deeper networks. We use the same residual blocks as the original paper, described in Figure <a class="reference internal" href="#residual-net-figure"><span class="std std-numref">Fig. 7</span></a>. Our ResNet used ELU activation functions throughout the network (same as the deep ConvNet) and also starts with a splitted temporal and spatial convolution (same as the deep and shallow ConvNets), followed by 14 residual blocks, mean pooling and a final softmax dense classification layer.</p>
<p>In total, the ResNet has 31 convolutional layers, a depth where ConvNets without residual blocks started to show problems converging in the original ResNet paper <span id="id10">[<a class="reference internal" href="Abstract.html#id110" title="Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image Recognition. arXiv:1512.03385 [cs], December 2015. arXiv: 1512.03385. URL: http://arxiv.org/abs/1512.03385 (visited on 2016-05-11).">He <em>et al.</em>, 2015</a>]</span>. In layers where the number of channels is increased, we padded the incoming feature map with zeros to match the new channel dimensionality for the shortcut, as in option A of the original paper <span id="id11">[<a class="reference internal" href="Abstract.html#id110" title="Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image Recognition. arXiv:1512.03385 [cs], December 2015. arXiv: 1512.03385. URL: http://arxiv.org/abs/1512.03385 (visited on 2016-05-11).">He <em>et al.</em>, 2015</a>]</span>. The overall architecture is also shown in <a class="reference internal" href="#residual-architectures-table"><span class="std std-numref">Table 4</span></a>.</p>
</div>
</div>
<span id="document-CroppedTraining"></span><div class="tex2jax_ignore mathjax_ignore section" id="cropped-training">
<span id="id1"></span><h2>Cropped Training<a class="headerlink" href="#cropped-training" title="Permalink to this headline">#</a></h2>
<p>In this chapter, we describe a training strategy called “cropped training” which addresses the problem of the relatively low number of training examples in typical EEG datasets. The goal of this strategy is to improve the performance of deep networks by training them on many sliding temporal windows within the data. This approach had been similarly used as spatial cropping in computer vision, where networks are trained on multiple cropped versions of images. We first describe the concept of regular, non-cropped training and then introduce cropped training on a conceptual level. Finally, we discuss how to implement this approach efficiently. Our aim is to demonstrate the effectiveness and computational efficiency of cropped training as a regularization technique for deep networks on EEG data.</p>
<div class="section" id="non-cropped-trialwise-training">
<h3>Non-Cropped/Trialwise Training<a class="headerlink" href="#non-cropped-trialwise-training" title="Permalink to this headline">#</a></h3>
<div class="figure align-default" id="trialwise-figure">
<img alt="_images/trialwise_explanation.png" src="_images/trialwise_explanation.png" />
<p class="caption"><span class="caption-number">Fig. 8 </span><span class="caption-text">Trialwise training example. An entire single trial is fed through the network and the network’s prediction is compared to the trial target to train the network.</span><a class="headerlink" href="#trialwise-figure" title="Permalink to this image">#</a></p>
</div>
<p>In the trialwise training of deep networks on EEG data, each example consists of the EEG signals from a single trial and its corresponding label. Due to the typically small size of EEG datasets, networks trained in this way may only be trained on a few hundred to a few thousand examples per subject. This is significantly fewer examples than those used to train networks in computer vision, where tens of thousands or even millions of images are commonly used.</p>
</div>
<div class="section" id="id2">
<h3>Cropped Training<a class="headerlink" href="#id2" title="Permalink to this headline">#</a></h3>
<div class="figure align-default" id="cropped-figure">
<img alt="_images/cropped_explanation.png" src="_images/cropped_explanation.png" />
<p class="caption"><span class="caption-number">Fig. 9 </span><span class="caption-text">Cropped training example. A compute window contains many temporal windows (crops) inside that are used as individual examples to train the network.</span><a class="headerlink" href="#cropped-figure" title="Permalink to this image">#</a></p>
</div>
<p>Cropped training increases the number of training examples by training on many crops, i.e., temporal windows, within the trial.  For example, in a 4-second trial, all possible 2-second windows within the trial could be used as “independent” examples. This  approach drastically increases the number of training examples, although many of the examples are highly overlapping. This can be seen as an extreme version of the method to use random crops of images that is used to train deep networks in computer vision. However, a naive implementation of cropped training would greatly increase the computational cost per epoch due to the highly increased number of examples. Thankfully, the high overlap between neighbouring crops can be exploited for a more efficient implementation.</p>
</div>
<div class="section" id="computationally-faster-cropped-training">
<h3>Computationally Faster Cropped Training<a class="headerlink" href="#computationally-faster-cropped-training" title="Permalink to this headline">#</a></h3>
<div class="figure align-default" id="cropped-naive-computation-figure">
<img alt="_images/Multiple_Prediction_Matplotlib_Graphics.ipynb.2.png" src="_images/Multiple_Prediction_Matplotlib_Graphics.ipynb.2.png" />
<p class="caption"><span class="caption-number">Fig. 10 </span><span class="caption-text">Naive cropped training toy example. Each possible length-5 crop is taken from the original length-7 trial and independently processed by the Conv-Conv-Linear projection network. All filter values of the network are assumed to be ones. Each crop is processed independently. The values in red are identical and unnecessarily computed independently for each crop.</span><a class="headerlink" href="#cropped-naive-computation-figure" title="Permalink to this image">#</a></p>
</div>
<div class="figure align-default" id="cropped-efficient-computation-figure">
<a class="reference internal image-reference" href="_images/Multiple_Prediction_Matplotlib_Graphics.ipynb.3.png"><img alt="_images/Multiple_Prediction_Matplotlib_Graphics.ipynb.3.png" src="_images/Multiple_Prediction_Matplotlib_Graphics.ipynb.3.png" style="width: 50%;" /></a>
<p class="caption"><span class="caption-number">Fig. 11 </span><span class="caption-text">Efficient cropped training.</span><a class="headerlink" href="#cropped-efficient-computation-figure" title="Permalink to this image">#</a></p>
</div>
<p>Cropped training can be implemented with substantially less computations by exploiting that highly overlapping crops result in highly overlapping intermediate network activations. By passing a group of neighbouring crops together to the network, we can reuse intermediate computations. See <a class="reference internal" href="#cropped-naive-computation-figure"><span class="std std-numref">Fig. 10</span></a> and  <a class="reference internal" href="#cropped-efficient-computation-figure"><span class="std std-numref">Fig. 11</span></a> for a concrete example of this speedup method. This idea had been used in the same way for dense predictions on images, e.g. for segmentation <span id="id3">[<a class="reference internal" href="Abstract.html#id177" title="A. Giusti, D. C. Cireşan, J. Masci, L. M. Gambardella, and J. Schmidhuber. Fast image scanning with deep max-pooling convolutional neural networks. In 2013 IEEE International Conference on Image Processing, 4034–4038. September 2013. doi:10.1109/ICIP.2013.6738831.">Giusti <em>et al.</em>, 2013</a>, <a class="reference internal" href="Abstract.html#id210" title="Fabian Nasse, Christian Thurau, and Gernot A. Fink. Face detection using gpu-based convolutional neural networks. In International Conference on Computer Analysis of Images and Patterns, 83–90. Springer, 2009. URL: http://link.springer.com/chapter/10.1007/978-3-642-03767-2_10 (visited on 2017-01-09).">Nasse <em>et al.</em>, 2009</a>, <a class="reference internal" href="Abstract.html#id139" title="Pierre Sermanet, David Eigen, Xiang Zhang, Michael Mathieu, Rob Fergus, and Yann LeCun. OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks. arXiv:1312.6229 [cs], December 2013. arXiv: 1312.6229. URL: http://arxiv.org/abs/1312.6229 (visited on 2016-08-12).">Sermanet <em>et al.</em>, 2013</a>, <a class="reference internal" href="Abstract.html#id181" title="E. Shelhamer, J. Long, and T. Darrell. Fully Convolutional Networks for Semantic Segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, PP(99):1–1, 2016. doi:10.1109/TPAMI.2016.2572683.">Shelhamer <em>et al.</em>, 2016</a>]</span>.</p>
<p>Efficient cropped training then results in the exact same predictions and training as if the neighbouring crops were passed separately through the network. This is only true for networks that either use left-padding or no padding at all to the input and the intermediate activations. In the deep and shallow network described here, we do not use any padding. In the residual network, we use padding, hence the training is not exactly identical to passing neighbouring crops separately, but we still found it to improve over trial-wise training.</p>
<p>The more efficient way to do cropped training introduces a new hyperparameter, the number of neighbouring crops that are decoded together. The larger this hyperparameter, the more computations are saved and the more speedup one gets (see <span id="id4">[<a class="reference internal" href="Abstract.html#id177" title="A. Giusti, D. C. Cireşan, J. Masci, L. M. Gambardella, and J. Schmidhuber. Fast image scanning with deep max-pooling convolutional neural networks. In 2013 IEEE International Conference on Image Processing, 4034–4038. September 2013. doi:10.1109/ICIP.2013.6738831.">Giusti <em>et al.</em>, 2013</a>]</span> for a more detailed speedup analysis on images). Larger numbers of neighbouring crops that are trained on simultanaeously require more memory and may also affect the training dynamics due to more neighbouring crops being in the same mini-batch. However, we did not find negative effects on the training dynamics from larger number of simultaneously decoded neighbouring crops, consistent with prior work in computer vision <span id="id5">[<a class="reference internal" href="Abstract.html#id181" title="E. Shelhamer, J. Long, and T. Darrell. Fully Convolutional Networks for Semantic Segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, PP(99):1–1, 2016. doi:10.1109/TPAMI.2016.2572683.">Shelhamer <em>et al.</em>, 2016</a>]</span>.</p>
</div>
</div>
<span id="document-PerturbationVisualization"></span><div class="tex2jax_ignore mathjax_ignore section" id="perturbation-visualization">
<span id="id1"></span><h2>Perturbation Visualization<a class="headerlink" href="#perturbation-visualization" title="Permalink to this headline">#</a></h2>
<p>What features the EEG-decoding ConvNet learns is not obvious and can be scientifically interesting. Through end-to-end training, the networks may learn a variety of features to solve their task, brain-signal features  or even non-brain-signal features, e.g., eye movements that correlate to a movement. The learned features may be already known from prior research on brain-signal decoding or represent novel features that had not been described in the literature. However, there is no straightforward way to find out what the deep networks have learned from the brain signals.</p>
<p>Therefore, we developed an input amplitude perturbation method to investigate in how far the deep networks learn to extract spectral amplitude features, which are very commonly used in many EEG decoding pipelines. For example, it is known that the amplitudes, for example of the alpha, beta and gamma bands, provide class-discriminative information for motor tasks <span id="id2">[<a class="reference internal" href="Abstract.html#id57" title="Tonio Ball, Evariste Demandt, Isabella Mutschler, Eva Neitzel, Carsten Mehring, Klaus Vogt, Ad Aertsen, and Andreas Schulze-Bonhage. Movement related activity in the high gamma range of the human EEG. NeuroImage, 41(2):302–310, June 2008. URL: http://www.sciencedirect.com/science/article/pii/S1053811908001717 (visited on 2015-07-15), doi:10.1016/j.neuroimage.2008.02.032.">Ball <em>et al.</em>, 2008</a>, <a class="reference internal" href="Abstract.html#id199" title="G Pfurtscheller. Central beta rhythm during sensorimotor activities in man. Electroencephalography and Clinical Neurophysiology, 51(3):253–264, March 1981. URL: http://www.sciencedirect.com/science/article/pii/0013469481901395 (visited on 2017-01-09), doi:10.1016/0013-4694(81)90139-5.">Pfurtscheller, 1981</a>, <a class="reference internal" href="Abstract.html#id148" title="G Pfurtscheller and A Aranibar. Evaluation of event-related desynchronization (ERD) preceding and following voluntary self-paced movement. Electroencephalography and Clinical Neurophysiology, 46(2):138–146, February 1979. URL: http://www.sciencedirect.com/science/article/pii/0013469479900634 (visited on 2017-01-09), doi:10.1016/0013-4694(79)90063-4.">Pfurtscheller and Aranibar, 1979</a>]</span>. Hence, it seems a priori very likely that the deep networks learn to extract such features and worthwhile to check whether they indeed do so.</p>
<div class="section" id="input-perturbation-network-prediction-correlation-map">
<h3>Input-perturbation network-prediction correlation map<a class="headerlink" href="#input-perturbation-network-prediction-correlation-map" title="Permalink to this headline">#</a></h3>
<div class="figure align-default" id="input-perturbation-overview-figure">
<a class="reference internal image-reference" href="_images/input-perturbation-overview.png"><img alt="_images/input-perturbation-overview.png" src="_images/input-perturbation-overview.png" style="width: 70%;" /></a>
<p class="caption"><span class="caption-number">Fig. 12 </span><span class="caption-text"><strong>Computation overview for input-perturbation network-prediction correlation map.</strong>  (a) Example spectral amplitude perturbation and resulting classification difference. Top: Spectral amplitude perturbation as used to perturb the trials. Bottom: unit-output difference between unperturbed and perturbed trials for the classification layer units before the softmax. (b) Input-perturbation network-prediction correlations and corresponding network correlation scalp map for alpha band. Left: Correlation coefficients between spectral amplitude perturbations for all frequency bins and differences of the unit outputs for the four classes (differences between unperturbed and perturbed trials) for one electrode. Middle: Mean of the correlation coefficients over the the alpha (7–13 Hz), beta (13–31 Hz) and gamma (71–91 Hz) frequency ranges. Right: An exemplary scalp map for the alpha band, where the color of each dot encodes the correlation of amplitude changes at that electrode and the corresponding prediction changes of the ConvNet. Negative correlations on the left sensorimotor hand/arm areas show an amplitude decrease in these areas leads to a prediction increase for the Hand (R) class, whereas positive correlations on the right sensorimotor hand/arm areas show an amplitude decrease leads to a prediction decrease for the Hand (R) class.</span><a class="headerlink" href="#input-perturbation-overview-figure" title="Permalink to this image">#</a></p>
</div>
<p>To investigate the causal effect of changes in power on the deep ConvNet, we correlated changes in ConvNet predictions with changes in amplitudes by perturbing the original trial amplitudes (see <a class="reference internal" href="#input-perturbation-overview-figure"><span class="std std-numref">Fig. 12</span></a> for an overview). Concretely, tbe visualization method performs the following steps:</p>
<ol class="simple">
<li><p>Transform all training trials into the frequency domain by a Fourier transformation</p></li>
<li><p>Randomly perturb the amplitudes by adding Gaussian noise (with mean 0 and variance 1) to them (phases were kept unperturbed)</p></li>
<li><p>Retransform perturbed trials to the time domain by the inverse Fourier transformation</p></li>
<li><p>Compute predictions of the deep ConvNet for these trials before and after the perturbation (predictions here refers to outputs of the ConvNet directly before the softmax activation)</p></li>
<li><p>Repeat this procedure with 400 perturbations sampled from aforementioned Gaussian distribution</p></li>
<li><p>Correlate the change in input amplitudes (i.e., the perturbation/noise we added) with the change in the ConvNet predictions.</p></li>
</ol>
<p>To ensure that the effects of our perturbations reflect the behavior of the ConvNet on realistic data, we also checked that the perturbed input does not cause the ConvNet to misclassify the trials (as can easily happen even from small perturbations, see <span id="id3">[<a class="reference internal" href="Abstract.html#id211" title="Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In International Conference on Learning Representations. 2014. URL: http://arxiv.org/abs/1312.6199.">Szegedy <em>et al.</em>, 2014</a>]</span>. For that, we computed accuracies on the perturbed trials. For all perturbations of the training sets of all subjects, accuracies stayed above 99.5% of the accuracies achieved with the unperturbed data.</p>
</div>
<div class="section" id="gradient-based-implementation">
<h3>Gradient-based implementation<a class="headerlink" href="#gradient-based-implementation" title="Permalink to this headline">#</a></h3>
<p>An simpler way to implement the idea of testing the sensitivity of the network to spectral amplitude features is through gradient-based analysis <a class="footnote-reference brackets" href="#krm" id="id4">1</a>. There, we directly compute the gradient of the output unit with respect to the amplitudes of all frequency bins of all electrodes of the original unperturbed trial. To practically implement this, one must first transform the time domain input signal into the frequency domain and to a amplitude/phase representation via the Fourier transform. Then, one can transform the amplitude/phase representation back to the time domain using the inverse Fourier Transform. Since the inverse Fourier transform is differentiable one can backpropagate the gradients from the output unit through the time domain input back to the amplitudes. The more the network behaves locally linear around the input, the closer the results of this variant would be to the original perturbation variant. It is computationally substantially faster as everything is computed in one forward-backward pass, without needing to iterate over many perturbations.</p>
</div>
<div class="section" id="interpretation-and-limitations">
<span id="perturbation-visualization-interpretation"></span><h3>Interpretation and limitations<a class="headerlink" href="#interpretation-and-limitations" title="Permalink to this headline">#</a></h3>
<p>The perturbation-based visualization reflects network behavior and one cannot directly draw inferences about the data-generating process from it. This is because a prediction change caused by an amplitude perturbation may reflect both learned task-relevant factors as well as learned noise correlations. For example, increasing the amplitude in the alpha frequency range at C4, an electrode on the right side, may increase the predicted probability for right hand movement. That would likely not be because the alpha amplitude actually increases at C4 during right hand movement, but because the amplitude <em>decreases</em> on C3 <em>and</em> is correlated between C3 and C4. Hence, first subtracting the C4 amplitude from the C3 amplitude and then decoding negative values of this computation as indicating right hand movement is a reasonable learned prediction function. And this learned prediction function would cause the amplitude-perturbation function to show that an alpha increase at C4 causes an increase in the predicted probability for right hand movement. For a more detailed discussion of this effect in the context of linear models, see <span id="id5">[<a class="reference internal" href="Abstract.html#id63" title="Stefan Haufe, Frank Meinecke, Kai Görgen, Sven Dähne, John-Dylan Haynes, Benjamin Blankertz, and Felix Bießmann. On the interpretation of weight vectors of linear models in multivariate neuroimaging. NeuroImage, 87:96–110, February 2014. URL: http://www.sciencedirect.com/science/article/pii/S1053811913010914 (visited on 2015-08-07), doi:10.1016/j.neuroimage.2013.10.067.">Haufe <em>et al.</em>, 2014</a>]</span>.</p>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="krm"><span class="brackets"><a class="fn-backref" href="#id4">1</a></span></dt>
<dd><p>This idea was suggested to us in personal communication by Klaus-Robert Müller</p>
</dd>
</dl>
</div>
</div>
<span id="document-MovementDecoding"></span><div class="tex2jax_ignore mathjax_ignore section" id="decoding-movement-related-brain-activity">
<span id="movement-related"></span><h2>Decoding Movement-Related Brain Activity<a class="headerlink" href="#decoding-movement-related-brain-activity" title="Permalink to this headline">#</a></h2>
<p>Movement-related decoding problems are among the most researched in EEG decoding and were hence our problem of choice for the first evaluation of deep learning on EEG. A typical movement-related experimental setting is that subjects receive a cue for a specific body part (e.g. right hand, feet, tongue, etc.) and either move (motor execution) or imagine to move (motor imagery) this body part. The EEG signals recorded during the imagined or executed movements then often contain patterns specific to the body part being moved or thought about. These patterns can then be decoded using machine learning. In the following, I will describe our study on movement-related EEG decoding using deep learning, mostly using content adapted from <span id="id1">Schirrmeister <em>et al.</em> [<a class="reference internal" href="Abstract.html#id34" title="Robin Tibor Schirrmeister, Jost Tobias Springenberg, Lukas Dominique Josef Fiederer, Martin Glasstetter, Katharina Eggensperger, Michael Tangermann, Frank Hutter, Wolfram Burgard, and Tonio Ball. Deep learning with convolutional neural networks for eeg decoding and visualization. Human Brain Mapping, aug 2017. URL: http://dx.doi.org/10.1002/hbm.23730, doi:10.1002/hbm.23730.">2017</a>]</span>.</p>
<div class="section" id="datasets">
<h3>Datasets<a class="headerlink" href="#datasets" title="Permalink to this headline">#</a></h3>
<div class="section" id="high-gamma-dataset">
<h4>High-Gamma Dataset<a class="headerlink" href="#high-gamma-dataset" title="Permalink to this headline">#</a></h4>
<p>Our High-Gamma Dataset is a 128-electrode dataset (of which we later only use 44 sensors covering the motor cortex)
obtained from 14 healthy subjects (6 female, 2 left-handed, age 27.2<span class="math notranslate nohighlight">\(\pm\)</span>3.6 (mean<span class="math notranslate nohighlight">\(\pm\)</span>std)) with roughly 1000 (963.1<span class="math notranslate nohighlight">\(\pm\)</span>150.9, mean<span class="math notranslate nohighlight">\(\pm\)</span>std) four-second trials of executed movements divided into 13 runs per subject. The four classes of movements were movements of either the left hand, the right hand, both feet,
and rest (no movement, but same type of visual cue as for the other classes). The training set consists of the approx. 880 trials of all runs except the last two runs, the test set of the approx. 160 trials of the last 2 runs. This dataset was acquired in an EEG lab optimized for non-invasive detection of high-frequency movement-related EEG components <span id="id2">[<a class="reference internal" href="Abstract.html#id57" title="Tonio Ball, Evariste Demandt, Isabella Mutschler, Eva Neitzel, Carsten Mehring, Klaus Vogt, Ad Aertsen, and Andreas Schulze-Bonhage. Movement related activity in the high gamma range of the human EEG. NeuroImage, 41(2):302–310, June 2008. URL: http://www.sciencedirect.com/science/article/pii/S1053811908001717 (visited on 2015-07-15), doi:10.1016/j.neuroimage.2008.02.032.">Ball <em>et al.</em>, 2008</a>, <a class="reference internal" href="Abstract.html#id223" title="F. Darvas, R. Scherer, J. G. Ojemann, R. P. Rao, K. J. Miller, and L. B. Sorensen. High gamma mapping using EEG. NeuroImage, 49(1):930–938, January 2010. URL: http://www.sciencedirect.com/science/article/pii/S1053811909009513 (visited on 2017-01-10), doi:10.1016/j.neuroimage.2009.08.041.">Darvas <em>et al.</em>, 2010</a>]</span>. Such high-frequency components in the range of approx. 60 to above 100 Hz are typically increased during movement execution and may contain useful movement-related information <span id="id3">[<a class="reference internal" href="Abstract.html#id169" title="N. E. Crone, D. L. Miglioretti, B. Gordon, and R. P. Lesser. Functional mapping of human sensorimotor cortex with electrocorticographic spectral analysis. II. Event-related synchronization in the gamma band. Brain, 121(12):2301–2315, December 1998. URL: https://academic.oup.com/brain/article/121/12/2301/371496/Functional-mapping-of-human-sensorimotor-cortex (visited on 2017-01-17), doi:10.1093/brain/121.12.2301.">Crone <em>et al.</em>, 1998</a>, <a class="reference internal" href="Abstract.html#id190" title="Jiří Hammer, Tobias Pistohl, Jörg Fischer, Pavel Kršek, Martin Tomášek, Petr Marusič, Andreas Schulze-Bonhage, Ad Aertsen, and Tonio Ball. Predominance of Movement Speed Over Direction in Neuronal Population Signals of Motor Cortex: Intracranial EEG Data and A Simple Explanatory Model. Cerebral Cortex (New York, NY), 26(6):2863–2881, June 2016. URL: http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4869816/ (visited on 2017-01-11), doi:10.1093/cercor/bhw033.">Hammer <em>et al.</em>, 2016</a>, <a class="reference internal" href="Abstract.html#id161" title="F. Quandt, C. Reichert, H. Hinrichs, H. J. Heinze, R. T. Knight, and J. W. Rieger. Single trial discrimination of individual finger movements on one hand: A combined MEG and EEG study. NeuroImage, 59(4):3316–3324, February 2012. URL: http://www.sciencedirect.com/science/article/pii/S1053811911013358 (visited on 2017-01-17), doi:10.1016/j.neuroimage.2011.11.053.">Quandt <em>et al.</em>, 2012</a>]</span>. Our technical EEG Setup comprised (1.) Active electromagnetic shielding: optimized for frequencies from DC - 10 kHz (-30 dB to -50 dB), shielded window, ventilation &amp; cable feedthrough (mrShield, CFW EMV-Consulting AG, Reute, CH) (2.) Suitable
amplifiers: high-resolution (24 bits/sample) and low-noise (\textless{}0.6  <span class="math notranslate nohighlight">\(\mu V\)</span> RMS 0.16–200 Hz, 1.5 <span class="math notranslate nohighlight">\(\mu V\)</span> RMS 0.16–3500 Hz), 5 kHz sampling rate (NeurOne, Mega Electronics Ltd, Kuopio, FI) (3.) actively shielded EEG caps: 128 channels (WaveGuard Original, ANT, Enschede, NL) and (4.) full optical decoupling: All devices are battery powered and communicate via optic fibers.</p>
<p>Subjects sat in a comfortable armchair in the dimly lit Faraday cabin. The contact impedance from electrodes to skin was typically reduced below 5 kOhm using electrolyte gel (SUPER-VISC, EASYCAP GmbH, Herrsching, GER) and blunt cannulas. Visual cues were presented using a monitor outside the cabin, visible through the shielded window. The distance between the display and the subjects’ eyes was approx. 1 m. A fixation point was attached at the center of the screen. The subjects
were instructed to relax, fixate the fixation mark and to keep as still as possible during the motor execution task. Blinking and swallowing was restricted to the inter-trial intervals. The electromagnetic shielding combined with the comfortable armchair, dimly lit Faraday cabin and the relatively long 3-4 second inter-trial intervals (see below) were used to minimize artifacts produced by the subjects during the trials.</p>
<p>The tasks were as following. Depending on the direction of a gray arrow that was shown on black background, the subjects had to repetitively clench their toes (downward arrow), perform sequential finger-tapping of their left (leftward arrow) or right (rightward arrow) hand, or relax (upward arrow). The movements were selected to require little proximal muscular activity while still being complex enough to keep subjects involved. Within the 4-s trials, the subjects performed
the repetitive movements at their own pace, which had to be maintained as long as the arrow was showing. Per run, 80 arrows were displayed for 4 s each, with 3 to 4 s of continuous random inter-trial interval. The order of presentation was pseudo-randomized, with all four arrows being shown every four trials. Ideally 13 runs were performed to collect 260 trials of each movement and rest. The stimuli were presented and the data recorded with BCI2000 <span id="id4">[<a class="reference internal" href="Abstract.html#id174" title="G. Schalk, D. J. McFarland, T. Hinterberger, N. Birbaumer, and J. R. Wolpaw. BCI2000: a general-purpose brain-computer interface (BCI) system. IEEE Transactions on Biomedical Engineering, 51(6):1034–1043, June 2004. doi:10.1109/TBME.2004.827072.">Schalk <em>et al.</em>, 2004</a>]</span>. The experiment was approved by the ethical committee of the University of Freiburg.</p>
</div>
<div class="section" id="bci-competition-iv-2a">
<h4>BCI Competition IV 2a<a class="headerlink" href="#bci-competition-iv-2a" title="Permalink to this headline">#</a></h4>
<p>The BCI competition IV dataset 2a is a 22-electrode EEG motor-imagery dataset, with 9 subjects and 2
sessions, each with 288 four-second trials of imagined movements per subject (movements of the left hand,
the right hand, the feet and the tongue), for details see <span id="id5">Brunner <em>et al.</em> [<a class="reference internal" href="Abstract.html#id206" title="C. Brunner, R. Leeb, G. Müller-Putz, A. Schlögl, and G. Pfurtscheller. BCI Competition 2008–Graz data set A. Institute for Knowledge Discovery (Laboratory of Brain-Computer Interfaces), Graz University of Technology, pages 136–142, 2008. URL: http://www.bbci.de/competition/iv/desc_2a.pdf (visited on 2017-01-09).">2008</a>]</span>. The training set consists of the 288 trials of
the first session, the test set of the 288 trials of the second session.</p>
</div>
<div class="section" id="bci-competition-iv-2b">
<h4>BCI Competition IV 2b<a class="headerlink" href="#bci-competition-iv-2b" title="Permalink to this headline">#</a></h4>
<p>The BCI competition IV dataset 2b is a 3-electrode EEG motor-imagery dataset with 9 subjects and 5 sessions of imagined movements of the left or the right hand, the latest 3 sessions include online feedback <span id="id6">[<a class="reference internal" href="Abstract.html#id233" title="R Leeb, C Brunner, GR Müller-Putz, A Schlögl, and G Pfurtscheller. BCI Competition 2008–Graz data set B. Graz University of Technology, Austria, 2008.">Leeb <em>et al.</em>, 2008</a>]</span>.
The training set consists of the approx. 400  trials of the first 3 sessions (408.9<span class="math notranslate nohighlight">\(\pm\)</span>13.7, mean<span class="math notranslate nohighlight">\(\pm\)</span>std), the test set consists of the approx. 320 trials (315.6<span class="math notranslate nohighlight">\(\pm\)</span>12.6, mean<span class="math notranslate nohighlight">\(\pm\)</span>std) of the last two sessions.</p>
</div>
</div>
<div class="section" id="preprocessing">
<h3>Preprocessing<a class="headerlink" href="#preprocessing" title="Permalink to this headline">#</a></h3>
<p>We only minimally preprocessed the data to allow the networks to extract as much information as possible while keeping the input distribution in a value range suitable for stable network training.</p>
<p>Concretely, our preprocessing steps were:</p>
<ol class="simple">
<li><p><strong>Remove outlier trials:</strong> Any trial where at least one channel had a value outside +- 800 mV was removed to ensure stable training.</p></li>
<li><p><strong>Channel selection:</strong> For the high-gamma dataset, we selected only the 44 sensors covering the motor cortex for faster and more  accurate motor decoding.</p></li>
<li><p><strong>High/bandpass (Optional) :</strong> Highpass signal to above 4 Hz. This should partially remove potentially informative eye components from the signal and ensure that the decoding relies more on brain signals. For the BCI competition datasets, in this step we bandpassed to 4-38 Hz as using only frequencies until ~38-40 Hz was commonly done in prior work in this dataset.</p></li>
<li><p><strong>Standardization:</strong> Exponential moving standardization as described below to make sure the input distribution value range is suitable for network training.</p></li>
</ol>
<p>Our electrode-wise exponential moving standardization computes exponential moving means and variances with a decay factor of 0.999  for each channel and used these to standardize the continuous data.
Formally,</p>
<p><span class="math notranslate nohighlight">\(x't = (x_t - \mu_t) / \sqrt{\sigma_t^2}\)</span></p>
<p><span class="math notranslate nohighlight">\(\mu_t = 0.001 x_t + 0.999\mu_{t-1}\)</span></p>
<p><span class="math notranslate nohighlight">\(\sigma_t^2 = 0.001(x_t - \mu_t)^2 + 0.999 \sigma_{t-1}^2\)</span></p>
<p>where <span class="math notranslate nohighlight">\(x't\)</span> and <span class="math notranslate nohighlight">\(x_t\)</span> are the standardized and the original signal for one electrode at time <span class="math notranslate nohighlight">\(t\)</span>, respectively. As starting values for these recursive formulas we set the first 1000 mean values <span class="math notranslate nohighlight">\(\mu_t\)</span> and first 1000 variance values <span class="math notranslate nohighlight">\(\sigma_t^2\)</span> to the mean and the variance of the first 1000 samples, which were always completely inside the training set (so we never used future test data in our preprocessing). Some form of standardization is a commonly used procedure for ConvNets; exponentially moving standardization has the advantage that it is also applicable for an online BCI.</p>
<p>For FBCSP, this standardization always worsened accuracies in preliminary experiments, so we did not use it. Overall, the minimal preprocessing without any manual feature extraction ensured our end-to-end pipeline could in  principle be applied to a large number of brain-signal decoding tasks as we validated later, see <a class="reference internal" href="Abstract.html#task-related"><span class="std std-ref">Generalization to Other Tasks</span></a>.</p>
</div>
<div class="section" id="training-details">
<h3>Training details<a class="headerlink" href="#training-details" title="Permalink to this headline">#</a></h3>
<p>As our optimization method, we used Adam <span id="id7">[<a class="reference internal" href="Abstract.html#id167" title="Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In Proceedings of the 3rd International Conference on Learning Representations (ICLR). 2014. URL: http://arxiv.org/abs/1412.6980 (visited on 2017-01-09).">Kingma and Ba, 2014</a>]</span> together with a specific early stopping method, as this consistently yielded good accuracy in preliminary experiments on the training set. Adam is a variant of stochastic gradient descent designed to work well with high-dimensional parameters, which makes it  suitable for optimizing the large number of parameters of a ConvNet <span id="id8">[<a class="reference internal" href="Abstract.html#id167" title="Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In Proceedings of the 3rd International Conference on Learning Representations (ICLR). 2014. URL: http://arxiv.org/abs/1412.6980 (visited on 2017-01-09).">Kingma and Ba, 2014</a>]</span>. The early stopping strategy that we use throughout these experiments, developed in the computer vision field <a class="footnote-reference brackets" href="#earlystoppingurl" id="id9">1</a>, splits the training set into a training and validation fold and stops the first phase of the training when validation accuracy does not improve for a predefined number of epochs. The training continues on the combined training and validation fold starting from the parameter values that led to the  best accuracies on the validation fold so far. The training ends when the loss function on the validation fold drops to the same value as the loss function on the training fold at the end of the first training phase (we do not continue training in a third phase as in the original description). Early stopping in general  allows training on different types of networks and datasets without choosing the number of training epochs by hand. Our specific strategy uses the entire training data while only training once. In our study, all reported accuracies have been determined on an independent test set.</p>
<p>Note that in later works we do not use this early stopping method anymore as we found training on the whole training set with a cosine learning rate schedule <span id="id10">[<a class="reference internal" href="Abstract.html#id7" title="Ilya Loshchilov and Frank Hutter. SGDR: stochastic gradient descent with warm restarts. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL: https://openreview.net/forum?id=Skq89Scxx.">Loshchilov and Hutter, 2017</a>]</span> to lead to better final decoding performance.</p>
</div>
<div class="section" id="design-choices">
<h3>Design Choices<a class="headerlink" href="#design-choices" title="Permalink to this headline">#</a></h3>
<table class="colwidths-auto docutils align-default" id="design-choices-table">
<caption><span class="caption-number">Table 5 </span><span class="caption-text">Evaluated design choices.</span><a class="headerlink" href="#design-choices-table" title="Permalink to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="text-align:left head"><p>Design aspect</p></th>
<th class="text-align:left head"><p>Our Choice</p></th>
<th class="text-align:left head"><p>Variants</p></th>
<th class="head"><p>Motivation</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:left"><p>Activation functions</p></td>
<td class="text-align:left"><p>ELU</p></td>
<td class="text-align:left"><p>Square, ReLU</p></td>
<td><p>We expected these choices to be sensitive to the type of feature (e.g., signal phase or power), as squaring and mean pooling results in mean power (given a zero-mean signal). Different features may play different roles in the low-frequency components vs the higher frequencies (see the section “Datasets and Preprocessing”).</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Pooling mode</p></td>
<td class="text-align:left"><p>Max</p></td>
<td class="text-align:left"><p>Mean</p></td>
<td><p>(see above)</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Regularization and intermediate normalization</p></td>
<td class="text-align:left"><p>Dropout + batch normalization + a new tied loss function (explanations see text)</p></td>
<td class="text-align:left"><p>Only batch normalization, only dropout, neither of both, nor tied loss</p></td>
<td><p>We wanted to investigate whether recent deep learning advances improve accuracies and check how much regularization is required.</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Factorized temporal convolutions</p></td>
<td class="text-align:left"><p>One 10 × 1 convolution per convolutional layer</p></td>
<td class="text-align:left"><p>Two 6 × 1 convolutions per convolutional layer</p></td>
<td><p>Factorized convolutions are used by other successful ConvNets [Szegedy et al., 2015]</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Splitted vs one-step convolution</p></td>
<td class="text-align:left"><p>Splitted convolution in first layer (see the section “Deep ConvNet for raw EEG signals”)</p></td>
<td class="text-align:left"><p>One-step convolution in first layer</p></td>
<td><p>Factorizing convolution into spatial and temporal parts may improve accuracies for the large number of EEG input channels (compared with three rgb color channels of regular image datasets).</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p><span id="id11">[<a class="reference internal" href="Abstract.html#id34" title="Robin Tibor Schirrmeister, Jost Tobias Springenberg, Lukas Dominique Josef Fiederer, Martin Glasstetter, Katharina Eggensperger, Michael Tangermann, Frank Hutter, Wolfram Burgard, and Tonio Ball. Deep learning with convolutional neural networks for eeg decoding and visualization. Human Brain Mapping, aug 2017. URL: http://dx.doi.org/10.1002/hbm.23730, doi:10.1002/hbm.23730.">Schirrmeister <em>et al.</em>, 2017</a>]</span></p></td>
<td class="text-align:left"><p></p></td>
<td class="text-align:left"><p></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<p>For the shallow and deep network, we evaluated how a number of design choices affect the final accuracies.</p>
<div class="section" id="tied-loss-function">
<h4>Tied Loss Function<a class="headerlink" href="#tied-loss-function" title="Permalink to this headline">#</a></h4>
<p>Our tied loss function penalizes the discrepancy between neighbouring predictions. Concretely, in this \textit{tied sample loss function}, we added the cross-entropy of two neighboring predictions to the usual loss
of of negative log likelihood of the labels.
So, denoting the predicted probabilties <span class="math notranslate nohighlight">\(p\big(l_k|f_k(X^j_{t..t+T'};\theta)\big)\)</span> for crop
<span class="math notranslate nohighlight">\(X^j_{t..t+T'}\)</span> with label <span class="math notranslate nohighlight">\(l_k\)</span> from time step <span class="math notranslate nohighlight">\(t\)</span> to <span class="math notranslate nohighlight">\(t+T'\)</span> by <span class="math notranslate nohighlight">\(p_{f,k}(X^j_{t..t+T'})\)</span>, the loss now also depends on the
predicted probabilties for the next crop  <span class="math notranslate nohighlight">\(p_{f,k}(X^j_{t..t+T'+1})\)</span> and is then:</p>
<p><span class="math notranslate nohighlight">\(\textrm{loss}\big(y^j, p_{f,k}(X^j_{t..t+T'})\big)=\sum_{k=1}^{K}-log\big(p_{f,k}(X^j_{t..t+T'})\big)\cdot \delta(y^j=l_k)
\quad + \quad \sum_{k=1}^{K}-log\big(p_{f,k}(X^j_{t..t+T'})\big) \cdot p_{f,k}(X^j_{t..t+T'+1})\)</span></p>
<p>This is meant to make the ConvNet focus on features which are stable for several neighboring input crops.</p>
</div>
</div>
<div class="section" id="results">
<h3>Results<a class="headerlink" href="#results" title="Permalink to this headline">#</a></h3>
<div class="section" id="validation-of-fbcsp-pipeline">
<h4>Validation of FBCSP Pipeline<a class="headerlink" href="#validation-of-fbcsp-pipeline" title="Permalink to this headline">#</a></h4>
<p>As a first step before moving to the evaluation of ConvNet decoding, we validated our FBCSP implementation, as this was the baseline we compared the ConvNets results against. To validate our FBCSP implementation, we compared its accuracies to those published in the literature for the BCI competition IV dataset 2a <span id="id12">[<a class="reference internal" href="Abstract.html#id117" title="S. Sakhavi, C. Guan, and S. Yan. Parallel convolutional-linear neural network for motor imagery classification. In Signal Processing Conference (EUSIPCO), 2015 23rd European, 2736–2740. August 2015. doi:10.1109/EUSIPCO.2015.7362882.">Sakhavi <em>et al.</em>, 2015</a>]</span>. Using the same 0.5–2.5 s (relative to trial onset) time window, we reached an accuracy of 67.6%, statistically not significantly different from theirs (67.0%, p=0.73, Wilcoxon signed-rank test). Note however, that we used the full trial window for later experiments with convolutional networks, i.e., from 0.5–4 seconds. This yielded a slightly better accuracy of 67.8%, which was still not statistically significantly different from the original results on the 0.5–2.5 s window (p=0.73). For all later comparisons, we use the 0.5–4 seconds time window on all datasets.</p>
</div>
<div class="section" id="filterbank-network">
<h4>Filterbank Network<a class="headerlink" href="#filterbank-network" title="Permalink to this headline">#</a></h4>
<table class="colwidths-auto docutils align-default" id="filterbank-net-results">
<caption><span class="caption-number">Table 6 </span><span class="caption-text">Filterbank Net vs FBCSP Accuracies. Std is standard deviation over the 18 subjects used here.</span><a class="headerlink" href="#filterbank-net-results" title="Permalink to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Decoding Method</p></th>
<th class="head"><p>Sampling rate</p></th>
<th class="head"><p>Test Accuracy [%]</p></th>
<th class="head"><p>Std [%]</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>FBCSP</p></td>
<td><p>300</p></td>
<td><p>88.1</p></td>
<td><p>13.9</p></td>
</tr>
<tr class="row-odd"><td><p>FBCSP</p></td>
<td><p>150</p></td>
<td><p>86.7</p></td>
<td><p>14.3</p></td>
</tr>
<tr class="row-even"><td><p>Filterbank Net</p></td>
<td><p>300</p></td>
<td><p>90.5</p></td>
<td><p>10.4</p></td>
</tr>
<tr class="row-odd"><td><p>Filterbank Net</p></td>
<td><p>150</p></td>
<td><p>87.9</p></td>
<td><p>13.9</p></td>
</tr>
</tbody>
</table>
<p>Prior to our more extensive study, we had evaluated the filterbank network on a different version of the High-Gamma Dataset in a master thesis <span id="id13">[<a class="reference internal" href="Abstract.html#id2" title="Robin Tibor Schirrmeister. Convolutional neural networks for movement decoding from eeg signals. Master's thesis, Albert-Ludwigs-Universität Freiburg, 2015.">Schirrmeister, 2015</a>]</span>. This version of the dataset included different subjects, as some subjects had not been recorded yet and other subjects were later excluded for the work here due to the presence of too many artifacts. Furthermore, we evaluated 150Hz and 300 Hz as sampling rates here, in the remainder we will use 250 Hz.</p>
<p>The results in <a class="reference internal" href="#filterbank-net-results"><span class="std std-numref">Table 6</span></a> show that the Filterbank net outperformed FBCSP by 2.4% (300 Hz) and 1.3% (150 Hz) respectively. Despite the good performance, we did not evaluate this network further due to the very large GPU memory requirement of our implementation and our interest in evaluating more expressive architectures not as tightly constrained to implement FBCSP steps.</p>
</div>
<div class="section" id="convnets-reached-fbcsp-accuracies">
<h4>ConvNets reached FBCSP accuracies<a class="headerlink" href="#convnets-reached-fbcsp-accuracies" title="Permalink to this headline">#</a></h4>
<div class="figure align-default" id="movement-decoding-result-comparison-figure">
<a class="reference internal image-reference" href="_images/Final_Comparison.ipynb.2.png"><img alt="_images/Final_Comparison.ipynb.2.png" src="_images/Final_Comparison.ipynb.2.png" style="width: 70%;" /></a>
<p class="caption"><span class="caption-number">Fig. 13 </span><span class="caption-text"><strong>FBCSP vs. ConvNet decoding accuracies.</strong> Each small marker represents
accuracy of one subject, the large square markers represent average
accuracies across all subjects of both datasets. Markers above the
dashed line indicate experiments where ConvNets performed better than
FBCSP and opposite for markers below the dashed line. Stars indicate
statistically significant differences between FBCSP and ConvNets
(Wilcoxon signed-rank test, p&lt;0.05: *, p&lt;0.01: **,
p&lt;0.001=***). Bottom left of every plot: linear correlation
coefficient between FBCSP and ConvNet decoding accuracies. Mean
accuracies were very similar for ConvNets and FBCSP, the (small)
statistically significant differences were in direction of the ConvNets.</span><a class="headerlink" href="#movement-decoding-result-comparison-figure" title="Permalink to this image">#</a></p>
</div>
<p>Both the deep and the shallow ConvNets, with appropriate design choices (see <a class="reference internal" href="#design-choices-results"><span class="std std-ref">Design Choices affected decoding performance</span></a>), reached similar accuracies as FBCSP-based decoding, with small but statistically significant advantages for the ConvNets in some settings. For the mean of all subjects of both datasets, accuracies of the shallow ConvNet on <span class="math notranslate nohighlight">\(0-f_\textrm{end}\)</span> Hz and for the deep ConvNet on <span class="math notranslate nohighlight">\(4-f_\textrm{end}\)</span> Hz were not statistically significantly different from FBCSP (see <a class="reference internal" href="#movement-decoding-result-comparison-figure"><span class="std std-numref">Fig. 13</span></a>). The deep ConvNet on <span class="math notranslate nohighlight">\(0-f_\textrm{end}\)</span> Hz and the shallow ConvNet on <span class="math notranslate nohighlight">\(4-f_\textrm{end}\)</span> Hz reached slightly higher (1.9% and 3.3% higher, respectively) accuracies that were also statistically significantly different (P &lt; 0.05, Wilcoxon signed-rank test). Note that all results in this section were obtained with cropped training. Note that all P values below 0.01 in this study remain significant when controlled with false-discovery-rate correction at <span class="math notranslate nohighlight">\(\alpha=0.05\)</span> across all tests involving ConvNet accuracies.</p>
<div class="figure align-default" id="confusion-mat-figure">
<a class="reference internal image-reference" href="_images/Confusion_Mats.jpg"><img alt="_images/Confusion_Mats.jpg" src="_images/Confusion_Mats.jpg" style="width: 70%;" /></a>
<p class="caption"><span class="caption-number">Fig. 14 </span><span class="caption-text"><strong>Confusion matrices for FBCSP- and ConvNet-based decoding.</strong> Results are shown for the High-Gamma Dataset, on <span class="math notranslate nohighlight">\(0–f_\textrm{end}\)</span> Hz. Each entry of row r and column c for upper-left 4×4-square: Number of trials of target r predicted as class c (also written in percent of all trials). Bold diagonal corresponds to correctly predicted trials of the different classes. Percentages and colors indicate fraction of trials in this cell from all trials of the corresponding column (i.e., from all trials of the corresponding target class). The lower-right value corresponds to overall accuracy. Bottom row corresponds to sensitivity defined as the number of trials correctly predicted for class c/number of trials for class c. Rightmost column corresponds to precision defined as the number of trials correctly predicted for class r/number of trials predicted as class r. Stars indicate statistically significantly different values of ConvNet decoding from FBCSP, diamonds indicate statistically significantly different values between the shallow and deep ConvNets. P&lt;0.05: <span class="math notranslate nohighlight">\(\diamond\)</span>/*, P&lt;0.01: <span class="math notranslate nohighlight">\(\diamond\diamond\)</span>/**, P&lt;0.001: <span class="math notranslate nohighlight">\(\diamond\diamond\diamond\)</span>/***, Wilcoxon signed-rank test.</span><a class="headerlink" href="#confusion-mat-figure" title="Permalink to this image">#</a></p>
</div>
<table class="colwidths-auto docutils align-default" id="hgd-class-mistakes-table">
<caption><span class="caption-number">Table 7 </span><span class="caption-text">Decoding errors between class pairs. Results for the High-Gamma Dataset.  Number of trials where one class  was mistaken for the other for each decoding method, summed per class pair. The largest number of errors was between Hand(L) and Hand (R) for all three decoding methods, the second largest between Feet and Rest (on average across the three decoding methods). Together, these two class pairs accounted for more than 50% of all errors for all three decoding methods. In contrast, Hand (L and R) and Feet had a small number of errors irrespective of the decoding method used.</span><a class="headerlink" href="#hgd-class-mistakes-table" title="Permalink to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>Hand (L) / Hand (R)</p></th>
<th class="head"><p>Hand (L) / Feet</p></th>
<th class="head"><p>Hand (L) / Rest</p></th>
<th class="head"><p>Hand (R) / Feet</p></th>
<th class="head"><p>Hand (R) / Rest</p></th>
<th class="head"><p>Feet / Rest</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>FBCSP</p></td>
<td><p>82</p></td>
<td><p>28</p></td>
<td><p>31</p></td>
<td><p>3</p></td>
<td><p>12</p></td>
<td><p>42</p></td>
</tr>
<tr class="row-odd"><td><p>Deep</p></td>
<td><p>70</p></td>
<td><p>13</p></td>
<td><p>27</p></td>
<td><p>13</p></td>
<td><p>21</p></td>
<td><p>26</p></td>
</tr>
<tr class="row-even"><td><p>Shallow</p></td>
<td><p>99</p></td>
<td><p>3</p></td>
<td><p>34</p></td>
<td><p>5</p></td>
<td><p>37</p></td>
<td><p>73</p></td>
</tr>
</tbody>
</table>
<p>Confusion matrices for the High-Gamma Dataset on 0–<span class="math notranslate nohighlight">\(f_{end}\)</span> Hz were very similar for FBCSP and both ConvNets (see <a class="reference internal" href="#confusion-mat-figure"><span class="std std-numref">Fig. 14</span></a>). The majority of all mistakes were due to discriminating between Hand (L) / Hand (R) and Feet / Rest, see Table <a class="reference internal" href="#hgd-class-mistakes-table"><span class="std std-numref">Table 7</span></a>. Seven entries of the confusion matrix had a statistically significant difference (p&lt;0.05, Wilcoxon signed-rank test) between the deep and the shallow ConvNet, in all of them the deep ConvNet performed better. Only two differences between the deep ConvNet and FBCSP were statistically significant (p&lt;0.05), none for the shallow  ConvNet and FBCSP. Confusion matrices for the BCI competition IV dataset 2a showed a larger variability and hence a less consistent pattern, possibly because of the much smaller number of trials.</p>
</div>
</div>
<div class="section" id="design-choices-affected-decoding-performance">
<span id="design-choices-results"></span><h3>Design Choices affected decoding performance<a class="headerlink" href="#design-choices-affected-decoding-performance" title="Permalink to this headline">#</a></h3>
<div class="figure align-default" id="design-choices-a-fig">
<img alt="_images/Final_Comparison.ipynb.9.pdf-1.png" src="_images/Final_Comparison.ipynb.9.pdf-1.png" />
</div>
<div class="figure align-default" id="design-choices-b-fig">
<img alt="_images/Final_Comparison.ipynb.10.pdf-1.png" src="_images/Final_Comparison.ipynb.10.pdf-1.png" />
<p class="caption"><span class="caption-number">Fig. 15 </span><span class="caption-text">Impact of ConvNet design choices on decoding accuracy. Accuracy differences of baseline and design choices on x-axis for the <span class="math notranslate nohighlight">\(0-f_\textrm{end}\)</span> Hz and <span class="math notranslate nohighlight">\(4-f_\textrm{end}\)</span> Hz datasets. Each small marker represents accuracy difference for one subject, and each larger marker represents mean accuracy difference across all subjects of both datasets. Bars: standard error of the differences across subjects. Stars indicate statistically significant differences to baseline (Wilcoxon signed-rank test, P &lt; 0.05: <span class="math notranslate nohighlight">\(\diamond\)</span>*, P &lt; 0.01: <span class="math notranslate nohighlight">\(\diamond\diamond\)</span>**, P &lt; 0.001=***). Top: Impact of design choices applicable to both ConvNets. Shown are the effects from the removal of one aspect from the architecture on decoding accuracies. All statistically significant differences were accuracy decreases. Notably, there was a clear negative effect of removing both dropout and batch normalization, seen in both ConvNets’ accuracies and for both frequency ranges. Bottom: Impact of different types of nonlinearities, pooling modes and filter sizes. Results are given independently for the deep ConvNet and the shallow ConvNet. As before, all statistically significant differences were from accuracy decreases. Notably, replacing ELU by ReLU as nonlinearity led to decreases on both frequency ranges, which were both statistically significant.</span><a class="headerlink" href="#design-choices-b-fig" title="Permalink to this image">#</a></p>
</div>
<p>Design choices substantially affected deep network accuracies on both datasets, meaning BCI Competition IV 2a and the High Gamma Dataset. Batch normalization and dropout significantly increased accuracies. This became especially clear when omitting both simultaneously <a class="reference internal" href="#design-choices-b-fig"><span class="std std-numref">Fig. 15</span></a>. Batch normalization provided a larger accuracy increase for the shallow ConvNet, whereas dropout provided a larger increase for the deep ConvNet. For both networks and for both frequency bands, the only statistically significant accuracy differences were accuracy decreases after removing dropout for the deep ConvNet on <span class="math notranslate nohighlight">\(0-f_\textrm{end} \)</span> Hz data or removing batch normalization and dropout for both networks and frequency ranges (<span class="math notranslate nohighlight">\(p&lt;0.05\)</span>, Wilcoxon signed-rank test). Usage of tied loss did not affect the accuracies very much, never yielding statistically significant differences (<span class="math notranslate nohighlight">\(p&gt;0.05\)</span>). Splitting the first layer into two convolutions had the strongest accuracy increase on the <span class="math notranslate nohighlight">\(0-f_\textrm{end} \)</span> Hz data for the shallow ConvNet, where it is also the only statistically significant difference (<span class="math notranslate nohighlight">\(p&lt;0.01\)</span>).</p>
</div>
<div class="section" id="cropped-training-strategy-improved-deep-convnet-on-higher-frequencies">
<h3>Cropped training strategy improved deep ConvNet on higher frequencies<a class="headerlink" href="#cropped-training-strategy-improved-deep-convnet-on-higher-frequencies" title="Permalink to this headline">#</a></h3>
<div class="figure align-default" id="cropped-training-results-fig">
<img alt="_images/Final_Comparison.ipynb.8.pdf-1.png" src="_images/Final_Comparison.ipynb.8.pdf-1.png" />
<p class="caption"><span class="caption-number">Fig. 16 </span><span class="caption-text"><strong>Impact of training strategy (cropped vs trial-wise training) on accuracy.</strong> Accuracy difference for both frequency ranges and both ConvNets when using cropped training instead of trial-wise training. Other conventions as in <a class="reference internal" href="#design-choices-b-fig"><span class="std std-numref">Fig. 15</span></a>. Cropped training led to better accuracies for almost all subjects for the deep ConvNet on the <span class="math notranslate nohighlight">\(4-f_\textrm{end}\)</span>-Hz frequency range.</span><a class="headerlink" href="#cropped-training-results-fig" title="Permalink to this image">#</a></p>
</div>
<p>Cropped training increased accuracies statistically significantly for the deep ConvNet on the <span class="math notranslate nohighlight">\(4-f_\textrm{end}\)</span>-Hz data (p&lt;1e-5, Wilcoxon signed-rank test, see <a class="reference internal" href="#cropped-training-results-fig"><span class="std std-numref">Fig. 16</span></a>). In all other settings (<span class="math notranslate nohighlight">\(0-f_\textrm{end}\)</span>-Hz data, shallow ConvNet), the accuracy differences were not statistically
significant (p&gt;0.1) and showed a lot of variation between subjects.</p>
</div>
<div class="section" id="results-on-bci-competition-iv-2b">
<h3>Results on BCI Competition IV 2b<a class="headerlink" href="#results-on-bci-competition-iv-2b" title="Permalink to this headline">#</a></h3>
<table class="colwidths-auto docutils align-default" id="bcic-iv-2b-results">
<caption><span class="caption-number">Table 8 </span><span class="caption-text">Kappa values on the BCIC IV 2b dataset. ConvNet kappa values show the difference to the FBCSP kappa value.</span><a class="headerlink" href="#bcic-iv-2b-results" title="Permalink to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>FBCSP</p></th>
<th class="head"><p>Deep ConvNet</p></th>
<th class="head"><p>Shallow ConvNet</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0.599</p></td>
<td><p>−0.001</p></td>
<td><p>+0.030</p></td>
</tr>
</tbody>
</table>
<p>To ensure that the results also generalize to further datasets and also rule out hyperparameter overfitting, the FBCSP pipeline and the deep network pipelines were applied with the exact same hyperparameters on BCI Competition IV 2b. A few choices like the use of the decoding time window had been done after already seeing results from the evaluation sets of the High-Gamma dataset and the BCIC IV 2a dataset, hence it was valuable to validate the results on the BCIC IV 2b dataset. Results in <a class="reference internal" href="#bcic-iv-2b-results"><span class="std std-numref">Table 8</span></a> show that the networks perform as good or better than FBCSP. Results on further datasets, also non-movement-decoding datasets are presented in the next chapter.</p>
</div>
<div class="section" id="convnet-independent-visualizations">
<h3>ConvNet-independent visualizations<a class="headerlink" href="#convnet-independent-visualizations" title="Permalink to this headline">#</a></h3>
<div class="figure align-default" id="envelope-class-fig">
<img alt="_images/Envelope_Correlations.ipynb.1.pdf-1.png" src="_images/Envelope_Correlations.ipynb.1.pdf-1.png" />
<p class="caption"><span class="caption-number">Fig. 17 </span><span class="caption-text">Average over subjects from the High-Gamma Dataset. Colormaps are scaled per frequency band/row. This is a ConvNet-independent visualization. Scalp plots show spatial distributions of class-related spectral amplitude changes well in line with the literature.</span><a class="headerlink" href="#envelope-class-fig" title="Permalink to this image">#</a></p>
</div>
<p>Before moving to ConvNet visualization, we examined the spectral amplitude changes associated with the different movement classes in the alpha, beta and gamma frequency bands. For that, we first computed the moving average of the squared envelope in narrow frequency bands via the Hilbert transform as a measure of the power in those frequency bands.Then we computed linear correlations of these moving averages with the class label. This results in frequency-resolved envelope-class label correlations.</p>
<p>We found the expected overall scalp topographies (see Figure \ref{fig:results-spectral-topo}) to show physiologically plausible patterns. For example, for the alpha (7–13 Hz) frequency band, there was a class-related power decrease (anti-correlation in the class-envelope correlations) in the left and right pericentral regions with respect to the hand classes, stronger contralaterally to the side of the hand movement , i.e., the regions with pronounced power decreases lie around the primary sensorimotor hand representation areas. For the feet class, there was a power decrease located around the vertex, i.e.,  approx. above the primary motor foot area. As expected, opposite changes (power increases) with a similar topography were visible for the gamma band (71–91 Hz).</p>
</div>
<div class="section" id="amplitude-perturbation-visualizations">
<h3>Amplitude Perturbation Visualizations<a class="headerlink" href="#amplitude-perturbation-visualizations" title="Permalink to this headline">#</a></h3>
<div class="figure align-default" id="bandpower-perturbation-per-class-fig">
<img alt="_images/Bandpower_Perturbation.ipynb.0.pdf-1.png" src="_images/Bandpower_Perturbation.ipynb.0.pdf-1.png" />
<p class="caption"><span class="caption-number">Fig. 18 </span><span class="caption-text">Input-perturbation network-prediction correlations for all frequencies for the deep ConvNet, per class. Plausible correlations, for example, rest positively, other classes negatively correlated with the amplitude changes in frequency range from 20 to 30 Hz.</span><a class="headerlink" href="#bandpower-perturbation-per-class-fig" title="Permalink to this image">#</a></p>
</div>
<p>Our amplitude perturbation visualizations show that the network have learned to extract commonly used spectral amplitude features.We show three visualizations extracted from input-perturbation network-prediction correlations, the first two to show the frequency profile of the causal effects, the third to show their topography.  Thus, first, we computed the mean across electrodes for each class separately to show correlations between classes and frequency bands. We see plausible results, for example, for the rest class, positive correlations in the alpha and beta bands and negative correlations in the gamma band in <a class="reference internal" href="#bandpower-perturbation-per-class-fig"><span class="std std-numref">Fig. 18</span></a>.</p>
<div class="figure align-default" id="bandpower-overall-fig">
<img alt="_images/Bandpower_Perturbation.ipynb.12.pdf-1.png" src="_images/Bandpower_Perturbation.ipynb.12.pdf-1.png" />
<p class="caption"><span class="caption-number">Fig. 19 </span><span class="caption-text">Absolute input-perturbation network-prediction correlation frequency profile for the deep ConvNet. Mean absolute correlation value across classes. CSP binary decoding accuracies for different frequency bands for comparison, averaged across subjects and class pairs. Peaks in alpha, beta, and gamma band for input-perturbation network-prediction correlations and CSP accuracies.</span><a class="headerlink" href="#bandpower-overall-fig" title="Permalink to this image">#</a></p>
</div>
<p>Then, second, by taking the mean of the absolute values both over all classes and electrodes, we computed a general frequency profile. This showed clear peaks in the alpha, beta, and gamma bands (<a class="reference internal" href="#bandpower-overall-fig"><span class="std std-numref">Fig. 19</span></a>). Similar peaks were seen in the means of the CSP binary decoding accuracies for the same frequency range.</p>
<div class="figure align-default" id="bandpower-perturbation-topo-fig">
<img alt="_images/Bandpower_Perturbation.ipynb.3.pdf-1.png" src="_images/Bandpower_Perturbation.ipynb.3.pdf-1.png" />
<p class="caption"><span class="caption-number">Fig. 20 </span><span class="caption-text">Input-perturbation network-prediction correlation maps for the deep ConvNet. Correlation of class predictions and amplitude changes. Averaged over all subjects of the High-Gamma Dataset. Colormaps are scaled per scalp plot. Plausible scalp maps for all frequency bands, for example, contralateral positive correlations for the hand classes in the gamma band.</span><a class="headerlink" href="#bandpower-perturbation-topo-fig" title="Permalink to this image">#</a></p>
</div>
<p>Third, scalp maps of the input-perturbation effects on network predictions for the different frequency bands, as shown in <a class="reference internal" href="#bandpower-perturbation-topo-fig"><span class="std std-numref">Fig. 20</span></a>, show spatial distributions expected for motor tasks in the alpha, beta and—for the first time for such a noninvasive EEG decoding visualization—for the high gamma band. These scalp maps directly reflect the behavior of the ConvNets and one needs to be careful when making inferences about the data from them. For example, the positive correlation on the right side of the scalp for the Hand (R) class in the alpha band only means the ConvNet increased its prediction when the amplitude at these electrodes was increased independently of other frequency bands and electrodes. It does not imply that there was an increase of amplitude for the right hand class in the data. Rather, this correlation could be explained by the ConvNet reducing common noise between both locations, for more explanations of these effects in case of linear models, see <span id="id14">[<a class="reference internal" href="Abstract.html#id63" title="Stefan Haufe, Frank Meinecke, Kai Görgen, Sven Dähne, John-Dylan Haynes, Benjamin Blankertz, and Felix Bießmann. On the interpretation of weight vectors of linear models in multivariate neuroimaging. NeuroImage, 87:96–110, February 2014. URL: http://www.sciencedirect.com/science/article/pii/S1053811913010914 (visited on 2015-08-07), doi:10.1016/j.neuroimage.2013.10.067.">Haufe <em>et al.</em>, 2014</a>]</span>. Nevertheless, for the first time in noninvasive EEG, these maps clearly revealed the global somatotopic organization of causal contributions of motor cortical gamma band activity to decoding right and left hand and foot movements. Interestingly, these maps revealed highly focalized patterns, particularly during hand movement in the gamma frequency range (<a class="reference internal" href="#bandpower-perturbation-topo-fig"><span class="std std-numref">Fig. 20</span></a>, first plots in last row), in contrast to the more diffuse patterns in the conventional task-related spectral analysis as shown in <a class="reference internal" href="#envelope-class-fig"><span class="std std-numref">Fig. 17</span></a>.</p>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="earlystoppingurl"><span class="brackets"><a class="fn-backref" href="#id9">1</a></span></dt>
<dd><p><a class="reference external" href="https://web.archive.org/web/20160809230156/https://code.google.com/p/cuda-convnet/wiki/Methodology">https://web.archive.org/web/20160809230156/https://code.google.com/p/cuda-convnet/wiki/Methodology</a></p>
</dd>
</dl>
</div>
</div>
<span id="document-TaskDecoding"></span><div class="tex2jax_ignore mathjax_ignore section" id="generalization-to-other-tasks">
<span id="task-related"></span><h2>Generalization to Other Tasks<a class="headerlink" href="#generalization-to-other-tasks" title="Permalink to this headline">#</a></h2>
<p>After our initial work designing and evaluating convolutional neural networks for movement decoding from EEG, we evaluated the resulting networks on a wide variety of other EEG decoding tasks, finding that they generalize well to a large number of settings.</p>
<div class="section" id="decoding-different-mental-imageries">
<h3>Decoding different mental imageries<a class="headerlink" href="#decoding-different-mental-imageries" title="Permalink to this headline">#</a></h3>
<table class="colwidths-auto docutils align-default" id="mixed-imagery-dataset-results">
<caption><span class="caption-number">Table 9 </span><span class="caption-text">Accuracies on the Mixed-Imagery dataset. ConvNet accuracies show the difference to the FBCSP accuracy.</span><a class="headerlink" href="#mixed-imagery-dataset-results" title="Permalink to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>FBCSP</p></th>
<th class="head"><p>Deep ConvNet</p></th>
<th class="head"><p>Shallow ConvNet</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>71.2</p></td>
<td><p>+1.0</p></td>
<td><p>-3.5</p></td>
</tr>
<tr class="row-odd"><td><p><span id="id1">[<a class="reference internal" href="Abstract.html#id34" title="Robin Tibor Schirrmeister, Jost Tobias Springenberg, Lukas Dominique Josef Fiederer, Martin Glasstetter, Katharina Eggensperger, Michael Tangermann, Frank Hutter, Wolfram Burgard, and Tonio Ball. Deep learning with convolutional neural networks for eeg decoding and visualization. Human Brain Mapping, aug 2017. URL: http://dx.doi.org/10.1002/hbm.23730, doi:10.1002/hbm.23730.">Schirrmeister <em>et al.</em>, 2017</a>]</span></p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<p>The Mixed Imagery Dataset (MID) was obtained from 4 healthy subjects (3 female, all right-handed, age
26.75±5.9 (mean±std)) with a varying number of trials (S1: 675, S2: 2172, S3: 698, S4: 464) of imagined
movements (right hand and feet), mental rotation and mental word generation. All details were the same as
for the High Gamma Dataset, except: a 64-electrode subset of electrodes was used for recording, recordings
were not performed in the electromagnetically shielded cabin, thus possibly better approximating conditions
of real-world BCI usage, and trials varied in duration between 1 to 7 seconds. The dataset was analyzed
by cutting out time windows of 2 seconds with 1.5 second overlap from all trials longer than 2 seconds (S1:
6074 windows, S2: 21339, S3: 6197, S4: 4220), and both methods were evaluated using the accuracy of the
predictions for all the 2-second windows for the last two runs of roughly 130 trials (S1: 129, S2: 160, S3:
124, S4: 123).</p>
<p>For the mixed imagery dataset, we find the deep ConvNet to perform slightly better and the shallow ConvNet to perform slightly worse than the FBCSP algorithm, as can be seen in <a class="reference internal" href="#mixed-imagery-dataset-results"><span class="std std-numref">Table 9</span></a>.</p>
</div>
<div class="section" id="decoding-error-related-signals">
<h3>Decoding error-related signals<a class="headerlink" href="#decoding-error-related-signals" title="Permalink to this headline">#</a></h3>
<div class="section" id="decoding-observation-of-robots-making-errors">
<h4>Decoding Observation of Robots Making Errors<a class="headerlink" href="#decoding-observation-of-robots-making-errors" title="Permalink to this headline">#</a></h4>
<table class="colwidths-auto docutils align-default" id="robot-ball-results">
<caption><span class="caption-number">Table 10 </span><span class="caption-text">Accuracies for decoding watching of successful or unsuccessful robot-liquid pouring or ball-lifting.</span><a class="headerlink" href="#robot-ball-results" title="Permalink to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>robot task</p></th>
<th class="head"><p>time interval</p></th>
<th class="head"><p>Deep ConvNet</p></th>
<th class="head"><p>rLDA</p></th>
<th class="head"><p>FBCSP</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Pouring Liquid</p></td>
<td><p>2-5s</p></td>
<td><p>78.2 ± 8.4</p></td>
<td><p>67.5 ± 8.5</p></td>
<td><p>60.1 ± 3.7</p></td>
</tr>
<tr class="row-odd"><td><p>Pouring Liquid</p></td>
<td><p>3.3-7.5s</p></td>
<td><p>71.9 ± 7.6</p></td>
<td><p>63.0 ± 9.3</p></td>
<td><p>66.5 ± 5.7</p></td>
</tr>
<tr class="row-even"><td><p>Lifting Ball</p></td>
<td><p>4.8-6.3s</p></td>
<td><p>59.6 ± 6.4</p></td>
<td><p>58.1 ± 6.6</p></td>
<td><p>52.4 ± 2.8</p></td>
</tr>
<tr class="row-odd"><td><p>Lifting Ball</p></td>
<td><p>4-7s</p></td>
<td><p>64.6 ± 6.1</p></td>
<td><p>58.5 ± 8.2</p></td>
<td><p>53.1 ± 2.5</p></td>
</tr>
<tr class="row-even"><td><p><span id="id2">[<a class="reference internal" href="Abstract.html#id39" title="Joos Behncke, Robin T Schirrmeister, Wolfram Burgard, and Tonio Ball. The signature of robot action success in eeg signals of a human observer: decoding and visualization using deep convolutional neural networks. In 2018 6th international conference on brain-computer interface (BCI), 1–6. IEEE, 2018.">Behncke <em>et al.</em>, 2018</a>]</span></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<p>In this study, we aimed to classify whether a person had watched a video of a successful or an unsuccessful attempt of a robot performing one of two tasks (lifting a ball or pouring liquid) based on EEG recorded during the video observation. We compared the performance of our deep ConvNet to that of regularized linear discriminant analysis (rLDA) and FBCSP in this task. Our results, presented in <a class="reference internal" href="#robot-ball-results"><span class="std std-numref">Table 10</span></a>, demonstrate that the deep ConvNet outperformed the other methods for both tasks and both decoding intervals.</p>
</div>
<div class="section" id="decoding-of-eriksen-flanker-task-errors-and-errors-during-online-gui-control">
<span id="flanker-and-gui-section"></span><h4>Decoding of Eriksen Flanker Task Errors and Errors during Online GUI Control<a class="headerlink" href="#decoding-of-eriksen-flanker-task-errors-and-errors-during-online-gui-control" title="Permalink to this headline">#</a></h4>
<div class="sd-container-fluid sd-sphinx-override sd-mb-4 docutils">
<div class="sd-row sd-g-2 sd-g-xs-2 sd-g-sm-2 sd-g-md-2 sd-g-lg-2 docutils">
<div class="sd-col sd-d-flex-column docutils">
<div class="figure align-default" id="within-subject-flanker-gui-fig">
<a class="reference internal image-reference" href="_images/within-subject-flanker-gui.png"><img alt="_images/within-subject-flanker-gui.png" src="_images/within-subject-flanker-gui.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 21 </span><span class="caption-text">Comparison of within-subject decoding by rLDA and deep ConvNets. Error bars show the SEM. A) Eriksen flanker task (mean of 31 subjects), last 20% of subject data as test set. Deep ConvNets were 7.12% better than rLDA, pval = 6.24 *10-20 (paired t-test). B) Online GUI control (mean of 4 subjects), last session of each subject as test data <span id="id3">[<a class="reference internal" href="Abstract.html#id40" title="Martin Völker, Robin T Schirrmeister, Lukas DJ Fiederer, Wolfram Burgard, and Tonio Ball. Deep transfer learning for error decoding from non-invasive eeg. In 2018 6th International Conference on Brain-Computer Interface (BCI), 1–6. IEEE, 2018.">Völker <em>et al.</em>, 2018</a>]</span></span><a class="headerlink" href="#within-subject-flanker-gui-fig" title="Permalink to this image">#</a></p>
</div>
</div>
<div class="sd-col sd-d-flex-column docutils">
<div class="figure align-default" id="cross-subject-flanker-gui-fig">
<a class="reference internal image-reference" href="_images/cross-subject-flanker-gui.png"><img alt="_images/cross-subject-flanker-gui.png" src="_images/cross-subject-flanker-gui.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 22 </span><span class="caption-text">Mean normalized decoding accuracy on unknown subjects. Error bars show the SEM. A) Eriksen flanker task, trained on 30 subjects, tested on 1 subject.  Deep ConvNets were 5.05% better than rLDA, p = 3.16 *10-4 (paired t-test). B) Online GUI control. Trained on 3 subjects, tested on the  respective remaining subject. <span id="id4">[<a class="reference internal" href="Abstract.html#id40" title="Martin Völker, Robin T Schirrmeister, Lukas DJ Fiederer, Wolfram Burgard, and Tonio Ball. Deep transfer learning for error decoding from non-invasive eeg. In 2018 6th International Conference on Brain-Computer Interface (BCI), 1–6. IEEE, 2018.">Völker <em>et al.</em>, 2018</a>]</span></span><a class="headerlink" href="#cross-subject-flanker-gui-fig" title="Permalink to this image">#</a></p>
</div>
</div>
</div>
</div>
<p>In two addtional error-related decoding experiments, we evaluated an Eriksen flanker task and errors during an the online control of a graphical user interface through a brain-computer-interface. In the Eriksen flanker task, the subjects were asked to press the left or right button on a gamepad depending on whether an ‘L’ or an ‘R’ was the middle character of a 5-letter string displayed on the screen. For the online graphical user interface (GUI) control, the subjects were given an aim to reach using the GUI. They had to think of one of the classes of the aforementioned Mixed Imagery Dataset to choose one of four possible GUI actions. The correct GUI action was always determined by the specificed aim for the subject, hence an erroneous action could be detected. The decoding task in this paper was to distinguish whether the BCI-selected action was correct or erroneous. Results in <a class="reference internal" href="#within-subject-flanker-gui-fig"><span class="std std-numref">Fig. 21</span></a> and <a class="reference internal" href="#cross-subject-flanker-gui-fig"><span class="std std-numref">Fig. 22</span></a> show that deep ConvNets outperform rLDA in all settings except cross-subject error-decoding for online GUI control, where the low number of subjects (4) may prevent the ConvNets to learn enough to outperform rLDA.</p>
</div>
</div>
<div class="section" id="proof-of-concept-assistive-system">
<h3>Proof-of-concept assistive system<a class="headerlink" href="#proof-of-concept-assistive-system" title="Permalink to this headline">#</a></h3>
<div class="figure align-default" id="robot-bci-overview-fig">
<img alt="_images/robot-bci-overview.png" src="_images/robot-bci-overview.png" />
<p class="caption"><span class="caption-number">Fig. 23 </span><span class="caption-text">Overview of the proof-of-concept assistive system from <span id="id5">[<a class="reference internal" href="Abstract.html#id41" title="Felix Burget, Lukas Dominique Josef Fiederer, Daniel Kuhner, Martin Völker, Johannes Aldinger, Robin Tibor Schirrmeister, Chau Do, Joschka Boedecker, Bernhard Nebel, Tonio Ball, and others. Acting thoughts: towards a mobile robotic service assistant for users with limited communication skills. In 2017 European Conference on Mobile Robots (ECMR), 1–6. IEEE, 2017.">Burget <em>et al.</em>, 2017</a>]</span> using the deep ConvNet in the BCI component. Robotic arm could be given high-level commands via the BCI, high-level commands were extracted from a knowledge base. The commands were then autonomously planned and executed by the robotic arm.</span><a class="headerlink" href="#robot-bci-overview-fig" title="Permalink to this image">#</a></p>
</div>
<table class="colwidths-auto docutils align-default" id="bci-robot-results">
<caption><span class="caption-number">Table 11 </span><span class="caption-text">Results for BCI control of the GUI. Accuracy is fraction of correct commands, time is time per command, steps is steps needed to reach the aim, path optimality is ratio of miniminally needed  nubmer of steps to actually used number of steps when every step is optimal, and time/step is time per step.</span><a class="headerlink" href="#bci-robot-results" title="Permalink to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Subject</p></th>
<th class="head"><p>Runs</p></th>
<th class="head"><p>Accuracy* [%]</p></th>
<th class="head"><p>Time [s]</p></th>
<th class="head"><p>Steps</p></th>
<th class="head"><p>Path Optimality [%]</p></th>
<th class="head"><p>Time/Step [s]</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>S1</p></td>
<td><p>18</p></td>
<td><p>84.1 ± 6.1</p></td>
<td><p>125 ± 84</p></td>
<td><p>13.0 ± 7.8</p></td>
<td><p>70.1 ± 22.3</p></td>
<td><p>9 ± 2</p></td>
</tr>
<tr class="row-odd"><td><p>S2</p></td>
<td><p>14</p></td>
<td><p>76.8 ± 14.1</p></td>
<td><p>150 ± 32</p></td>
<td><p>10.1 ± 2.8</p></td>
<td><p>91.3 ± 12.0</p></td>
<td><p>9 ± 3</p></td>
</tr>
<tr class="row-even"><td><p>S3</p></td>
<td><p>17</p></td>
<td><p>82.0 ± 7.4</p></td>
<td><p>200 ± 159</p></td>
<td><p>17.6 ± 11.4</p></td>
<td><p>65.7 ± 28.9</p></td>
<td><p>11 ± 4</p></td>
</tr>
<tr class="row-odd"><td><p>S4</p></td>
<td><p>3</p></td>
<td><p>63.8 ± 15.6</p></td>
<td><p>176 ± 102</p></td>
<td><p>26.3 ± 11.2</p></td>
<td><p>34.5 ± 1.2</p></td>
<td><p>6 ± 2</p></td>
</tr>
<tr class="row-even"><td><p>Average</p></td>
<td><p>13</p></td>
<td><p>76.7 ± 9.1</p></td>
<td><p>148 ± 50</p></td>
<td><p>16.7 ± 7.1</p></td>
<td><p>65.4 ± 23.4</p></td>
<td><p>9 ± 2</p></td>
</tr>
<tr class="row-odd"><td><p><span id="id6">[<a class="reference internal" href="Abstract.html#id41" title="Felix Burget, Lukas Dominique Josef Fiederer, Daniel Kuhner, Martin Völker, Johannes Aldinger, Robin Tibor Schirrmeister, Chau Do, Joschka Boedecker, Bernhard Nebel, Tonio Ball, and others. Acting thoughts: towards a mobile robotic service assistant for users with limited communication skills. In 2017 European Conference on Mobile Robots (ECMR), 1–6. IEEE, 2017.">Burget <em>et al.</em>, 2017</a>]</span></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<p>We also evaluated the use of our deep ConvNet as part of an assistive robot system where the brain-computer interface was sending high-level commands to a robotic arm. In this proof-of-concept system, the robotic arm could be instructed by the user via the BCI to fetch a cup and directly move the cup to the persons mouth to drink from it. An overview can be seen in <a class="reference internal" href="#robot-bci-overview-fig"><span class="std std-numref">Fig. 23</span></a>. Results from <a class="reference internal" href="#bci-robot-results"><span class="std std-numref">Table 11</span></a> show that 3 out of 4 subjects had a command accuracy of more than 75% and were able to reach the target using less than twice the steps of the minimal path through the GUI (path optimality &gt; %50%).</p>
</div>
<div class="section" id="intracranial-eeg-decoding">
<h3>Intracranial EEG decoding<a class="headerlink" href="#intracranial-eeg-decoding" title="Permalink to this headline">#</a></h3>
<div class="section" id="intracranial-eeg-decoding-of-eriksen-flanker-task">
<h4>Intracranial EEG Decoding of Eriksen Flanker Task<a class="headerlink" href="#intracranial-eeg-decoding-of-eriksen-flanker-task" title="Permalink to this headline">#</a></h4>
<table class="colwidths-auto docutils align-default" id="intracranial-error-results-table">
<caption><span class="caption-number">Table 12 </span><span class="caption-text">Results for single-channel intracranial decoding of errors during an Eriksen flanker task. Balanced Accuracy is the mean of the accuracies for correct class ground truth labels and error class ground truth labels.</span><a class="headerlink" href="#intracranial-error-results-table" title="Permalink to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Classifier</p></th>
<th class="head"><p>Balanced Accuracy  [%]</p></th>
<th class="head"><p>Accuracy Correct Class [%]</p></th>
<th class="head"><p>Accuracy Error Class  [%]</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Deep4Net</p></td>
<td><p>59.28 ± 0.50</p></td>
<td><p>69.37 ± 0.44</p></td>
<td><p>49.19 ± 0.56</p></td>
</tr>
<tr class="row-odd"><td><p>ShallowNet</p></td>
<td><p>58.42 ± 0.32</p></td>
<td><p>74.83 ± 0.25</p></td>
<td><p>42.01 ± 0.40</p></td>
</tr>
<tr class="row-even"><td><p>EEGNet</p></td>
<td><p>57.73 ± 0.52</p></td>
<td><p>57.78 ± 0.48</p></td>
<td><p>57.68 ± 0.56</p></td>
</tr>
<tr class="row-odd"><td><p>rLDA</p></td>
<td><p>53.76 ± 0.32</p></td>
<td><p>76.12 ± 0.26</p></td>
<td><p>31.40 ± 0.38</p></td>
</tr>
<tr class="row-even"><td><p>ResNet</p></td>
<td><p>52.45 ± 0.21</p></td>
<td><p>95.47 ± 0.14</p></td>
<td><p>09.43 ± 0.28</p></td>
</tr>
<tr class="row-odd"><td><p><span id="id7">[<a class="reference internal" href="Abstract.html#id38" title="Martin Volker, Jiri Hammer, Robin T Schirrmeister, Joos Behncke, Lukas DJ Fiederer, Andreas Schulze-Bonhage, Petr Marusic, Wolfram Burgard, and Tonio Ball. Intracranial error detection via deep learning. In 2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC), 568–575. IEEE, 2018.">Volker <em>et al.</em>, 2018</a>]</span></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<div class="figure align-default" id="intracranial-error-results-fig">
<img alt="_images/IntracranialError.png" src="_images/IntracranialError.png" />
<p class="caption"><span class="caption-number">Fig. 24 </span><span class="caption-text">Results for all-channel intracranial decoding of errors during an Eriksen flanker task <span id="id8">[<a class="reference internal" href="Abstract.html#id38" title="Martin Volker, Jiri Hammer, Robin T Schirrmeister, Joos Behncke, Lukas DJ Fiederer, Andreas Schulze-Bonhage, Petr Marusic, Wolfram Burgard, and Tonio Ball. Intracranial error detection via deep learning. In 2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC), 568–575. IEEE, 2018.">Volker <em>et al.</em>, 2018</a>]</span>.</span><a class="headerlink" href="#intracranial-error-results-fig" title="Permalink to this image">#</a></p>
</div>
<p>We further evaluated whether the same networks developed for noninvasive EEG decoding can successfully learn to decode intracranial EEG. Therefore, in one work we used the same Eriksen flanker task as described in <a class="reference internal" href="#flanker-and-gui-section"><span class="std std-ref">Decoding of Eriksen Flanker Task Errors and Errors during Online GUI Control</span></a>, but recorded intracranial EEG from 23 patients who had pharmacoresistant epilepsy <span id="id9">[<a class="reference internal" href="Abstract.html#id38" title="Martin Volker, Jiri Hammer, Robin T Schirrmeister, Joos Behncke, Lukas DJ Fiederer, Andreas Schulze-Bonhage, Petr Marusic, Wolfram Burgard, and Tonio Ball. Intracranial error detection via deep learning. In 2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC), 568–575. IEEE, 2018.">Volker <em>et al.</em>, 2018</a>]</span>.</p>
</div>
<div class="section" id="transfer-learning-for-intracranial-error-decoding">
<h4>Transfer Learning for Intracranial Error Decoding<a class="headerlink" href="#transfer-learning-for-intracranial-error-decoding" title="Permalink to this headline">#</a></h4>
<div class="figure align-default" id="eriksen-flanker-car-driving-tasks-fig">
<img alt="_images/eriksen-flanker-car-driving-tasks.png" src="_images/eriksen-flanker-car-driving-tasks.png" />
<p class="caption"><span class="caption-number">Fig. 25 </span><span class="caption-text">Sketch of the Eriksen flanker task (A) and screenshot of the car driving task (B). <span id="id10">[<a class="reference internal" href="Abstract.html#id37" title="Joos Behncke, Robin Tibor Schirrmeister, Martin Volker, Jiri Hammer, Petr Marusic, Andreas Schulze-Bonhage, Wolfram Burgard, and Tonio Ball. Cross-paradigm pretraining of convolutional networks improves intracranial eeg decoding. In 2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC), 1046–1053. IEEE, 2018.">Behncke <em>et al.</em>, 2018</a>]</span>.</span><a class="headerlink" href="#eriksen-flanker-car-driving-tasks-fig" title="Permalink to this image">#</a></p>
</div>
<div class="figure align-default" id="cross-training-eft-cdt-results-fig">
<img alt="_images/cross-training-eft-cdt-results.png" src="_images/cross-training-eft-cdt-results.png" />
<p class="caption"><span class="caption-number">Fig. 26 </span><span class="caption-text">Results for transfer learning on the Eriksen flanker task (EFT) and the car driving task (CDT) <span id="id11">[<a class="reference internal" href="Abstract.html#id37" title="Joos Behncke, Robin Tibor Schirrmeister, Martin Volker, Jiri Hammer, Petr Marusic, Andreas Schulze-Bonhage, Wolfram Burgard, and Tonio Ball. Cross-paradigm pretraining of convolutional networks improves intracranial eeg decoding. In 2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC), 1046–1053. IEEE, 2018.">Behncke <em>et al.</em>, 2018</a>]</span>. All results are computed for a varying fraction of available data for the target decoding task (bottom row). <strong>A</strong> compares CDT accuracies after training only on CDT or pretraining on EFT and  finetuning on CDT. <strong>B</strong> compares EFT accuracies after only training on EFT or after  pretraining on CDT and finetuning on EFT. As a sanity check for the results in <strong>B</strong>, <strong>C</strong> compares EFT accuracies when pretraining on original CDT data and finetuning on EFT to pretraining on CDT data with shuffled labels (CDT*) and finetuning on EFT. Results show that pretraining on CDT helps EFT decoding when little EFT data is available.</span><a class="headerlink" href="#cross-training-eft-cdt-results-fig" title="Permalink to this image">#</a></p>
</div>
<p>We further tested the potential of ConvNets to transfer knowledge learned from decoding intracranial signals in error-decoding paradigm to decoding signals in another a different error-decoding paradigm <span id="id12">[<a class="reference internal" href="Abstract.html#id37" title="Joos Behncke, Robin Tibor Schirrmeister, Martin Volker, Jiri Hammer, Petr Marusic, Andreas Schulze-Bonhage, Wolfram Burgard, and Tonio Ball. Cross-paradigm pretraining of convolutional networks improves intracranial eeg decoding. In 2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC), 1046–1053. IEEE, 2018.">Behncke <em>et al.</em>, 2018</a>]</span>. The two error-decoding paradigms were the aforementioned Eriksen flanker task (EFT) and a car driving task (CDT), where subjects had to use a steering wheel to steer a car in a computer game and avoid hitting obstacles, where hitting an obstacle was considered an error event (see <a class="reference internal" href="#eriksen-flanker-car-driving-tasks-fig"><span class="std std-numref">Fig. 25</span></a>). Results in <a class="reference internal" href="#cross-training-eft-cdt-results-fig"><span class="std std-numref">Fig. 26</span></a> show that pretraining on CDT helps EFT decoding when few EDT data is available.</p>
</div>
<div class="section" id="microelectrocorticography-decoding-of-auditory-evoked-responses-in-sheep">
<h4>Microelectrocorticography decoding of auditory evoked responses in sheep<a class="headerlink" href="#microelectrocorticography-decoding-of-auditory-evoked-responses-in-sheep" title="Permalink to this headline">#</a></h4>
<div class="figure align-default" id="sheep-sounds-fig">
<img alt="_images/sheep-sounds.jpg" src="_images/sheep-sounds.jpg" />
<p class="caption"><span class="caption-number">Fig. 27 </span><span class="caption-text">Overview over decoding tasks for auditory evoked responses in a sheep <span id="id13">[<a class="reference internal" href="Abstract.html#id36" title="X. Wang, C. A. Gkogkidis, R. T. Schirrmeister, F. A. Heilmeyer, M. Gierthmuehlen, F. Kohler, M. Schuettler, T. Stieglitz, and T. Ball. Deep learning for micro-electrocorticographic (µecog) data. In 2018 IEEE-EMBS Conference on Biomedical Engineering and Sciences (IECBES), volume, 63-68. 2018. doi:10.1109/IECBES.2018.8626607.">Wang <em>et al.</em>, 2018</a>]</span>. First task (top) was to distingish 3 seconds when the sound was playing from the second before and the second after. Second task (bottom) was to distinguish the first, second and third second during theplaying of the sound. Signals are averaged responses from one electrode during different days, with black and grey being signals while the sheep was awake and red ones while the sheep was under general anesthesia.</span><a class="headerlink" href="#sheep-sounds-fig" title="Permalink to this image">#</a></p>
</div>
<div class="figure align-default" id="sheep-accuracies-fig">
<img alt="_images/sheep-accuracies.png" src="_images/sheep-accuracies.png" />
<p class="caption"><span class="caption-number">Fig. 28 </span><span class="caption-text">Results of decoding auditory evoked responses from sheep with rlDA and FBSCP or the deep ConvNet. Open circles represent accuracies for individual experiment days and closed circles represent the average over these accuracies.</span><a class="headerlink" href="#sheep-accuracies-fig" title="Permalink to this image">#</a></p>
</div>
<p>In this study, we evaluated the ConvNets for decoding auditory evoked responses played to a sheep that was chronically implanted with  a μECoG-based neural interfacing device <span id="id14">[<a class="reference internal" href="Abstract.html#id36" title="X. Wang, C. A. Gkogkidis, R. T. Schirrmeister, F. A. Heilmeyer, M. Gierthmuehlen, F. Kohler, M. Schuettler, T. Stieglitz, and T. Ball. Deep learning for micro-electrocorticographic (µecog) data. In 2018 IEEE-EMBS Conference on Biomedical Engineering and Sciences (IECBES), volume, 63-68. 2018. doi:10.1109/IECBES.2018.8626607.">Wang <em>et al.</em>, 2018</a>]</span>. 3-seconds-long sounds were presented to the sheep and two decoding tasks were defined from those 3 seconds as well as the second immediately before and after the playing of the sound. The first decoding task was to distinguish the 3 seconds when the sound was playing from the second  immediately before and the second immediately after the sound. The second task was distinguishing the first, second and third second of the playing of the sound to discriminate early, intermediate and late auditory evoked response (see <a class="reference internal" href="#sheep-sounds-fig"><span class="std std-numref">Fig. 27</span></a>). Results in <a class="reference internal" href="#sheep-accuracies-fig"><span class="std std-numref">Fig. 28</span></a> show that the  deep ConvNet can perform as good as FBSCP and rLDA, and perform well on both tasks, whereas rLDA performs competitively only on the first and FBSCP only on the second task.</p>
</div>
</div>
<div class="section" id="evaluation-on-large-scale-task-diverse-dataset">
<h3>Evaluation on large-scale task-diverse dataset<a class="headerlink" href="#evaluation-on-large-scale-task-diverse-dataset" title="Permalink to this headline">#</a></h3>
<table class="colwidths-auto docutils align-default" id="large-framework-overview-table">
<caption><span class="caption-number">Table 13 </span><span class="caption-text">Datasets for the large-scale evaluation framework.</span><a class="headerlink" href="#large-framework-overview-table" title="Permalink to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Name (Acronym)</p></th>
<th class="head"><p>#Classes</p></th>
<th class="head"><p>Task Type</p></th>
<th class="head"><p>#Subjects</p></th>
<th class="head"><p>Trials per Subject</p></th>
<th class="head"><p>Class balance</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>High-Gamma Dataset (Motor)</p></td>
<td><p>4</p></td>
<td><p>Motor task</p></td>
<td><p>20</p></td>
<td><p>1000</p></td>
<td><p>balanced</p></td>
</tr>
<tr class="row-odd"><td><p>KUKA Pouring Observation (KPO)</p></td>
<td><p>2</p></td>
<td><p>Error observation</p></td>
<td><p>5</p></td>
<td><p>720-800</p></td>
<td><p>balanced</p></td>
</tr>
<tr class="row-even"><td><p>Robot-Grasping Observation (RGO)</p></td>
<td><p>2</p></td>
<td><p>Error observation</p></td>
<td><p>12</p></td>
<td><p>720-800</p></td>
<td><p>balanced</p></td>
</tr>
<tr class="row-odd"><td><p>Error-Related Negativity (ERN)</p></td>
<td><p>2</p></td>
<td><p>Eriksen flanker task</p></td>
<td><p>31</p></td>
<td><p>1000</p></td>
<td><p>1/2 up to 1/15</p></td>
</tr>
<tr class="row-even"><td><p>Semantic Categories</p></td>
<td><p>3</p></td>
<td><p>Speech imagery</p></td>
<td><p>16</p></td>
<td><p>750</p></td>
<td><p>balanced</p></td>
</tr>
<tr class="row-odd"><td><p>Real vs. Pseudo Words</p></td>
<td><p>2</p></td>
<td><p>Speech imagery</p></td>
<td><p>16</p></td>
<td><p>1000</p></td>
<td><p>3/1</p></td>
</tr>
<tr class="row-even"><td><p><span id="id15">[<a class="reference internal" href="Abstract.html#id35" title="Felix A Heilmeyer, Robin T Schirrmeister, Lukas DJ Fiederer, Martin Volker, Joos Behncke, and Tonio Ball. A large-scale evaluation framework for eeg deep learning architectures. In 2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC), 1039–1045. IEEE, 2018.">Heilmeyer <em>et al.</em>, 2018</a>]</span></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<div class="figure align-default" id="large-framework-per-dataset-results-fig">
<img alt="_images/large-framework-per-dataset-results.png" src="_images/large-framework-per-dataset-results.png" />
<p class="caption"><span class="caption-number">Fig. 29 </span><span class="caption-text">Per-dataset results for the large-scale evaluation of deep ConvNet, shallow ConvNet and two versions of EEGNet. Boxplots show the distribution over per-subject accuracies for the individual decoding tasks. ern, kpo and rgo are the error-related datasets, ern: Error-related negativity Eriksen flanker task, KPO: KUKA Pouring Observation paradigm, rgo: robot-grasping observation paradigm. motor is the high-gamma dataset with 6 additional subjects that were excluded for data quality reasons from <span id="id16">[<a class="reference internal" href="Abstract.html#id34" title="Robin Tibor Schirrmeister, Jost Tobias Springenberg, Lukas Dominique Josef Fiederer, Martin Glasstetter, Katharina Eggensperger, Michael Tangermann, Frank Hutter, Wolfram Burgard, and Tonio Ball. Deep learning with convolutional neural networks for eeg decoding and visualization. Human Brain Mapping, aug 2017. URL: http://dx.doi.org/10.1002/hbm.23730, doi:10.1002/hbm.23730.">Schirrmeister <em>et al.</em>, 2017</a>]</span>. pseudovsreal and semantic are two semantic processing datasets to classify silent repetitions of  pseudowords vs. realwords (pseudovsreal) or different semantic categories (semantic) .</span><a class="headerlink" href="#large-framework-per-dataset-results-fig" title="Permalink to this image">#</a></p>
</div>
<div class="figure align-default" id="large-framework-averaged-results-fig">
<img alt="_images/large-framework-averaged-results.png" src="_images/large-framework-averaged-results.png" />
<p class="caption"><span class="caption-number">Fig. 30 </span><span class="caption-text">Dataset-averaged results for the large-scale evaluation of deep ConvNet, shallow ConvNet and two versions of EEGNet. Accuracies are normalized to the average of the accuracies of all models.</span><a class="headerlink" href="#large-framework-averaged-results-fig" title="Permalink to this image">#</a></p>
</div>
<table class="colwidths-auto docutils align-default" id="large-framework-results-table">
<caption><span class="caption-number">Table 14 </span><span class="caption-text">Dataset-averaged results for the large-scale evaluation of deep ConvNet, shallow ConvNet and two versions of EEGNet. Accuracies are normalized to the average of the accuracies of all models.</span><a class="headerlink" href="#large-framework-results-table" title="Permalink to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>Mean accuracy</p></th>
<th class="head"><p>Mean normalized accuracy</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Deep ConvNet</p></td>
<td><p>70.08% ± 20.92%</p></td>
<td><p>1.00 ± 0.05</p></td>
</tr>
<tr class="row-odd"><td><p>EEGNetv2</p></td>
<td><p>70.00% ±18.86%</p></td>
<td><p>1.02 ± 0.08</p></td>
</tr>
<tr class="row-even"><td><p>EEGNet</p></td>
<td><p>67.71% ± 19.04%</p></td>
<td><p>0.98 ± 0.06</p></td>
</tr>
<tr class="row-odd"><td><p>Shallow ConvNet</p></td>
<td><p>67.71% ±19.04%</p></td>
<td><p>0.99 ± 0.06</p></td>
</tr>
<tr class="row-even"><td><p><span id="id17">[<a class="reference internal" href="Abstract.html#id35" title="Felix A Heilmeyer, Robin T Schirrmeister, Lukas DJ Fiederer, Martin Volker, Joos Behncke, and Tonio Ball. A large-scale evaluation framework for eeg deep learning architectures. In 2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC), 1039–1045. IEEE, 2018.">Heilmeyer <em>et al.</em>, 2018</a>]</span></p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<p>We also compared the deep and shallow ConvNet architectures as well as EEGNet on six classification tasks with more than 90000 trials in total (see <a class="reference internal" href="#large-framework-overview-table"><span class="std std-numref">Table 13</span></a>) <span id="id18">[<a class="reference internal" href="Abstract.html#id35" title="Felix A Heilmeyer, Robin T Schirrmeister, Lukas DJ Fiederer, Martin Volker, Joos Behncke, and Tonio Ball. A large-scale evaluation framework for eeg deep learning architectures. In 2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC), 1039–1045. IEEE, 2018.">Heilmeyer <em>et al.</em>, 2018</a>]</span>. The datasets tasks were all recorded in our lab and included the high-gamma dataset, three error-related tasks described before (Eriksen flanker task, robot grasping and robot pouring observations) as well as two tasks on semantic processing. In the semantic processing dataset, the classification tasks were to distinguish different types of words that a subject silently repeated <span id="id19">[<a class="reference internal" href="Abstract.html#id33" title="V. Rau. Eeg correlates of inner speech. Bachelor's Thesis, University of Freiburg, DOI, 2015.">Rau, 2015</a>]</span>. The first task was to distinguish existing real words from nonexisting pseudowords. The second classification task was to distingiush three semantic categories (food, animals, tools) the word may belong to. The evaluation code for all models always used the original code and hyperparameters from the original studies in order to ensure a fair comparison. Results show that the deep ConvNet and the more recent version of EEGNet (EEGNetv2) perform similarly well, with shallow and an older version of EEGNet performing slightly worse, see  <a class="reference internal" href="#large-framework-per-dataset-results-fig"><span class="std std-numref">Fig. 29</span></a>, <a class="reference internal" href="#large-framework-averaged-results-fig"><span class="std std-numref">Fig. 30</span></a>  and <a class="reference internal" href="#large-framework-results-table"><span class="std std-numref">Table 14</span></a>.</p>
</div>
</div>
<span id="document-Pathology"></span><div class="tex2jax_ignore mathjax_ignore section" id="decoding-pathology">
<span id="pathology"></span><h2>Decoding Pathology<a class="headerlink" href="#decoding-pathology" title="Permalink to this headline">#</a></h2>
<p>We also evaluated our deep ConvNets for automatic medical diagnosis from EEG. EEG is important in clinical practice both as a screening method as well as for hypothesis-based diagnostics, e.g., in epilepsy or stroke. One of the main limitations of using EEG for diagnostics is the required time and specialized knowledge of experts that need to be well-trained on EEG diagnostics to reach reliable results. Therefore, a deep-learning approach that aids in the diagnostic process could make EEG diagnosis more widely accessible, reduce time and effort for clinicians and potentially make diagnoses more accurate. Text and figures in this section are adapted from <span id="id1">[<a class="reference internal" href="Abstract.html#id31" title="R. Schirrmeister, L. Gemein, K. Eggensperger, F. Hutter, and T. Ball. Deep learning with convolutional neural networks for decoding and visualization of eeg pathology. In 2017 IEEE Signal Processing in Medicine and Biology Symposium (SPMB), volume, 1-7. 2017. doi:10.1109/SPMB.2017.8257015.">Schirrmeister <em>et al.</em>, 2017</a>]</span>.</p>
<div class="section" id="dataset-and-preprocessing">
<h3>Dataset and Preprocessing<a class="headerlink" href="#dataset-and-preprocessing" title="Permalink to this headline">#</a></h3>
<div class="section" id="temple-university-hospital-eeg-abnormal-corpus">
<h4>Temple University Hospital EEG Abnormal Corpus<a class="headerlink" href="#temple-university-hospital-eeg-abnormal-corpus" title="Permalink to this headline">#</a></h4>
<table class="colwidths-auto docutils align-default" id="table-tuh-dataset">
<caption><span class="caption-number">Table 15 </span><span class="caption-text">TUH EEG Abnormal Corpus 1.1.2 Statistics. Obtained from <a class="reference external" href="https://www.isip.piconepress.com/projects/tuh_eeg/">https://www.isip.piconepress.com/projects/tuh_eeg/</a>. Rater agreements refer to the agreement between the student annotator of the file and the medical report written by a certified neurologist.</span><a class="headerlink" href="#table-tuh-dataset" title="Permalink to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p></p></th>
<th class="head"><p>Files</p></th>
<th class="head"><p>Patients</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Train</p></td>
<td><p>Normal</p></td>
<td><p>1379 (50%)</p></td>
<td><p>1238 (58%)</p></td>
</tr>
<tr class="row-odd"><td><p></p></td>
<td><p>Pathological</p></td>
<td><p>1361(50%)</p></td>
<td><p>894 (42%)</p></td>
</tr>
<tr class="row-even"><td><p></p></td>
<td><p>Rater Agreement</p></td>
<td><p>2704 (99%)</p></td>
<td><p>2107 (97%)</p></td>
</tr>
<tr class="row-odd"><td><p></p></td>
<td><p>Rater Disagreement</p></td>
<td><p>36 (1%)</p></td>
<td><p>25 (0%)</p></td>
</tr>
<tr class="row-even"><td><p>Evaluation</p></td>
<td><p>Normal</p></td>
<td><p>150 (54%)</p></td>
<td><p>148 (58%)</p></td>
</tr>
<tr class="row-odd"><td><p></p></td>
<td><p>Pathological</p></td>
<td><p>127 (46%)</p></td>
<td><p>105 (42%)</p></td>
</tr>
<tr class="row-even"><td><p></p></td>
<td><p>Rater Agreement</p></td>
<td><p>277 (100%)</p></td>
<td><p>253 (100%)</p></td>
</tr>
<tr class="row-odd"><td><p></p></td>
<td><p>Rater Disagreement</p></td>
<td><p>0 (0%)</p></td>
<td><p>0 (0%)</p></td>
</tr>
</tbody>
</table>
<p>We used the Temple University Hospital (TUH) EEG Abnormal Corpus for evaluating our deep ConvNets on pathology detection from EEG. The Temple University Hospital (TUH) EEG Abnormal Corpus 1.1.2 is a dataset of manually labeled normal and pathological clinical EEG recordings. It is taken from the TUH EEG Data Corpus which contains over 16000 clinical recordings of more than 10000 subjects from over 12 years <span id="id2">[<a class="reference internal" href="Abstract.html#id32" title="Iyad Obeid and Joseph Picone. The temple university hospital eeg data corpus. Frontiers in Neuroscience, 2016. URL: https://www.frontiersin.org/articles/10.3389/fnins.2016.00196, doi:10.3389/fnins.2016.00196.">Obeid and Picone, 2016</a>]</span>. The Abnormal Corpus contains 3017 recordings, 1529 of which were labeled normal and 1488 of which were labeled pathological. The Corpus was split into a training and evaluation set, see <a class="reference internal" href="#table-tuh-dataset"><span class="std std-numref">Table 15</span></a>. Recordings were acquired from  at least 21 standard electrode positions and with a sampling rate of in most cases 250 Hz. Per recording, there are around 20 minutes of EEG data. The inter-rater agreement on between the medical report of a certified neurologist and a medical student annotator was 99% for the training recordings and 100% for the evaluation recordings, also see <a class="reference internal" href="#table-tuh-dataset"><span class="std std-numref">Table 15</span></a>.</p>
</div>
<div class="section" id="preprocessing">
<h4>Preprocessing<a class="headerlink" href="#preprocessing" title="Permalink to this headline">#</a></h4>
<p>We minimally preprocessed the data with these steps:</p>
<ol class="simple">
<li><p>Select a subset of 21 electrodes present in all recordings.</p></li>
<li><p>Remove the first minute of each recording as it contained stronger artifacts.</p></li>
<li><p>Use only up to 20 minutes of the remaining recording to speed up the computations.</p></li>
<li><p>Clip the amplitude values to the range of <span class="math notranslate nohighlight">\(\pm800\)</span> <span class="math notranslate nohighlight">\(\mu V\)</span> to reduce the effects of strong artifacts.</p></li>
<li><p>Resample the data to 100 Hz to further speed up the computation.</p></li>
</ol>
</div>
<div class="section" id="decoding-from-reduced-eeg-time-segments">
<h4>Decoding from reduced EEG time segments<a class="headerlink" href="#decoding-from-reduced-eeg-time-segments" title="Permalink to this headline">#</a></h4>
<p>We also evaluated the ConvNets on reduced versions of the datasets, using only the first 1, 2, 4, 8, or 16 minutes after the first minute of the recording (the first minute of the recordings was always excluded because it appeared to be more prone to artifact contamination than the later time windows).
We reduced either only the training data, only the test data, or both.
These analyses were carried out to study how long EEG recordings need to be for training and for predicting EEG pathologies with good accuracies.</p>
</div>
</div>
<div class="section" id="network-architectures">
<h3>Network architectures<a class="headerlink" href="#network-architectures" title="Permalink to this headline">#</a></h3>
<p>We used our deep and shallow ConvNets with only minor modifications to the architecture. To use larger time windows to make a single prediction, we adapted the architectures by changing the final layer kernel length so the ConvNets have an input length of about 600 input samples, which correspond to 6 seconds for the 100 Hz EEG input. Additionally, we moved the pooling strides of the deep ConvNet to the convolutional layers directly before each pooling. This modification, which we initially considered a mistake, allowed us to grow the ConvNet input length without strongly increased computation times and
provided good accuracies in preliminary experiments on the training data; therefore we decided to keep it.</p>
</div>
<div class="section" id="automatic-architecture-optimization">
<h3>Automatic architecture optimization<a class="headerlink" href="#automatic-architecture-optimization" title="Permalink to this headline">#</a></h3>
<p>We also carried out a preliminary study of automatic architecture optimization to further improve our ConvNet architectures. To that end, we used the automatic hyperparameter optimization algorithm SMAC \cite{hutter_sequential_2011} to optimize architecture hyperparameters of the deep and shallow ConvNets, such as filter lengths, strides and types of nonlinearities. As the objective function to optimize via SMAC, we used 10-fold cross-validation performance obtained on the first 1500 recordings of the training data (using each fold as an instance for SMAC to speed up the optimization).
We set a time limit of 3.5 hours for each configuration run on a single fold. Runs that timed out or crashed (e.g., networks configurations that did not fit in GPU memory) were scored with an accuracy of 0%.</p>
</div>
<div class="section" id="deep-and-shallow-convnets-reached-state-of-the-art-results">
<h3>Deep and shallow ConvNets reached state-of-the-art results<a class="headerlink" href="#deep-and-shallow-convnets-reached-state-of-the-art-results" title="Permalink to this headline">#</a></h3>
<table class="colwidths-auto docutils align-default" id="pathology-convnet-results">
<caption><span class="caption-number">Table 16 </span><span class="caption-text">Decoding accuracies for discriminating normal and pathological EEG with deep and shallow ConvNets. For deep and shallow ConvNets, mean over five independent runs with different random seeds. Deep and shallow ConvNet outperformed the feature-based deep learning baseline.</span><a class="headerlink" href="#pathology-convnet-results" title="Permalink to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>Accuracy</p></th>
<th class="head"><p>Sensitivity</p></th>
<th class="head"><p>Specificity</p></th>
<th class="head"><p>Crop-accuracy</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Baseline <span id="id3">[<a class="reference internal" href="Abstract.html#id30" title="S. Lopez de Diego. Automated interpretation of abnormal adult electroencephalography. Master's thesis, Temple University, 2017.">Lopez de Diego, 2017</a>]</span></p></td>
<td><p>78.8</p></td>
<td><p>75.4</p></td>
<td><p>81.9</p></td>
<td><p>n.a.</p></td>
</tr>
<tr class="row-odd"><td><p>Deep</p></td>
<td><p>85.4</p></td>
<td><p>75.1</p></td>
<td><p>94.1</p></td>
<td><p>82.5</p></td>
</tr>
<tr class="row-even"><td><p>Shallow</p></td>
<td><p>84.5</p></td>
<td><p>77.3</p></td>
<td><p>90.5</p></td>
<td><p>81.7</p></td>
</tr>
<tr class="row-odd"><td><p>Linear</p></td>
<td><p>51.4</p></td>
<td><p>20.9</p></td>
<td><p>77.3</p></td>
<td><p>50.2</p></td>
</tr>
<tr class="row-even"><td><p><span id="id4">[<a class="reference internal" href="Abstract.html#id31" title="R. Schirrmeister, L. Gemein, K. Eggensperger, F. Hutter, and T. Ball. Deep learning with convolutional neural networks for decoding and visualization of eeg pathology. In 2017 IEEE Signal Processing in Medicine and Biology Symposium (SPMB), volume, 1-7. 2017. doi:10.1109/SPMB.2017.8257015.">Schirrmeister <em>et al.</em>, 2017</a>]</span></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p>Note that the baseline was evaluated on an older version of the corpus that has since been corrected to not contain the same patient in training and test recordings among other things.</p>
</div>
<p>Both the deep and the shallow ConvNet outperformed the only results that had been published on the TUH Abnormal EEG Corpus at the time (see <a class="reference internal" href="#pathology-convnet-results"><span class="std std-numref">Table 16</span></a>). Both ConvNets were more than 5% better than the baseline method of a convolutional network that included multiple fully connected layers at the end and took precomputed EEG features of an entire recording as one input <span id="id5">[<a class="reference internal" href="Abstract.html#id30" title="S. Lopez de Diego. Automated interpretation of abnormal adult electroencephalography. Master's thesis, Temple University, 2017.">Lopez de Diego, 2017</a>]</span>.The ConvNets as applied here reduced the error rate from about 21% to about 15%. We also tested a linear classifier on the same 6-second inputs as our ConvNets. The linear classifier did not reach accuracies substantially different from chance (51.4%).</p>
<p>Interestingly, both of our ConvNet architectures already reached higher accuracies than the baseline when evaluating single predictions from 6-second crops. The average per-crop accuracy of individual predictions was only about 3% lower than average per-recording accuracy (averaged predictions of all crops in a recording). Furthermore, the individual prediction accuracies were already about 3% higher than the per-recording accuracies of the baseline.
This implies that predictions with high accuracies can be made from just 6 seconds of EEG data.</p>
<div class="sd-container-fluid sd-sphinx-override sd-mb-4 docutils">
<div class="sd-row sd-row-cols-2 sd-row-cols-xs-2 sd-row-cols-sm-2 sd-row-cols-md-2 sd-row-cols-lg-2 sd-g-2 sd-g-xs-2 sd-g-sm-2 sd-g-md-2 sd-g-lg-2 docutils">
<div class="sd-col sd-d-flex-column docutils">
<div class="figure align-default" id="conf-mat-deep-pathology-fig">
<a class="reference internal image-reference" href="_images/ConfMatDeep.pdf-1.png"><img alt="_images/ConfMatDeep.pdf-1.png" src="_images/ConfMatDeep.pdf-1.png" style="width: 73%;" /></a>
<p class="caption"><span class="caption-number">Fig. 31 </span><span class="caption-text">Confusion Matrix Deep ConvNet.</span><a class="headerlink" href="#conf-mat-deep-pathology-fig" title="Permalink to this image">#</a></p>
</div>
</div>
<div class="sd-col sd-d-flex-column docutils">
<div class="figure align-default" id="conf-mat-shallow-pathology-fig">
<a class="reference internal image-reference" href="_images/ConfMatShallow.pdf-1.png"><img alt="_images/ConfMatShallow.pdf-1.png" src="_images/ConfMatShallow.pdf-1.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 32 </span><span class="caption-text">Confusion Matrix Shallow ConvNet.</span><a class="headerlink" href="#conf-mat-shallow-pathology-fig" title="Permalink to this image">#</a></p>
</div>
</div>
<div class="sd-col sd-d-flex-row sd-col-12 sd-col-xs-12 sd-col-sm-12 sd-col-md-12 sd-col-lg-12 docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body docutils">
<p class="sd-card-text">Confusion Matrices for deep and shallow ConvNets}, summed over five independent runs <span id="id6">[<a class="reference internal" href="Abstract.html#id31" title="R. Schirrmeister, L. Gemein, K. Eggensperger, F. Hutter, and T. Ball. Deep learning with convolutional neural networks for decoding and visualization of eeg pathology. In 2017 IEEE Signal Processing in Medicine and Biology Symposium (SPMB), volume, 1-7. 2017. doi:10.1109/SPMB.2017.8257015.">Schirrmeister <em>et al.</em>, 2017</a>]</span>.
Each entry of row r and column c for upper-left 2x2-square: Number of trials of target r predicted as class c (also written in percent of all trials).
Bold diagonal corresponds to correctly predicted trials for both classes. Percentages and colors indicate fraction of trials in each cell relative to all trials.
The lower-right value: overall accuracy. The first two values in the bottom row correspond to sensitivity and specificity.
Rightmost column corresponds to precision defined as the number of trials correctly predicted for class r/number of trials predicted as class r.</p>
</div>
</div>
</div>
</div>
</div>
<p>Both of our ConvNets made more errors on the pathological recordings, as can be seen from <a class="reference internal" href="#time-crop-pred-fig"><span class="std std-numref">Fig. 34</span></a>. Both ConvNets reached a specificity of above 90% and a sensitivity of about 75-78%. Confusion matrices between both approaches were very similar. Relative to the baseline, they reached a similar sensitivity (0.3% smaller for the deep ConvNet, 1.9% higher for the shallow ConvNet), and a higher specificity (12.2% higher for the deep ConvNet and 8.6% higher for the shallow ConvNet).</p>
<div class="figure align-default" id="pathology-time-fig">
<a class="reference internal image-reference" href="_images/Time_Plot.pdf-1.png"><img alt="_images/Time_Plot.pdf-1.png" src="_images/Time_Plot.pdf-1.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 33 </span><span class="caption-text">Results on reduced datasets for deep ConvNet. Train and/or test (evaluation) dataset was reduced from 20 minutes per recording to 1,2,4,8, or 16 minutes per recording, results are shown on the test set. Notably, when only reducing the duration of the test set recordings, maximal accuracies were observed  when using just 1 minute. We note that these results are each based on one run only; the slightly better performance than in Table \ref{tab:main-results} may thus be due to noise.</span><a class="headerlink" href="#pathology-time-fig" title="Permalink to this image">#</a></p>
</div>
<div class="figure align-default" id="time-crop-pred-fig">
<a class="reference internal image-reference" href="_images/Time_Crop_Pred_Plot.pdf-1.png"><img alt="_images/Time_Crop_Pred_Plot.pdf-1.png" src="_images/Time_Crop_Pred_Plot.pdf-1.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 34 </span><span class="caption-text">Moving average of cropwise accuracies for the deep ConvNet. 5-minute moving averages of the cropwise accuracies of the deep ConvNet, averaged over all test set recordings. Dashed lines represent 5 individual training runs with different random seeds, solid black line represents mean over results for these runs. x-axis shows center of 5-minute averaging window.</span><a class="headerlink" href="#time-crop-pred-fig" title="Permalink to this image">#</a></p>
</div>
<p>Deep ConvNets already reached their best trialwise accuracies with only one minute of data used for the prediction. While the reduction of the amount of length  of the training data led to crop- and trialwise accuracy decreases on the test data, reductions in the test data did not have such an effect (see <a class="reference internal" href="#pathology-time-fig"><span class="std std-numref">Fig. 33</span></a>). Remarkably, both crop- and trialwise accuracies slightly decreased when going from 1 minute to 2 or 4 minutes of test data. To investigate whether earlier parts of the recordings might be more informative, we also computed a 5-minute moving average of the cropwise accuracies on the test data for the Deep ConvNet trained on the full data. We show the average over all recordings for these moving averages in (see <a class="reference internal" href="#time-crop-pred-fig"><span class="std std-numref">Fig. 34</span></a>). Noticeably, as expected, accuracies slightly decreased with increasing recording time. However, the decrease is below 0.5% and thus should be interpreted cautiously.</p>
</div>
<div class="section" id="architecture-optimization-yielded-unexpected-new-models">
<h3>Architecture optimization yielded unexpected new models<a class="headerlink" href="#architecture-optimization-yielded-unexpected-new-models" title="Permalink to this headline">#</a></h3>
<div class="figure align-default" id="shallow-smac-net-fig">
<a class="reference internal image-reference" href="_images/ShallowSmacNet.pdf-1.png"><img alt="_images/ShallowSmacNet.pdf-1.png" src="_images/ShallowSmacNet.pdf-1.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 35 </span><span class="caption-text">Final shallow ConvNet architecture selected by SMAC. Note that max pooling is the only nonlinearity SMAC decided to use.</span><a class="headerlink" href="#shallow-smac-net-fig" title="Permalink to this image">#</a></p>
</div>
<table class="colwidths-auto docutils align-default" id="pathology-smac-results">
<caption><span class="caption-number">Table 17 </span><span class="caption-text">Decoding accuracies with the default  of deep and shallow ConvNets as well as versions optimized by automatic architecture optimization. Train here refers to 10-fold cross-validation on the 1500 chronologically earliest recordings of the training data.</span><a class="headerlink" href="#pathology-smac-results" title="Permalink to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p></p></th>
<th class="head"><p>Train</p></th>
<th class="head"><p></p></th>
<th class="head"><p>Test</p></th>
<th class="head"><p></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p></p></td>
<td><p>Architecture <br>configuration</p></td>
<td><p>Trial</p></td>
<td><p>Crop</p></td>
<td><p>Trial</p></td>
<td><p>Crop</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Deep</strong></p></td>
<td><p>Default</p></td>
<td><p>84.2</p></td>
<td><p>81.6</p></td>
<td><p>85.4</p></td>
<td><p>82.5</p></td>
</tr>
<tr class="row-even"><td><p></p></td>
<td><p>Optimized</p></td>
<td><p>86.3</p></td>
<td><p>80.9</p></td>
<td><p>84.5</p></td>
<td><p>81.3</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Shallow</strong></p></td>
<td><p>Default</p></td>
<td><p>84.5</p></td>
<td><p>82.1</p></td>
<td><p>84.5</p></td>
<td><p>81.7</p></td>
</tr>
<tr class="row-even"><td><p></p></td>
<td><p>Optimized</p></td>
<td><p>85.9</p></td>
<td><p>80.3</p></td>
<td><p>83.0</p></td>
<td><p>79.8</p></td>
</tr>
<tr class="row-odd"><td><p><span id="id7">[<a class="reference internal" href="Abstract.html#id31" title="R. Schirrmeister, L. Gemein, K. Eggensperger, F. Hutter, and T. Ball. Deep learning with convolutional neural networks for decoding and visualization of eeg pathology. In 2017 IEEE Signal Processing in Medicine and Biology Symposium (SPMB), volume, 1-7. 2017. doi:10.1109/SPMB.2017.8257015.">Schirrmeister <em>et al.</em>, 2017</a>]</span></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<p>The models discovered by automated architecture optimization were markedly different from our original deep and shallow ConvNets. For example, the optimized architectures used only 1.8 and 3.7 seconds of EEG data for the optimized deep and shallow ConvNet, respectively, in contrast to about 6 seconds in the original versions.
While the improved performance of these modified architectures for the 10-fold cross-validation on the training dataset (2.1% and 1.4% improvement for deep and shallow ConvNets, respectively) did not generalize to the evaluation set (0.9% and  1.5% deterioration for deep and shallow ConvNets, respectively, see <a class="reference internal" href="#pathology-smac-results"><span class="std std-numref">Table 17</span></a>, the modifications to the original network architectures already provided interesting insights for further exploration:
For example, in the case of the shallow ConvNet, the modified architecture did not use any of the original nonlinearities, but used max pooling as the only nonlinearity (see Fig. <a class="reference internal" href="#shallow-smac-net-fig"><span class="std std-numref">Fig. 35</span></a>), a configuration we had not considered in our  manual search so far.</p>
</div>
<div class="section" id="visualization">
<h3>Visualization<a class="headerlink" href="#visualization" title="Permalink to this headline">#</a></h3>
<p>We analyzed the spectral power changes in the data itself and the spectral characteristics of the function the deep networks learned on the data.</p>
<p>To understand class-specific spectral characteristics in the EEG recordings, we analyzed band powers in five frequency ranges: delta (0–4 Hz), theta (4–8 Hz), alpha (8–14 Hz), low beta (14–20 Hz), high beta (20–30 Hz) and low gamma (30–50 Hz).</p>
<p>For this, we performed the following steps:</p>
<ol class="simple">
<li><p>Compute a short-term Fourier transformation with window size 12 seconds and overlap 6 seconds using a Blackman-Harris window.</p></li>
<li><p>Compute the median over all band powers of all windows and recordings in each frequency bin; independently for pathological and normal recordings.</p></li>
<li><p>Compute the log ratio of these median band powers of the pathological and normal recordings.</p></li>
<li><p>Compute the mean log ratio over all frequency bins in each desired frequency range for each electrode.</p></li>
<li><p>Visualize the resulting log ratios as a topographical map.</p></li>
</ol>
<p>To better understand the spectral characteristics of the function the ConvNets learned used in this study, we also used the perturbation-based visualization method described in <span id="id8">[<a class="reference internal" href="Abstract.html#id34" title="Robin Tibor Schirrmeister, Jost Tobias Springenberg, Lukas Dominique Josef Fiederer, Martin Glasstetter, Katharina Eggensperger, Michael Tangermann, Frank Hutter, Wolfram Burgard, and Tonio Ball. Deep learning with convolutional neural networks for eeg decoding and visualization. Human Brain Mapping, aug 2017. URL: http://dx.doi.org/10.1002/hbm.23730, doi:10.1002/hbm.23730.">Schirrmeister <em>et al.</em>, 2017</a>]</span>.</p>
<div class="figure align-default" id="bandpower-pathology-fig">
<a class="reference internal image-reference" href="_images/Bandpower.pdf-1.png"><img alt="_images/Bandpower.pdf-1.png" src="_images/Bandpower.pdf-1.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 36 </span><span class="caption-text">Pathological vs. normal relative spectral bandpower differences for the training set. Shown is the logarithm of the ratio of the median bandpower of the pathological  vs. normal (according to the experts’ ratings) EEG recordings.</span><a class="headerlink" href="#bandpower-pathology-fig" title="Permalink to this image">#</a></p>
</div>
<div class="figure align-default" id="perturbation-deep-pathology-fig">
<a class="reference internal image-reference" href="_images/PerturbationDeep.pdf-1.png"><img alt="_images/PerturbationDeep.pdf-1.png" src="_images/PerturbationDeep.pdf-1.png" style="width: 100%;" /></a>
</div>
<div class="figure align-default" id="perturbation-shallow-pathology-fig">
<a class="reference internal image-reference" href="_images/PerturbationShallow.pdf-1.png"><img alt="_images/PerturbationShallow.pdf-1.png" src="_images/PerturbationShallow.pdf-1.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 37 </span><span class="caption-text">Input-perturbation network-prediction correlation maps for the deep (top) and shallow (bottom) ConvNet. Correlation of predictions for the pathological class with amplitude perturbations. Scalp maps revealed for example a bilateral positive correlation for the delta and theta frequency ranges and a spatially more broadly distributed negative correlation for the beta and low gamma frequency ranges, indicating that the ConvNets used these frequency components in their decisions</span><a class="headerlink" href="#perturbation-shallow-pathology-fig" title="Permalink to this image">#</a></p>
</div>
<p>Power was broadly increased for the the pathological class in the low frequency bands (delta and theta range) and decreased in the beta and low gamma ranges (<a class="reference internal" href="#bandpower-pathology-fig"><span class="std std-numref">Fig. 36</span></a>).
Alpha power was decreased for the occipital electrodes and increased for more frontal electrodes.</p>
<p>Scalp maps of the input-perturbation effects on predictions for the pathological class for the different frequency bands showed effects consistent with the power spectra in (<a class="reference internal" href="#perturbation-shallow-pathology-fig"><span class="std std-numref">Fig. 37</span></a>).
Both networks strongly relied on the lower frequencies in the delta and theta frequency range for their decoding decisions.</p>
</div>
<div class="section" id="analysis-of-word-frequencies-in-the-medical-reports">
<h3>Analysis of word frequencies in the medical reports<a class="headerlink" href="#analysis-of-word-frequencies-in-the-medical-reports" title="Permalink to this headline">#</a></h3>
<p>Furthermore, to better understand what kind of recordings are easier or harder for the ConvNets to correctly decode, we analyzed the textual clinical reports of each recording as included in the TUH Abnormal EEG Corpus. %todecide remove last part?
Specifically, we investigated which words were relatively more or less frequent in the incorrectly compared with the correctly predicted recordings.
We performed this analysis independently for both the normal and the pathological class of recordings.
Concretely, for each class, we first computed the relative frequencies <span class="math notranslate nohighlight">\(f_{i-}\)</span> for each word <span class="math notranslate nohighlight">\(w_{i-}\)</span> in the incorrectly predicted recordings, i.e.:
<span class="math notranslate nohighlight">\(f_{i-} = \frac{|w_{i-}|}{\sum_{i}|w_{i-}|}\)</span>, where <span class="math notranslate nohighlight">\(|w_{i-}|\)</span> denotes the number of occurrences for word <span class="math notranslate nohighlight">\(w_i\)</span> in the incorrectly predicted recordings.
We then computed the frequencies <span class="math notranslate nohighlight">\(f_{i+}\)</span> in the same way and computed the ratios <span class="math notranslate nohighlight">\(r_i=f_{i-}/f_{i+}\)</span>.
Finally, we analyzed words with very large ratios (<span class="math notranslate nohighlight">\(\gg1\)</span>) and very small ratios (<span class="math notranslate nohighlight">\(\ll1\)</span>) by inspecting the contexts of their occurrences in the  clinical reports.
This allowed us to gain insights into which clinical/contextual aspects of the recordings correlated with ConvNets failures.</p>
<p>Most notably, <code class="docutils literal notranslate"><span class="pre">small</span></code> and <code class="docutils literal notranslate"><span class="pre">amount</span></code> had a much larger word frequency (15.5 times larger) in the incorrectly predicted pathological recordings compared with the correctly predicted pathological recordings.
Closer inspection showed this is very sensible, as <code class="docutils literal notranslate"><span class="pre">small</span> <span class="pre">amount</span></code> was often used to describe more subtle EEG abnormalities (<code class="docutils literal notranslate"><span class="pre">small</span> <span class="pre">amount</span> <span class="pre">of</span> <span class="pre">temporal</span> <span class="pre">slowing</span></code>,  <code class="docutils literal notranslate"><span class="pre">Small</span> <span class="pre">amount</span> <span class="pre">of</span> <span class="pre">excess</span> <span class="pre">theta</span></code>,  <code class="docutils literal notranslate"><span class="pre">Small</span> <span class="pre">amount</span> <span class="pre">of</span> <span class="pre">background</span> <span class="pre">disorganization</span></code>,  <code class="docutils literal notranslate"><span class="pre">A</span> <span class="pre">small</span> <span class="pre">amount</span> <span class="pre">of</span> <span class="pre">rhythmic,</span> <span class="pre">frontal</span> <span class="pre">slowing</span></code>), as this subtlety of changes was likely the cause of the classification errors.</p>
<p>Secondly, other words with a notably different frequency were <code class="docutils literal notranslate"><span class="pre">age</span></code> (9.7 times larger) and <code class="docutils literal notranslate"><span class="pre">sleep</span></code> (3 occurrences in 630 words of texts of incorrectly predicted recordings, not present in texts of correctly predicted recordings).
Both typically indicate the clinician used the age of the subject or the fact that they were (partially) asleep during the recording to interpret the EEG (<code class="docutils literal notranslate"><span class="pre">Somewhat</span> <span class="pre">disorganized</span> <span class="pre">pattern</span> <span class="pre">for</span> <span class="pre">age</span></code>,  <code class="docutils literal notranslate"><span class="pre">Greater</span> <span class="pre">than</span> <span class="pre">anticipated</span> <span class="pre">disorganization</span> <span class="pre">for</span> <span class="pre">age.</span></code>,  <code class="docutils literal notranslate"><span class="pre">A</span> <span class="pre">single</span> <span class="pre">generalized</span> <span class="pre">discharge</span> <span class="pre">noted</span> <span class="pre">in</span> <span class="pre">stage</span> <span class="pre">II</span> <span class="pre">sleep.</span></code>).
Obviously, our ConvNets trained only on EEG do not have access to this context information, leaving them at a disadvantage compared to the clinicians and highlighting the potential of including contextual cues such as age or vigilance in the training/decoding approach.</p>
<p>Inspection of the textual records of misclassified normal recordings did not provide much insight, as they are typically very short (e.g., <code class="docutils literal notranslate"><span class="pre">Normal</span> <span class="pre">EEG.</span></code>,  <code class="docutils literal notranslate"><span class="pre">Normal</span> <span class="pre">EEG</span> <span class="pre">in</span> <span class="pre">wakefulness.</span></code>).</p>
<p>Finally, consistent with the strong usage of the delta and theta frequency range by the ConvNets as seen in the input-perturbation network-prediction correlation maps (<a class="reference internal" href="#perturbation-shallow-pathology-fig"><span class="std std-numref">Fig. 37</span></a>), <code class="docutils literal notranslate"><span class="pre">slowing</span></code> and <code class="docutils literal notranslate"><span class="pre">temporal</span></code> are the 6th and 10th most frequently occurring words in the textual reports of the pathological recordings, while never occurring in the textual reports of the normal recordings (irrespective of correct or incorrect predictions).</p>
</div>
<div class="section" id="discussion">
<h3>Discussion<a class="headerlink" href="#discussion" title="Permalink to this headline">#</a></h3>
<p>To the best of our knowledge, the ConvNet architectures used in this study achieved the best accuracies published so far on the TUH EEG Abnormal Corpus.
The architectures used were only very slightly modified versions of ConvNet architectures that we previously introduced to decode task-related information. This suggests that these architectures might be broadly applicable both for physiological and clinical EEG.
The identification of all-round architectures would greatly simplify the application of deep learning to EEG decoding problems and expand their potential use cases.</p>
<p>Remarkably, the ConvNets already reached good accuracies based on very limited time segments of the EEG recordings.
Further accuracy improvements could thus be possible with improved decoding models that can extract and integrate additional information from longer timescales.
The exact nature of such models, as well as the amount of EEG they would require, remains to be determined.
More accurate decoding models could either be ConvNets that are designed to intelligently use a larger input length or recurrent neural networks, since these are known to inherently work well for data with information both on shorter and longer term scales.
Furthermore, combinations between both approaches, for example using a recurrent neural network on top of a ConvNet, as they have been used in other domains like speech recognition <span id="id9">[<a class="reference internal" href="Abstract.html#id201" title="X. Li and X. Wu. Constructing long short-term memory based deep recurrent neural networks for large vocabulary speech recognition. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 4520–4524. April 2015. doi:10.1109/ICASSP.2015.7178826.">Li and Wu, 2015</a>, <a class="reference internal" href="Abstract.html#id202" title="T. N. Sainath, O. Vinyals, A. Senior, and H. Sak. Convolutional, Long Short-Term Memory, fully connected Deep Neural Networks. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 4580–4584. April 2015. doi:10.1109/ICASSP.2015.7178838.">Sainath <em>et al.</em>, 2015</a>, <a class="reference internal" href="Abstract.html#id218" title="Haşim Sak, Andrew Senior, Kanishka Rao, and Françoise Beaufays. Fast and Accurate Recurrent Neural Network Acoustic Models for Speech Recognition. In arXiv:1507.06947 [cs, stat]. July 2015. arXiv: 1507.06947. URL: http://arxiv.org/abs/1507.06947 (visited on 2016-12-21).">Sak <em>et al.</em>, 2015</a>]</span>, are promising.</p>
<p>Our automated architecture optimization provided interesting insights by yielding configurations that were markedly different from our hand-engineered architectures, yet reached similar accuracies. Since the marked improvements in training performance did not improve the evaluation accuracies in this study, in future work, we plan to use more training recordings in the optimization and study different cross-validation methods to also improve evaluation accuracies.
A full-blown architecture search <span id="id10">[<a class="reference internal" href="Abstract.html#id26" title="H. Mendoza, A. Klein, M. Feurer, J. Springenberg, and F. Hutter. Towards Automatically-Tuned Neural Networks. In ICML 2016 AutoML Workshop. June 2016.">Mendoza <em>et al.</em>, 2016</a>, <a class="reference internal" href="Abstract.html#id24" title="Risto Miikkulainen, Jason Liang, Elliot Meyerson, Aditya Rawal, Dan Fink, Olivier Francon, Bala Raju, Hormoz Shahrzad, Arshak Navruzyan, Nigel Duffy, and Babak Hodjat. Evolving Deep Neural Networks. arXiv:1703.00548 [cs], March 2017. arXiv: 1703.00548. URL: http://arxiv.org/abs/1703.00548 (visited on 2017-08-26).">Miikkulainen <em>et al.</em>, 2017</a>, <a class="reference internal" href="Abstract.html#id25" title="Esteban Real, Sherry Moore, Andrew Selle, Saurabh Saxena, Yutaka Leon Suematsu, Jie Tan, Quoc Le, and Alex Kurakin. Large-Scale Evolution of Image Classifiers. arXiv:1703.01041 [cs], March 2017. arXiv: 1703.01041. URL: http://arxiv.org/abs/1703.01041 (visited on 2017-08-26).">Real <em>et al.</em>, 2017</a>, <a class="reference internal" href="Abstract.html#id23" title="Barret Zoph and Quoc V. Le. Neural Architecture Search with Reinforcement Learning. arXiv:1611.01578 [cs], November 2016. arXiv: 1611.01578. URL: http://arxiv.org/abs/1611.01578 (visited on 2017-08-26).">Zoph and Le, 2016</a>, <a class="reference internal" href="Abstract.html#id22" title="Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V. Le. Learning Transferable Architectures for Scalable Image Recognition. arXiv:1707.07012 [cs], July 2017. arXiv: 1707.07012. URL: http://arxiv.org/abs/1707.07012 (visited on 2017-08-26).">Zoph <em>et al.</em>, 2017</a>]</span> could also further improve accuracy. With such improved methods it would also be important not only to decode pathological vs. normal EEG in a binary fashion, but to also evaluate the possibility to derive more fine-grained clinical information, such as the type of pathological change (slowing, asymmetry, etc) or the likely underlying disorder (such as epilepsy).</p>
<p>Any of these or other improvements might eventually bring the machine-learning decoding performance of pathological EEG closer to human-level performance.
Since clinicians make their judgments from patterns they see in the EEG and other available context information, there is no clear reason why machine learning models with access to the same information could not reach human-level accuracy.
This human-level performance is a benchmark for decoding accuracies that does not exist for other brain-signal decoding tasks, e.g. in decoding task-related information for brain-computer interfaces, where there is inherent uncertainty what information is even present in the EEG and no human-level benchmark exists.</p>
<p>Our perturbation visualizations of the ConvNets’ decoding behavior showed that they used spectral power changes in the delta (0-4 Hz) and theta (4-8 Hz) frequency range, particularly from temporal EEG channels, possibly alongside other features (<a class="reference internal" href="#perturbation-shallow-pathology-fig"><span class="std std-numref">Fig. 37</span></a>). This observation is consistent both with the expectations implied by the spectral analysis of the EEG data (<a class="reference internal" href="#bandpower-pathology-fig"><span class="std std-numref">Fig. 36</span></a>) and by the textual reports that frequently mentioned <code class="docutils literal notranslate"><span class="pre">temporal</span></code>  and <code class="docutils literal notranslate"><span class="pre">slowing</span></code> with respect to the pathological samples, but never in the normal ones.
Our  perturbation visualization showed results that were consistent with expectations that the ConvNets would use the bandpower differences between the classes that were already visible in the spectra to perform their decoding.
Similarly, the textual reports also yielded plausible insights, e.g., that <code class="docutils literal notranslate"><span class="pre">small</span> <span class="pre">amounts</span></code> of abnormalities as indicated in the written clinical reports were more difficult for the networks to decode correctly.
Additionally, inspection of the textual reports also emphasized the importance of  integrating contextual information such as the age of the subject.</p>
<p>Still, to yield more clinically useful insights and diagnosis explanations, further improvements in ConvNet visualizations are needed.
Deep learning models that use an attention mechanism might be more interpretable, since these models can highlight which parts of the recording were most important for the decoding decision.
Other deep learning visualization methods like recent saliency map methods <span id="id11">[<a class="reference internal" href="Abstract.html#id21" title="Pieter-Jan Kindermans, Kristof T. Schütt, Maximilian Alber, Klaus-Robert Müller, and Sven Dähne. PatternNet and PatternLRP - Improving the interpretability of neural networks. CoRR, 2017. URL: http://arxiv.org/abs/1705.05598.">Kindermans <em>et al.</em>, 2017</a>, <a class="reference internal" href="Abstract.html#id20" title="Grégoire Montavon, Wojciech Samek, and Klaus-Robert Müller. Methods for Interpreting and Understanding Deep Neural Networks. CoRR, 2017. URL: http://arxiv.org/abs/1706.07979.">Montavon <em>et al.</em>, 2017</a>]</span> to explain individual decisions or conditional generative adversarial networks  <span id="id12">[<a class="reference internal" href="Abstract.html#id27" title="Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784, 2014.">Mirza and Osindero, 2014</a>, <a class="reference internal" href="Abstract.html#id29" title="Jost Tobias Springenberg. Unsupervised and semi-supervised learning with categorical generative adversarial networks. arXiv preprint arXiv:1511.06390, 2015.">Springenberg, 2015</a>]</span> to understand what makes a recording pathological or normal might further improve the clinical benefit of deep learning methods that decode pathological EEG.</p>
</div>
</div>
<span id="document-Invertible"></span><div class="tex2jax_ignore mathjax_ignore section" id="invertible-networks">
<span id="id1"></span><h2>Invertible Networks<a class="headerlink" href="#invertible-networks" title="Permalink to this headline">#</a></h2>
<p>Invertible networks are networks that are invertible by design, i.e., any network output can be bijectively mapped back to a corresponding input [refs]. The ability to invert any output back to the input enables different interpretability methods and furthermore allows training invertible networks as generative models via maximum likelhood.</p>
<ul class="simple">
<li><p>[todo explain]</p></li>
</ul>
<div class="figure align-default" id="bcic-iv-2a-right-vs-rest">
<img alt="_images/bcic_iv_2a_right_vs_rest.png" src="_images/bcic_iv_2a_right_vs_rest.png" />
<p class="caption"><span class="caption-number">Fig. 38 </span><span class="caption-text">Right hand vs resting state class prototypes learned by an invertible network on the BCIC IV 2a dataset. Note the increased alpha oscillation for resting state, especially on the left side.</span><a class="headerlink" href="#bcic-iv-2a-right-vs-rest" title="Permalink to this image">#</a></p>
</div>
<div class="figure align-default" id="bcic-iv-2a-right-vs-rest-c3">
<img alt="_images/bcic_iv_2a_right_vs_rest.png" src="_images/bcic_iv_2a_right_vs_rest.png" />
<p class="caption"><span class="caption-number">Fig. 39 </span><span class="caption-text">Right hand vs resting state class prototypes learned by an invertible network on the BCIC IV 2a dataset, showing the C3 electrode. Note the increased alpha oscillation.</span><a class="headerlink" href="#bcic-iv-2a-right-vs-rest-c3" title="Permalink to this image">#</a></p>
</div>
<p>Class prototypes are one directly obtainable visualization of the trained invertible network. Here, the invertible network was trained as a class-conditional generative model via maximum likelihood with an extra classification loss [refs etc.]. To obtain the class protypes we first found the maxima of each class  distribution, i.e., of the learned class-conditional distribution <span class="math notranslate nohighlight">\(p_{\theta}(x|y_c)\)</span>. From that starting point, the synthetic prototypes were further optimized to minimize <span class="math notranslate nohighlight">\(L_{proto}(x) = -w_{prob}\log_{}\left(p_{\theta}\left(x\mid y_{c}\right)\right)\,-w_{class}\,\log_{}\left(p_{\theta}\left(y_{c}\mid x\right)\right)\)</span>.</p>
<p>Prototypes for right hand vs resting state show a plausible discriminative pattern with increasing alpha oscillation of resting state compared to right hand, see <a class="reference internal" href="#bcic-iv-2a-right-vs-rest"><span class="std std-numref">Fig. 38</span></a>. Note that due to the discriminative training and the discriminative term in the optimization, these visualizations may show discriminative patterns that do not directly correspond to how actual signals for these classes look like. For example, in the actual data there may a decreasing right hand oscillation and a stable oscillation for resting state. See also the discussion in <a class="reference internal" href="Abstract.html#perturbation-visualization-interpretation"><span class="std std-ref">Interpretation and limitations</span></a>.</p>
<p>Todo:</p>
<ul class="simple">
<li><p>Neural architecture search</p></li>
<li><p>Simplebits</p></li>
</ul>
</div>
<span id="document-FutureWork"></span><div class="tex2jax_ignore mathjax_ignore section" id="future-work">
<span id="id1"></span><h2>Future Work<a class="headerlink" href="#future-work" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>transformer idea</p></li>
<li><p>invertible?</p></li>
</ul>
</div>
<span id="document-References"></span><div class="tex2jax_ignore mathjax_ignore section" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">#</a></h2>
<div class="docutils container" id="id1">
<dl class="citation">
<dt class="label" id="id5"><span class="brackets">ABS+19</span></dt>
<dd><p>Reza Abiri, Soheil Borhani, Eric W Sellers, Yang Jiang, and Xiaopeng Zhao. A comprehensive review of eeg-based brain–computer interface paradigms. <em>Journal of neural engineering</em>, 16(1):011001, 2019.</p>
</dd>
<dt class="label" id="id65"><span class="brackets">ACZG08</span></dt>
<dd><p>Kai Keng Ang, Zheng Yang Chin, Haihong Zhang, and Cuntai Guan. Filter Bank Common Spatial Pattern (FBCSP) in Brain-Computer Interface. In <em>IEEE International Joint Conference on Neural Networks, 2008. IJCNN 2008. (IEEE World Congress on Computational Intelligence)</em>, 2390–2397. June 2008. URL: <a class="reference external" href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4634130">http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4634130</a>, <a class="reference external" href="https://doi.org/10.1109/IJCNN.2008.4634130">doi:10.1109/IJCNN.2008.4634130</a>.</p>
</dd>
<dt class="label" id="id154"><span class="brackets">ASTS16</span></dt>
<dd><p>A. Antoniades, L. Spyrou, C. C. Took, and S. Sanei. Deep learning for epileptic intracranial EEG data. In <em>2016 IEEE 26th International Workshop on Machine Learning for Signal Processing (MLSP)</em>, 1–6. September 2016. <a class="reference external" href="https://doi.org/10.1109/MLSP.2016.7738824">doi:10.1109/MLSP.2016.7738824</a>.</p>
</dd>
<dt class="label" id="id57"><span class="brackets">BDM+08</span></dt>
<dd><p>Tonio Ball, Evariste Demandt, Isabella Mutschler, Eva Neitzel, Carsten Mehring, Klaus Vogt, Ad Aertsen, and Andreas Schulze-Bonhage. Movement related activity in the high gamma range of the human EEG. <em>NeuroImage</em>, 41(2):302–310, June 2008. URL: <a class="reference external" href="http://www.sciencedirect.com/science/article/pii/S1053811908001717">http://www.sciencedirect.com/science/article/pii/S1053811908001717</a> (visited on 2015-07-15), <a class="reference external" href="https://doi.org/10.1016/j.neuroimage.2008.02.032">doi:10.1016/j.neuroimage.2008.02.032</a>.</p>
</dd>
<dt class="label" id="id197"><span class="brackets">BRYC16</span></dt>
<dd><p>Pouya Bashivan, Irina Rish, Mohammed Yeasin, and Noel Codella. Learning Representations from EEG with Deep Recurrent-Convolutional Neural Networks. In <em>arXiv:1511.06448 [cs]</em>. 2016. arXiv: 1511.06448. URL: <a class="reference external" href="http://arxiv.org/abs/1511.06448">http://arxiv.org/abs/1511.06448</a> (visited on 2016-12-20).</p>
</dd>
<dt class="label" id="id39"><span class="brackets">BSBB18</span></dt>
<dd><p>Joos Behncke, Robin T Schirrmeister, Wolfram Burgard, and Tonio Ball. The signature of robot action success in eeg signals of a human observer: decoding and visualization using deep convolutional neural networks. In <em>2018 6th international conference on brain-computer interface (BCI)</em>, 1–6. IEEE, 2018.</p>
</dd>
<dt class="label" id="id37"><span class="brackets">BSV+18</span></dt>
<dd><p>Joos Behncke, Robin Tibor Schirrmeister, Martin Volker, Jiri Hammer, Petr Marusic, Andreas Schulze-Bonhage, Wolfram Burgard, and Tonio Ball. Cross-paradigm pretraining of convolutional networks improves intracranial eeg decoding. In <em>2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC)</em>, 1046–1053. IEEE, 2018.</p>
</dd>
<dt class="label" id="id62"><span class="brackets">BTL+08</span></dt>
<dd><p>B. Blankertz, R. Tomioka, S. Lemm, M. Kawanabe, and K.-R. Muller. Optimizing Spatial filters for Robust EEG Single-Trial Analysis. <em>IEEE Signal Processing Magazine</em>, 25(1):41–56, 2008. URL: <a class="reference external" href="http://ieeexplore.ieee.org/xpls/icp.jsp?arnumber=4408441">http://ieeexplore.ieee.org/xpls/icp.jsp?arnumber=4408441</a>, <a class="reference external" href="https://doi.org/10.1109/MSP.2008.4408441">doi:10.1109/MSP.2008.4408441</a>.</p>
</dd>
<dt class="label" id="id206"><span class="brackets">BLMP+08</span></dt>
<dd><p>C. Brunner, R. Leeb, G. Müller-Putz, A. Schlögl, and G. Pfurtscheller. BCI Competition 2008–Graz data set A. <em>Institute for Knowledge Discovery (Laboratory of Brain-Computer Interfaces), Graz University of Technology</em>, pages 136–142, 2008. URL: <a class="reference external" href="http://www.bbci.de/competition/iv/desc_2a.pdf">http://www.bbci.de/competition/iv/desc_2a.pdf</a> (visited on 2017-01-09).</p>
</dd>
<dt class="label" id="id41"><span class="brackets">BFK+17</span></dt>
<dd><p>Felix Burget, Lukas Dominique Josef Fiederer, Daniel Kuhner, Martin Völker, Johannes Aldinger, Robin Tibor Schirrmeister, Chau Do, Joschka Boedecker, Bernhard Nebel, Tonio Ball, and others. Acting thoughts: towards a mobile robotic service assistant for users with limited communication skills. In <em>2017 European Conference on Mobile Robots (ECMR)</em>, 1–6. IEEE, 2017.</p>
</dd>
<dt class="label" id="id212"><span class="brackets">CG11</span></dt>
<dd><p>Hubert Cecotti and Axel Graser. Convolutional Neural Networks for P300 Detection with Application to Brain-Computer Interfaces. <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 33(3):433–445, March 2011. URL: <a class="reference external" href="http://dx.doi.org/10.1109/TPAMI.2010.125">http://dx.doi.org/10.1109/TPAMI.2010.125</a> (visited on 2016-12-20), <a class="reference external" href="https://doi.org/10.1109/TPAMI.2010.125">doi:10.1109/TPAMI.2010.125</a>.</p>
</dd>
<dt class="label" id="id3"><span class="brackets">CCM+22</span></dt>
<dd><p>Isha R Chavva, Anna L Crawford, Mercy H Mazurek, Matthew M Yuen, Anjali M Prabhat, Sam Payabvash, Gordon Sze, Guido J Falcone, Charles C Matouk, Adam de Havenon, and others. Deep learning applications for acute stroke management. <em>Annals of Neurology</em>, 92(4):574–587, 2022.</p>
</dd>
<dt class="label" id="id67"><span class="brackets">CAW+09</span></dt>
<dd><p>Zheng Yang Chin, Kai Keng Ang, Chuanchu Wang, Cuntai Guan, and Haihong Zhang. Multi-class filter bank common spatial pattern for four-class motor imagery BCI. In <em>Annual International Conference of the IEEE Engineering in Medicine and Biology Society, 2009. EMBC 2009</em>, 571–574. September 2009. URL: <a class="reference external" href="http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=5332383">http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=5332383</a>, <a class="reference external" href="https://doi.org/10.1109/IEMBS.2009.5332383">doi:10.1109/IEMBS.2009.5332383</a>.</p>
</dd>
<dt class="label" id="id217"><span class="brackets">CUH16</span></dt>
<dd><p>Djork-Arné Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs). In <em>ArXiv e-prints</em>, volume 1511, arXiv:1511.07289. 2016. URL: <a class="reference external" href="http://adsabs.harvard.edu/abs/2015arXiv151107289C">http://adsabs.harvard.edu/abs/2015arXiv151107289C</a> (visited on 2016-12-21).</p>
</dd>
<dt class="label" id="id169"><span class="brackets">CMGL98</span></dt>
<dd><p>N. E. Crone, D. L. Miglioretti, B. Gordon, and R. P. Lesser. Functional mapping of human sensorimotor cortex with electrocorticographic spectral analysis. II. Event-related synchronization in the gamma band. <em>Brain</em>, 121(12):2301–2315, December 1998. URL: <a class="reference external" href="https://academic.oup.com/brain/article/121/12/2301/371496/Functional-mapping-of-human-sensorimotor-cortex">https://academic.oup.com/brain/article/121/12/2301/371496/Functional-mapping-of-human-sensorimotor-cortex</a> (visited on 2017-01-17), <a class="reference external" href="https://doi.org/10.1093/brain/121.12.2301">doi:10.1093/brain/121.12.2301</a>.</p>
</dd>
<dt class="label" id="id223"><span class="brackets">DSO+10</span></dt>
<dd><p>F. Darvas, R. Scherer, J. G. Ojemann, R. P. Rao, K. J. Miller, and L. B. Sorensen. High gamma mapping using EEG. <em>NeuroImage</em>, 49(1):930–938, January 2010. URL: <a class="reference external" href="http://www.sciencedirect.com/science/article/pii/S1053811909009513">http://www.sciencedirect.com/science/article/pii/S1053811909009513</a> (visited on 2017-01-10), <a class="reference external" href="https://doi.org/10.1016/j.neuroimage.2009.08.041">doi:10.1016/j.neuroimage.2009.08.041</a>.</p>
</dd>
<dt class="label" id="id4"><span class="brackets">GSChrabkaszcz+20</span></dt>
<dd><p>Lukas AW Gemein, Robin T Schirrmeister, Patryk Chrabąszcz, Daniel Wilson, Joschka Boedecker, Andreas Schulze-Bonhage, Frank Hutter, and Tonio Ball. Machine-learning-based diagnostics of eeg pathology. <em>NeuroImage</em>, 220:117021, 2020.</p>
</dd>
<dt class="label" id="id177"><span class="brackets">GCM+13</span></dt>
<dd><p>A. Giusti, D. C. Cireşan, J. Masci, L. M. Gambardella, and J. Schmidhuber. Fast image scanning with deep max-pooling convolutional neural networks. In <em>2013 IEEE International Conference on Image Processing</em>, 4034–4038. September 2013. <a class="reference external" href="https://doi.org/10.1109/ICIP.2013.6738831">doi:10.1109/ICIP.2013.6738831</a>.</p>
</dd>
<dt class="label" id="id180"><span class="brackets">HMJ+16</span></dt>
<dd><p>Mehdi Hajinoroozi, Zijing Mao, Tzyy-Ping Jung, Chin-Teng Lin, and Yufei Huang. EEG-based prediction of driver's cognitive performance by deep convolutional neural network. <em>Signal Processing: Image Communication</em>, 47:549–555, September 2016. URL: <a class="reference external" href="http://www.sciencedirect.com/science/article/pii/S0923596516300832">http://www.sciencedirect.com/science/article/pii/S0923596516300832</a> (visited on 2016-12-20), <a class="reference external" href="https://doi.org/10.1016/j.image.2016.05.018">doi:10.1016/j.image.2016.05.018</a>.</p>
</dd>
<dt class="label" id="id190"><span class="brackets">HPF+16</span></dt>
<dd><p>Jiří Hammer, Tobias Pistohl, Jörg Fischer, Pavel Kršek, Martin Tomášek, Petr Marusič, Andreas Schulze-Bonhage, Ad Aertsen, and Tonio Ball. Predominance of Movement Speed Over Direction in Neuronal Population Signals of Motor Cortex: Intracranial EEG Data and A Simple Explanatory Model. <em>Cerebral Cortex (New York, NY)</em>, 26(6):2863–2881, June 2016. URL: <a class="reference external" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4869816/">http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4869816/</a> (visited on 2017-01-11), <a class="reference external" href="https://doi.org/10.1093/cercor/bhw033">doi:10.1093/cercor/bhw033</a>.</p>
</dd>
<dt class="label" id="id63"><span class="brackets">HMG+14</span></dt>
<dd><p>Stefan Haufe, Frank Meinecke, Kai Görgen, Sven Dähne, John-Dylan Haynes, Benjamin Blankertz, and Felix Bießmann. On the interpretation of weight vectors of linear models in multivariate neuroimaging. <em>NeuroImage</em>, 87:96–110, February 2014. URL: <a class="reference external" href="http://www.sciencedirect.com/science/article/pii/S1053811913010914">http://www.sciencedirect.com/science/article/pii/S1053811913010914</a> (visited on 2015-08-07), <a class="reference external" href="https://doi.org/10.1016/j.neuroimage.2013.10.067">doi:10.1016/j.neuroimage.2013.10.067</a>.</p>
</dd>
<dt class="label" id="id110"><span class="brackets">HZRS15</span></dt>
<dd><p>Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image Recognition. <em>arXiv:1512.03385 [cs]</em>, December 2015. arXiv: 1512.03385. URL: <a class="reference external" href="http://arxiv.org/abs/1512.03385">http://arxiv.org/abs/1512.03385</a> (visited on 2016-05-11).</p>
</dd>
<dt class="label" id="id35"><span class="brackets">HSF+18</span></dt>
<dd><p>Felix A Heilmeyer, Robin T Schirrmeister, Lukas DJ Fiederer, Martin Volker, Joos Behncke, and Tonio Ball. A large-scale evaluation framework for eeg deep learning architectures. In <em>2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC)</em>, 1039–1045. IEEE, 2018.</p>
</dd>
<dt class="label" id="id21"><span class="brackets">KSA+17</span></dt>
<dd><p>Pieter-Jan Kindermans, Kristof T. Schütt, Maximilian Alber, Klaus-Robert Müller, and Sven Dähne. PatternNet and PatternLRP - Improving the interpretability of neural networks. <em>CoRR</em>, 2017. URL: <a class="reference external" href="http://arxiv.org/abs/1705.05598">http://arxiv.org/abs/1705.05598</a>.</p>
</dd>
<dt class="label" id="id167"><span class="brackets">KB14</span></dt>
<dd><p>Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In <em>Proceedings of the 3rd International Conference on Learning Representations (ICLR)</em>. 2014. URL: <a class="reference external" href="http://arxiv.org/abs/1412.6980">http://arxiv.org/abs/1412.6980</a> (visited on 2017-01-09).</p>
</dd>
<dt class="label" id="id157"><span class="brackets">KLZ90</span></dt>
<dd><p>Zoltan J. Koles, Michael S. Lazar, and Steven Z. Zhou. Spatial patterns underlying population differences in the background EEG. <em>Brain Topography</em>, 2(4):275–284, June 1990. URL: <a class="reference external" href="http://link.springer.com/article/10.1007/BF01129656">http://link.springer.com/article/10.1007/BF01129656</a> (visited on 2017-01-09), <a class="reference external" href="https://doi.org/10.1007/BF01129656">doi:10.1007/BF01129656</a>.</p>
</dd>
<dt class="label" id="id150"><span class="brackets">LSW+16</span></dt>
<dd><p>Vernon J. Lawhern, Amelia J. Solon, Nicholas R. Waytowich, Stephen M. Gordon, Chou P. Hung, and Brent J. Lance. EEGNet: A Compact Convolutional Network for EEG-based Brain-Computer Interfaces. <em>arXiv:1611.08024 [cs, q-bio, stat]</em>, November 2016. arXiv: 1611.08024. URL: <a class="reference external" href="http://arxiv.org/abs/1611.08024">http://arxiv.org/abs/1611.08024</a> (visited on 2016-12-20).</p>
</dd>
<dt class="label" id="id105"><span class="brackets">LBH15</span></dt>
<dd><p>Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. <em>Nature</em>, 521(7553):436–444, May 2015. URL: <a class="reference external" href="http://www.nature.com/nature/journal/v521/n7553/full/nature14539.html">http://www.nature.com/nature/journal/v521/n7553/full/nature14539.html</a> (visited on 2016-05-11), <a class="reference external" href="https://doi.org/10.1038/nature14539">doi:10.1038/nature14539</a>.</p>
</dd>
<dt class="label" id="id233"><span class="brackets">LBMP+08</span></dt>
<dd><p>R Leeb, C Brunner, GR Müller-Putz, A Schlögl, and G Pfurtscheller. BCI Competition 2008–Graz data set B. <em>Graz University of Technology, Austria</em>, 2008.</p>
</dd>
<dt class="label" id="id201"><span class="brackets">LW15</span></dt>
<dd><p>X. Li and X. Wu. Constructing long short-term memory based deep recurrent neural networks for large vocabulary speech recognition. In <em>2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 4520–4524. April 2015. <a class="reference external" href="https://doi.org/10.1109/ICASSP.2015.7178826">doi:10.1109/ICASSP.2015.7178826</a>.</p>
</dd>
<dt class="label" id="id172"><span class="brackets">LLZW16</span></dt>
<dd><p>J. Liang, R. Lu, C. Zhang, and F. Wang. Predicting Seizures from Electroencephalography Recordings: A Knowledge Transfer Strategy. In <em>2016 IEEE International Conference on Healthcare Informatics (ICHI)</em>, 184–191. October 2016. <a class="reference external" href="https://doi.org/10.1109/ICHI.2016.27">doi:10.1109/ICHI.2016.27</a>.</p>
</dd>
<dt class="label" id="id30"><span class="brackets">LdD17</span></dt>
<dd><p>S. Lopez de Diego. Automated interpretation of abnormal adult electroencephalography. Master's thesis, Temple University, 2017.</p>
</dd>
<dt class="label" id="id7"><span class="brackets">LH17</span></dt>
<dd><p>Ilya Loshchilov and Frank Hutter. SGDR: stochastic gradient descent with warm restarts. In <em>5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings</em>. OpenReview.net, 2017. URL: <a class="reference external" href="https://openreview.net/forum?id=Skq89Scxx">https://openreview.net/forum?id=Skq89Scxx</a>.</p>
</dd>
<dt class="label" id="id6"><span class="brackets">MJastrzkebskiO+22</span></dt>
<dd><p>Taro Makino, Stanisław Jastrzębski, Witold Oleszkiewicz, Celin Chacko, Robin Ehrenpreis, Naziya Samreen, Chloe Chhor, Eric Kim, Jiyon Lee, Kristine Pysarenko, and others. Differences between human and machine perception in medical diagnosis. <em>Scientific reports</em>, 12(1):1–13, 2022.</p>
</dd>
<dt class="label" id="id147"><span class="brackets">MG15</span></dt>
<dd><p>Ran Manor and Amir B. Geva. Convolutional Neural Network for Multi-Category Rapid Serial Visual Presentation BCI. <em>Frontiers in Computational Neuroscience</em>, 9:146, 2015. <a class="reference external" href="https://doi.org/10.3389/fncom.2015.00146">doi:10.3389/fncom.2015.00146</a>.</p>
</dd>
<dt class="label" id="id196"><span class="brackets">MMG16</span></dt>
<dd><p>Ran Manor, Liran Mishali, and Amir B. Geva. Multimodal Neural Network for Rapid Serial Visual Presentation Brain Computer Interface. <em>Frontiers in Computational Neuroscience</em>, December 2016. URL: <a class="reference external" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC5168930/">http://www.ncbi.nlm.nih.gov/pmc/articles/PMC5168930/</a> (visited on 2017-02-03), <a class="reference external" href="https://doi.org/10.3389/fncom.2016.00130">doi:10.3389/fncom.2016.00130</a>.</p>
</dd>
<dt class="label" id="id26"><span class="brackets">MKF+16</span></dt>
<dd><p>H. Mendoza, A. Klein, M. Feurer, J. Springenberg, and F. Hutter. Towards Automatically-Tuned Neural Networks. In <em>ICML 2016 AutoML Workshop</em>. June 2016.</p>
</dd>
<dt class="label" id="id24"><span class="brackets">MLM+17</span></dt>
<dd><p>Risto Miikkulainen, Jason Liang, Elliot Meyerson, Aditya Rawal, Dan Fink, Olivier Francon, Bala Raju, Hormoz Shahrzad, Arshak Navruzyan, Nigel Duffy, and Babak Hodjat. Evolving Deep Neural Networks. <em>arXiv:1703.00548 [cs]</em>, March 2017. arXiv: 1703.00548. URL: <a class="reference external" href="http://arxiv.org/abs/1703.00548">http://arxiv.org/abs/1703.00548</a> (visited on 2017-08-26).</p>
</dd>
<dt class="label" id="id27"><span class="brackets">MO14</span></dt>
<dd><p>Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. <em>arXiv preprint arXiv:1411.1784</em>, 2014.</p>
</dd>
<dt class="label" id="id20"><span class="brackets">MSM17</span></dt>
<dd><p>Grégoire Montavon, Wojciech Samek, and Klaus-Robert Müller. Methods for Interpreting and Understanding Deep Neural Networks. <em>CoRR</em>, 2017. URL: <a class="reference external" href="http://arxiv.org/abs/1706.07979">http://arxiv.org/abs/1706.07979</a>.</p>
</dd>
<dt class="label" id="id210"><span class="brackets">NTF09</span></dt>
<dd><p>Fabian Nasse, Christian Thurau, and Gernot A. Fink. Face detection using gpu-based convolutional neural networks. In <em>International Conference on Computer Analysis of Images and Patterns</em>, 83–90. Springer, 2009. URL: <a class="reference external" href="http://link.springer.com/chapter/10.1007/978-3-642-03767-2_10">http://link.springer.com/chapter/10.1007/978-3-642-03767-2_10</a> (visited on 2017-01-09).</p>
</dd>
<dt class="label" id="id32"><span class="brackets">OP16</span></dt>
<dd><p>Iyad Obeid and Joseph Picone. The temple university hospital eeg data corpus. <em>Frontiers in Neuroscience</em>, 2016. URL: <a class="reference external" href="https://www.frontiersin.org/articles/10.3389/fnins.2016.00196">https://www.frontiersin.org/articles/10.3389/fnins.2016.00196</a>, <a class="reference external" href="https://doi.org/10.3389/fnins.2016.00196">doi:10.3389/fnins.2016.00196</a>.</p>
</dd>
<dt class="label" id="id204"><span class="brackets">PSM16</span></dt>
<dd><p>A. Page, C. Shea, and T. Mohsenin. Wearable seizure detection using convolutional neural networks with transfer learning. In <em>2016 IEEE International Symposium on Circuits and Systems (ISCAS)</em>, 1086–1089. May 2016. <a class="reference external" href="https://doi.org/10.1109/ISCAS.2016.7527433">doi:10.1109/ISCAS.2016.7527433</a>.</p>
</dd>
<dt class="label" id="id199"><span class="brackets">Pfu81</span></dt>
<dd><p>G Pfurtscheller. Central beta rhythm during sensorimotor activities in man. <em>Electroencephalography and Clinical Neurophysiology</em>, 51(3):253–264, March 1981. URL: <a class="reference external" href="http://www.sciencedirect.com/science/article/pii/0013469481901395">http://www.sciencedirect.com/science/article/pii/0013469481901395</a> (visited on 2017-01-09), <a class="reference external" href="https://doi.org/10.1016/0013-4694(81)90139-5">doi:10.1016/0013-4694(81)90139-5</a>.</p>
</dd>
<dt class="label" id="id148"><span class="brackets">PA79</span></dt>
<dd><p>G Pfurtscheller and A Aranibar. Evaluation of event-related desynchronization (ERD) preceding and following voluntary self-paced movement. <em>Electroencephalography and Clinical Neurophysiology</em>, 46(2):138–146, February 1979. URL: <a class="reference external" href="http://www.sciencedirect.com/science/article/pii/0013469479900634">http://www.sciencedirect.com/science/article/pii/0013469479900634</a> (visited on 2017-01-09), <a class="reference external" href="https://doi.org/10.1016/0013-4694(79)90063-4">doi:10.1016/0013-4694(79)90063-4</a>.</p>
</dd>
<dt class="label" id="id161"><span class="brackets">QRH+12</span></dt>
<dd><p>F. Quandt, C. Reichert, H. Hinrichs, H. J. Heinze, R. T. Knight, and J. W. Rieger. Single trial discrimination of individual finger movements on one hand: A combined MEG and EEG study. <em>NeuroImage</em>, 59(4):3316–3324, February 2012. URL: <a class="reference external" href="http://www.sciencedirect.com/science/article/pii/S1053811911013358">http://www.sciencedirect.com/science/article/pii/S1053811911013358</a> (visited on 2017-01-17), <a class="reference external" href="https://doi.org/10.1016/j.neuroimage.2011.11.053">doi:10.1016/j.neuroimage.2011.11.053</a>.</p>
</dd>
<dt class="label" id="id8"><span class="brackets">RS20</span></dt>
<dd><p>Maithra Raghu and Eric Schmidt. A survey of deep learning for scientific discovery. <em>arXiv preprint arXiv:2003.11755</em>, 2020.</p>
</dd>
<dt class="label" id="id113"><span class="brackets">RMGP00</span></dt>
<dd><p>H. Ramoser, J. Muller-Gerking, and G. Pfurtscheller. Optimal spatial filtering of single trial EEG during imagined hand movement. <em>IEEE Transactions on Rehabilitation Engineering</em>, 8(4):441–446, December 2000. <a class="reference external" href="https://doi.org/10.1109/86.895946">doi:10.1109/86.895946</a>.</p>
</dd>
<dt class="label" id="id10"><span class="brackets">RPK+20</span></dt>
<dd><p>Nina Rank, Boris Pfahringer, Jörg Kempfert, Christof Stamm, Titus Kühne, Felix Schoenrath, Volkmar Falk, Carsten Eickhoff, and Alexander Meyer. Deep-learning-based real-time prediction of acute kidney injury outperforms human predictive performance. <em>NPJ digital medicine</em>, 3(1):1–12, 2020.</p>
</dd>
<dt class="label" id="id33"><span class="brackets">Rau15</span></dt>
<dd><p>V. Rau. Eeg correlates of inner speech. <em>Bachelor's Thesis, University of Freiburg, DOI</em>, 2015.</p>
</dd>
<dt class="label" id="id25"><span class="brackets">RMS+17</span></dt>
<dd><p>Esteban Real, Sherry Moore, Andrew Selle, Saurabh Saxena, Yutaka Leon Suematsu, Jie Tan, Quoc Le, and Alex Kurakin. Large-Scale Evolution of Image Classifiers. <em>arXiv:1703.01041 [cs]</em>, March 2017. arXiv: 1703.01041. URL: <a class="reference external" href="http://arxiv.org/abs/1703.01041">http://arxiv.org/abs/1703.01041</a> (visited on 2017-08-26).</p>
</dd>
<dt class="label" id="id202"><span class="brackets">SVSS15</span></dt>
<dd><p>T. N. Sainath, O. Vinyals, A. Senior, and H. Sak. Convolutional, Long Short-Term Memory, fully connected Deep Neural Networks. In <em>2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 4580–4584. April 2015. <a class="reference external" href="https://doi.org/10.1109/ICASSP.2015.7178838">doi:10.1109/ICASSP.2015.7178838</a>.</p>
</dd>
<dt class="label" id="id218"><span class="brackets">SSRB15</span></dt>
<dd><p>Haşim Sak, Andrew Senior, Kanishka Rao, and Françoise Beaufays. Fast and Accurate Recurrent Neural Network Acoustic Models for Speech Recognition. In <em>arXiv:1507.06947 [cs, stat]</em>. July 2015. arXiv: 1507.06947. URL: <a class="reference external" href="http://arxiv.org/abs/1507.06947">http://arxiv.org/abs/1507.06947</a> (visited on 2016-12-21).</p>
</dd>
<dt class="label" id="id117"><span class="brackets">SGY15</span></dt>
<dd><p>S. Sakhavi, C. Guan, and S. Yan. Parallel convolutional-linear neural network for motor imagery classification. In <em>Signal Processing Conference (EUSIPCO), 2015 23rd European</em>, 2736–2740. August 2015. <a class="reference external" href="https://doi.org/10.1109/EUSIPCO.2015.7362882">doi:10.1109/EUSIPCO.2015.7362882</a>.</p>
</dd>
<dt class="label" id="id174"><span class="brackets">SMH+04</span></dt>
<dd><p>G. Schalk, D. J. McFarland, T. Hinterberger, N. Birbaumer, and J. R. Wolpaw. BCI2000: a general-purpose brain-computer interface (BCI) system. <em>IEEE Transactions on Biomedical Engineering</em>, 51(6):1034–1043, June 2004. <a class="reference external" href="https://doi.org/10.1109/TBME.2004.827072">doi:10.1109/TBME.2004.827072</a>.</p>
</dd>
<dt class="label" id="id31"><span class="brackets">SGE+17</span></dt>
<dd><p>R. Schirrmeister, L. Gemein, K. Eggensperger, F. Hutter, and T. Ball. Deep learning with convolutional neural networks for decoding and visualization of eeg pathology. In <em>2017 IEEE Signal Processing in Medicine and Biology Symposium (SPMB)</em>, volume, 1–7. 2017. <a class="reference external" href="https://doi.org/10.1109/SPMB.2017.8257015">doi:10.1109/SPMB.2017.8257015</a>.</p>
</dd>
<dt class="label" id="id2"><span class="brackets">Sch15a</span></dt>
<dd><p>Robin Tibor Schirrmeister. Convolutional neural networks for movement decoding from eeg signals. Master's thesis, Albert-Ludwigs-Universität Freiburg, 2015.</p>
</dd>
<dt class="label" id="id34"><span class="brackets">SSF+17</span></dt>
<dd><p>Robin Tibor Schirrmeister, Jost Tobias Springenberg, Lukas Dominique Josef Fiederer, Martin Glasstetter, Katharina Eggensperger, Michael Tangermann, Frank Hutter, Wolfram Burgard, and Tonio Ball. Deep learning with convolutional neural networks for eeg decoding and visualization. <em>Human Brain Mapping</em>, aug 2017. URL: <a class="reference external" href="http://dx.doi.org/10.1002/hbm.23730">http://dx.doi.org/10.1002/hbm.23730</a>, <a class="reference external" href="https://doi.org/10.1002/hbm.23730">doi:10.1002/hbm.23730</a>.</p>
</dd>
<dt class="label" id="id78"><span class="brackets">Sch15b</span></dt>
<dd><p>Juergen Schmidhuber. Deep Learning in Neural Networks: An Overview. <em>Neural Networks</em>, 61:85–117, January 2015. arXiv: 1404.7828. URL: <a class="reference external" href="http://arxiv.org/abs/1404.7828">http://arxiv.org/abs/1404.7828</a> (visited on 2015-08-12), <a class="reference external" href="https://doi.org/10.1016/j.neunet.2014.09.003">doi:10.1016/j.neunet.2014.09.003</a>.</p>
</dd>
<dt class="label" id="id139"><span class="brackets">SEZ+13</span></dt>
<dd><p>Pierre Sermanet, David Eigen, Xiang Zhang, Michael Mathieu, Rob Fergus, and Yann LeCun. OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks. <em>arXiv:1312.6229 [cs]</em>, December 2013. arXiv: 1312.6229. URL: <a class="reference external" href="http://arxiv.org/abs/1312.6229">http://arxiv.org/abs/1312.6229</a> (visited on 2016-08-12).</p>
</dd>
<dt class="label" id="id200"><span class="brackets">SLK+16</span></dt>
<dd><p>Jared Shamwell, Hyungtae Lee, Heesung Kwon, Amar R. Marathe, Vernon Lawhern, and William Nothwang. Single-trial EEG RSVP classification using convolutional neural networks. In Thomas George, Achyut K. Dutta, and M. Saif Islam, editors, <em>SPIE Defense+ Security</em>, volume 9836. International Society for Optics and Photonics, May 2016. URL: <a class="reference external" href="http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.2224172">http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.2224172</a> (visited on 2017-02-14), <a class="reference external" href="https://doi.org/10.1117/12.2224172">doi:10.1117/12.2224172</a>.</p>
</dd>
<dt class="label" id="id181"><span class="brackets">SLD16</span></dt>
<dd><p>E. Shelhamer, J. Long, and T. Darrell. Fully Convolutional Networks for Semantic Segmentation. <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, PP(99):1–1, 2016. <a class="reference external" href="https://doi.org/10.1109/TPAMI.2016.2572683">doi:10.1109/TPAMI.2016.2572683</a>.</p>
</dd>
<dt class="label" id="id29"><span class="brackets">Spr15</span></dt>
<dd><p>Jost Tobias Springenberg. Unsupervised and semi-supervised learning with categorical generative adversarial networks. <em>arXiv preprint arXiv:1511.06390</em>, 2015.</p>
</dd>
<dt class="label" id="id226"><span class="brackets">Sto16</span></dt>
<dd><p>Sebastian Stober. Learning Discriminative Features from Electroencephalography Recordings by Encoding Similarity Constraints. In <em>Bernstein Conference 2016</em>. 2016. <a class="reference external" href="https://doi.org/10.12751/nncn.bc2016.0223">doi:10.12751/nncn.bc2016.0223</a>.</p>
</dd>
<dt class="label" id="id184"><span class="brackets">SCG14</span></dt>
<dd><p>Sebastian Stober, Daniel J. Cameron, and Jessica A. Grahn. Using Convolutional Neural Networks to Recognize Rhythm Stimuli from Electroencephalography Recordings. In <em>Proceedings of the 27th International Conference on Neural Information Processing Systems</em>, NIPS'14, 1449–1457. Cambridge, MA, USA, 2014. MIT Press. URL: <a class="reference external" href="http://dl.acm.org/citation.cfm?id=2968826.2968988">http://dl.acm.org/citation.cfm?id=2968826.2968988</a> (visited on 2016-12-20).</p>
</dd>
<dt class="label" id="id222"><span class="brackets">SQC+16</span></dt>
<dd><p>Xuyun Sun, Cunle Qian, Zhongqin Chen, Zhaohui Wu, Benyan Luo, and Gang Pan. Remembered or Forgotten?—An EEG-Based Computational Prediction Approach. <em>PLOS ONE</em>, 11(12):e0167497, December 2016. URL: <a class="reference external" href="http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0167497">http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0167497</a> (visited on 2017-02-14), <a class="reference external" href="https://doi.org/10.1371/journal.pone.0167497">doi:10.1371/journal.pone.0167497</a>.</p>
</dd>
<dt class="label" id="id211"><span class="brackets">SZS+14</span></dt>
<dd><p>Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In <em>International Conference on Learning Representations</em>. 2014. URL: <a class="reference external" href="http://arxiv.org/abs/1312.6199">http://arxiv.org/abs/1312.6199</a>.</p>
</dd>
<dt class="label" id="id215"><span class="brackets">TH17</span></dt>
<dd><p>Yousef Rezaei Tabar and Ugur Halici. A novel deep learning approach for classification of EEG motor imagery signals. <em>Journal of Neural Engineering</em>, 14(1):016003, 2017. URL: <a class="reference external" href="http://stacks.iop.org/1741-2552/14/i=1/a=016003">http://stacks.iop.org/1741-2552/14/i=1/a=016003</a> (visited on 2017-02-14), <a class="reference external" href="https://doi.org/10.1088/1741-2560/14/1/016003">doi:10.1088/1741-2560/14/1/016003</a>.</p>
</dd>
<dt class="label" id="id80"><span class="brackets">TMA+12</span></dt>
<dd><p>Michael Tangermann, Klaus-Robert Müller, Ad Aertsen, Niels Birbaumer, Christoph Braun, Clemens Brunner, Robert Leeb, Carsten Mehring, Kai J. Miller, Gernot R. Müller-Putz, Guido Nolte, Gert Pfurtscheller, Hubert Preissl, Gerwin Schalk, Alois Schlögl, Carmen Vidaurre, Stephan Waldert, and Benjamin Blankertz. Review of the BCI Competition IV. <em>Frontiers in Neuroscience</em>, July 2012. URL: <a class="reference external" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3396284/">http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3396284/</a> (visited on 2015-08-20), <a class="reference external" href="https://doi.org/10.3389/fnins.2012.00055">doi:10.3389/fnins.2012.00055</a>.</p>
</dd>
<dt class="label" id="id182"><span class="brackets">TPL16</span></dt>
<dd><p>Pierre Thodoroff, Joelle Pineau, and Andrew Lim. Learning Robust Features using Deep Learning for Automatic Seizure Detection. In <em>JMLR Workshop and Conference Proceedings</em>, volume 56. 2016. URL: <a class="reference external" href="http://www.jmlr.org/proceedings/papers/v56/Thodoroff16.pdf">http://www.jmlr.org/proceedings/papers/v56/Thodoroff16.pdf</a> (visited on 2017-02-14).</p>
</dd>
<dt class="label" id="id9"><span class="brackets">TomavsevGR+19</span></dt>
<dd><p>Nenad Tomašev, Xavier Glorot, Jack W Rae, Michal Zielinski, Harry Askham, Andre Saraiva, Anne Mottram, Clemens Meyer, Suman Ravuri, Ivan Protsyuk, and others. A clinically applicable approach to continuous prediction of future acute kidney injury. <em>Nature</em>, 572(7767):116–119, 2019.</p>
</dd>
<dt class="label" id="id38"><span class="brackets">VHS+18</span></dt>
<dd><p>Martin Volker, Jiri Hammer, Robin T Schirrmeister, Joos Behncke, Lukas DJ Fiederer, Andreas Schulze-Bonhage, Petr Marusic, Wolfram Burgard, and Tonio Ball. Intracranial error detection via deep learning. In <em>2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC)</em>, 568–575. IEEE, 2018.</p>
</dd>
<dt class="label" id="id40"><span class="brackets">VolkerSF+18</span></dt>
<dd><p>Martin Völker, Robin T Schirrmeister, Lukas DJ Fiederer, Wolfram Burgard, and Tonio Ball. Deep transfer learning for error decoding from non-invasive eeg. In <em>2018 6th International Conference on Brain-Computer Interface (BCI)</em>, 1–6. IEEE, 2018.</p>
</dd>
<dt class="label" id="id36"><span class="brackets">WGS+18</span></dt>
<dd><p>X. Wang, C. A. Gkogkidis, R. T. Schirrmeister, F. A. Heilmeyer, M. Gierthmuehlen, F. Kohler, M. Schuettler, T. Stieglitz, and T. Ball. Deep learning for micro-electrocorticographic (µecog) data. In <em>2018 IEEE-EMBS Conference on Biomedical Engineering and Sciences (IECBES)</em>, volume, 63–68. 2018. <a class="reference external" href="https://doi.org/10.1109/IECBES.2018.8626607">doi:10.1109/IECBES.2018.8626607</a>.</p>
</dd>
<dt class="label" id="id209"><span class="brackets">WLSJ13</span></dt>
<dd><p>Zuoguan Wang, Siwei Lyu, Gerwin Schalk, and Qiang Ji. Deep Feature Learning Using Target Priors with Applications in ECoG Signal Decoding for BCI. In <em>Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence</em>, IJCAI '13, 1785–1791. Beijing, China, 2013. AAAI Press. URL: <a class="reference external" href="http://dl.acm.org/citation.cfm?id=2540128.2540384">http://dl.acm.org/citation.cfm?id=2540128.2540384</a> (visited on 2017-01-16).</p>
</dd>
<dt class="label" id="id23"><span class="brackets">ZL16</span></dt>
<dd><p>Barret Zoph and Quoc V. Le. Neural Architecture Search with Reinforcement Learning. <em>arXiv:1611.01578 [cs]</em>, November 2016. arXiv: 1611.01578. URL: <a class="reference external" href="http://arxiv.org/abs/1611.01578">http://arxiv.org/abs/1611.01578</a> (visited on 2017-08-26).</p>
</dd>
<dt class="label" id="id22"><span class="brackets">ZVSL17</span></dt>
<dd><p>Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V. Le. Learning Transferable Architectures for Scalable Image Recognition. <em>arXiv:1707.07012 [cs]</em>, July 2017. arXiv: 1707.07012. URL: <a class="reference external" href="http://arxiv.org/abs/1707.07012">http://arxiv.org/abs/1707.07012</a> (visited on 2017-08-26).</p>
</dd>
</dl>
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By The Jupyter Book community<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>