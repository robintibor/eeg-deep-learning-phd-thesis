
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>old, copied &#8212; Deep Learning for Brain-Signal Decoding from Electroencephalography (EEG)</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script async="async" kind="hypothesis" src="https://hypothes.is/embed.js"></script>
    <script kind="utterances">

    var commentsRunWhenDOMLoaded = cb => {
    if (document.readyState != 'loading') {
        cb()
    } else if (document.addEventListener) {
        document.addEventListener('DOMContentLoaded', cb)
    } else {
        document.attachEvent('onreadystatechange', function() {
        if (document.readyState == 'complete') cb()
        })
    }
}

var addUtterances = () => {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src = "https://utteranc.es/client.js";
    script.async = "async";

    script.setAttribute("repo", "robintibor/eeg-deep-learning-phd-thesis");
    script.setAttribute("issue-term", "pathname");
    script.setAttribute("theme", "github-light");
    script.setAttribute("label", "💬 comment");
    script.setAttribute("crossorigin", "anonymous");

    sections = document.querySelectorAll("div.section");
    if (sections !== null) {
        section = sections[sections.length-1];
        section.appendChild(script);
    }
}
commentsRunWhenDOMLoaded(addUtterances);
</script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/braindecode-logo.svg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Deep Learning for Brain-Signal Decoding from Electroencephalography (EEG)</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="Abstract.html">
                    Deep Learning for Brain-Signal Decoding from Electroencephalography (EEG)
                </a>
            </li>
        </ul>
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="PriorWork.html">
   Prior Work
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="FBCSPAndFBCSPNet.html">
   Filterbank Common Spatial Patterns and Filterbank Network
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="DeepArchitectures.html">
   Neural Network Architectures for EEG-Decoding
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="CroppedTraining.html">
   Cropped Training
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="PerturbationVisualization.html">
   Perturbation Visualization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="MovementDecoding.html">
   Decoding Movement-Related Brain Activity
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="TaskDecoding.html">
   Generalization to Other Tasks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Pathology.html">
   Decoding Pathology
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Invertible.html">
   Invertible Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="FutureWork.html">
   Future Work
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="References.html">
   References
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/ConvertLatex.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/ConvertLatex.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   old, copied
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#visulization-table-with-empty-papers">
     visulization table with empty papers
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#layer-stuff">
   layer stuff
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>old, copied</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   old, copied
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#visulization-table-with-empty-papers">
     visulization table with empty papers
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#layer-stuff">
   layer stuff
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <div class="section" id="old-copied">
<h1>old, copied<a class="headerlink" href="#old-copied" title="Permalink to this headline">#</a></h1>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:left head"><p>Study</p></th>
<th class="head"><p>Decoding problem</p></th>
<th class="head"><p>External Baseline</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:left"><p>This manuscript, Schirrmeister et. al (2017)</p></td>
<td><p>Imagined and executed movement classes, within subject</p></td>
<td><p>FBCSP + rLDA</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Single-trial EEG classification of motor imagery using deep convolutional neural networks, citet{tang_single-trial_2017}</p></td>
<td><p>Imagined movement classes, within-subject</p></td>
<td><p>FBCSP</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>EEGNet: A Compact Convolutional Network for EEG-based Brain-Computer Interfaces, <span id="id1">[<a class="reference internal" href="References.html#id150" title="Vernon J. Lawhern, Amelia J. Solon, Nicholas R. Waytowich, Stephen M. Gordon, Chou P. Hung, and Brent J. Lance. EEGNet: A Compact Convolutional Network for EEG-based Brain-Computer Interfaces. arXiv:1611.08024 [cs, q-bio, stat], November 2016. arXiv: 1611.08024. URL: http://arxiv.org/abs/1611.08024 (visited on 2016-12-20).">Lawhern <em>et al.</em>, 2016</a>]</span></p></td>
<td><p>Oddball response (RSVP), error response (ERN), movement classes (voluntarily started and imagined)</p></td>
<td><p>[recheck]</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Remembered or Forgotten? –- An EEG-Based Computational Prediction Approach, <span id="id2">[<a class="reference internal" href="References.html#id222" title="Xuyun Sun, Cunle Qian, Zhongqin Chen, Zhaohui Wu, Benyan Luo, and Gang Pan. Remembered or Forgotten?—An EEG-Based Computational Prediction Approach. PLOS ONE, 11(12):e0167497, December 2016. URL: http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0167497 (visited on 2017-02-14), doi:10.1371/journal.pone.0167497.">Sun <em>et al.</em>, 2016</a>]</span></p></td>
<td><p>Memory performance, within-subject</p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Multimodal Neural Network for Rapid Serial Visual Presentation Brain Computer Interface, <span id="id3">[<a class="reference internal" href="References.html#id196" title="Ran Manor, Liran Mishali, and Amir B. Geva. Multimodal Neural Network for Rapid Serial Visual Presentation Brain Computer Interface. Frontiers in Computational Neuroscience, December 2016. URL: http://www.ncbi.nlm.nih.gov/pmc/articles/PMC5168930/ (visited on 2017-02-03), doi:10.3389/fncom.2016.00130.">Manor <em>et al.</em>, 2016</a>]</span></p></td>
<td><p>Oddball response using RSVP and image (combined image-EEG decoding), within-subject</p></td>
<td><p>Time, 0.3–20 Hz</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>A novel deep learning approach for classification of EEG motor imagery signals, <span id="id4">[<a class="reference internal" href="References.html#id215" title="Yousef Rezaei Tabar and Ugur Halici. A novel deep learning approach for classification of EEG motor imagery signals. Journal of Neural Engineering, 14(1):016003, 2017. URL: http://stacks.iop.org/1741-2552/14/i=1/a=016003 (visited on 2017-02-14), doi:10.1088/1741-2560/14/1/016003.">Tabar and Halici, 2017</a>]</span></p></td>
<td><p>Imagined and executed movement classes, within-subject</p></td>
<td><p>Frequency, 6–30 Hz</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Predicting Seizures from Electroencephalography Recordings: A Knowledge Transfer Strategy, <span id="id5">[<a class="reference internal" href="References.html#id172" title="J. Liang, R. Lu, C. Zhang, and F. Wang. Predicting Seizures from Electroencephalography Recordings: A Knowledge Transfer Strategy. In 2016 IEEE International Conference on Healthcare Informatics (ICHI), 184–191. October 2016. doi:10.1109/ICHI.2016.27.">Liang <em>et al.</em>, 2016</a>]</span></p></td>
<td><p>Seizure prediction, within-subject</p></td>
<td><p>Frequency, 0–200 Hz</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>EEG-based prediction of driver’s cognitive performance by deep convolutional neural network, <span id="id6">[<a class="reference internal" href="References.html#id180" title="Mehdi Hajinoroozi, Zijing Mao, Tzyy-Ping Jung, Chin-Teng Lin, and Yufei Huang. EEG-based prediction of driver's cognitive performance by deep convolutional neural network. Signal Processing: Image Communication, 47:549–555, September 2016. URL: http://www.sciencedirect.com/science/article/pii/S0923596516300832 (visited on 2016-12-20), doi:10.1016/j.image.2016.05.018.">Hajinoroozi <em>et al.</em>, 2016</a>]</span></p></td>
<td><p>Driver performance, within- and cross-subject</p></td>
<td><p>Time,  1–50 Hz</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Deep learning for epileptic intracranial EEG data, <span id="id7">[<a class="reference internal" href="References.html#id154" title="A. Antoniades, L. Spyrou, C. C. Took, and S. Sanei. Deep learning for epileptic intracranial EEG data. In 2016 IEEE 26th International Workshop on Machine Learning for Signal Processing (MLSP), 1–6. September 2016. doi:10.1109/MLSP.2016.7738824.">Antoniades <em>et al.</em>, 2016</a>]</span></p></td>
<td><p>Epileptic discharges, cross-subject</p></td>
<td><p>Time,  0–100 HZ</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Learning Robust Features using Deep Learning for Automatic Seizure Detection, <span id="id8">[<a class="reference internal" href="References.html#id182" title="Pierre Thodoroff, Joelle Pineau, and Andrew Lim. Learning Robust Features using Deep Learning for Automatic Seizure Detection. In JMLR Workshop and Conference Proceedings, volume 56. 2016. URL: http://www.jmlr.org/proceedings/papers/v56/Thodoroff16.pdf (visited on 2017-02-14).">Thodoroff <em>et al.</em>, 2016</a>]</span></p></td>
<td><p>Start of epileptic seizure, within- and cross-subject</p></td>
<td><p>Frequency, mean amplitude for 0–7 Hz, 7–14 Hz, 14–49 Hz</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Single-trial EEG RSVP classification using convolutional neural networks, <span id="id9">[<a class="reference internal" href="References.html#id200" title="Jared Shamwell, Hyungtae Lee, Heesung Kwon, Amar R. Marathe, Vernon Lawhern, and William Nothwang. Single-trial EEG RSVP classification using convolutional neural networks. In Thomas George, Achyut K. Dutta, and M. Saif Islam, editors, SPIE Defense+ Security, volume 9836. International Society for Optics and Photonics, May 2016. URL: http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.2224172 (visited on 2017-02-14), doi:10.1117/12.2224172.">Shamwell <em>et al.</em>, 2016</a>]</span></p></td>
<td><p>Oddball response (RSVP), groupwise (ConvNet trained on all subjects)</p></td>
<td><p>Time, 0.5–50 Hz</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Wearable seizure detection using convolutional neural networks with transfer learning, <span id="id10">[<a class="reference internal" href="References.html#id204" title="A. Page, C. Shea, and T. Mohsenin. Wearable seizure detection using convolutional neural networks with transfer learning. In 2016 IEEE International Symposium on Circuits and Systems (ISCAS), 1086–1089. May 2016. doi:10.1109/ISCAS.2016.7527433.">Page <em>et al.</em>, 2016</a>]</span></p></td>
<td><p>Seizure detection, cross-subject, within-subject, groupwise</p></td>
<td><p>Time,  0–128 Hz</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Learning Representations from EEG with Deep Recurrent-Convolutional Neural Networks, <span id="id11">[<a class="reference internal" href="References.html#id197" title="Pouya Bashivan, Irina Rish, Mohammed Yeasin, and Noel Codella. Learning Representations from EEG with Deep Recurrent-Convolutional Neural Networks. In arXiv:1511.06448 [cs]. 2016. arXiv: 1511.06448. URL: http://arxiv.org/abs/1511.06448 (visited on 2016-12-20).">Bashivan <em>et al.</em>, 2016</a>]</span></p></td>
<td><p>Cognitive load (number of characters to memorize), cross-subject</p></td>
<td><p>Frequency, mean power for 4–7 Hz, 8–13 Hz, 13–30 Hz</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Deep Feature Learning for EEG Recordings, <span id="id12">[<a class="reference internal" href="References.html#id226" title="Sebastian Stober. Learning Discriminative Features from Electroencephalography Recordings by Encoding Similarity Constraints. In Bernstein Conference 2016. 2016. doi:10.12751/nncn.bc2016.0223.">Stober, 2016</a>]</span></p></td>
<td><p>Type of music rhythm, groupwise (ensembles of leave-one-subject-out trained models, evaluated on separate test set of same subjects)</p></td>
<td><p>Time, 0.5–30Hz</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Convolutional Neural Network for Multi-Category Rapid Serial Visual Presentation BCI, <span id="id13">[<a class="reference internal" href="References.html#id147" title="Ran Manor and Amir B. Geva. Convolutional Neural Network for Multi-Category Rapid Serial Visual Presentation BCI. Frontiers in Computational Neuroscience, 9:146, 2015. doi:10.3389/fncom.2015.00146.">Manor and Geva, 2015</a>]</span></p></td>
<td><p>Oddball response (RSVP), within-subject</p></td>
<td><p>Time, 0.1–50 Hz</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Parallel Convolutional-Linear Neural Network for Motor Imagery Classification, <span id="id14">[<a class="reference internal" href="References.html#id117" title="S. Sakhavi, C. Guan, and S. Yan. Parallel convolutional-linear neural network for motor imagery classification. In Signal Processing Conference (EUSIPCO), 2015 23rd European, 2736–2740. August 2015. doi:10.1109/EUSIPCO.2015.7362882.">Sakhavi <em>et al.</em>, 2015</a>]</span></p></td>
<td><p>Imagined movement classes, within-subject</p></td>
<td><p>Frequency, 4–40 Hz, using FBCSP</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Using Convolutional Neural networks to Recognize Rhythm Stimuli form Electroencephalography Recordings, <span id="id15">[<a class="reference internal" href="References.html#id184" title="Sebastian Stober, Daniel J. Cameron, and Jessica A. Grahn. Using Convolutional Neural Networks to Recognize Rhythm Stimuli from Electroencephalography Recordings. In Proceedings of the 27th International Conference on Neural Information Processing Systems, NIPS'14, 1449–1457. Cambridge, MA, USA, 2014. MIT Press. URL: http://dl.acm.org/citation.cfm?id=2968826.2968988 (visited on 2016-12-20).">Stober <em>et al.</em>, 2014</a>]</span></p></td>
<td><p>Type of music rhythm, within-subject</p></td>
<td><p>Time and frequency evaluated, 0-200 Hz</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Convolutional deep belief networks for feature extraction of EEG signal, <span id="id16">[<a class="reference internal" href="References.html#id137" title="Y. Ren and Y. Wu. Convolutional deep belief networks for feature extraction of EEG signal. In 2014 International Joint Conference on Neural Networks (IJCNN), 2850–2853. July 2014. doi:10.1109/IJCNN.2014.6889383.">Ren and Wu, 2014</a>]</span></p></td>
<td><p>Imagined movement classes, within-subject</p></td>
<td><p>Frequency, 8–30 Hz</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Deep feature learning using target priors with applications in ECoG signal decoding for BCI, <span id="id17">[<a class="reference internal" href="References.html#id209" title="Zuoguan Wang, Siwei Lyu, Gerwin Schalk, and Qiang Ji. Deep Feature Learning Using Target Priors with Applications in ECoG Signal Decoding for BCI. In Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence, IJCAI '13, 1785–1791. Beijing, China, 2013. AAAI Press. URL: http://dl.acm.org/citation.cfm?id=2540128.2540384 (visited on 2017-01-16).">Wang <em>et al.</em>, 2013</a>]</span></p></td>
<td><p>Finger flexion trajectory (regression), within-subject</p></td>
<td><p>Time, 0.15–200 Hz</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Convolutional neural networks for P300 detection with application to brain-computer interfaces, <span id="id18">[<a class="reference internal" href="References.html#id212" title="Hubert Cecotti and Axel Graser. Convolutional Neural Networks for P300 Detection with Application to Brain-Computer Interfaces. IEEE Transactions on Pattern Analysis and Machine Intelligence, 33(3):433–445, March 2011. URL: http://dx.doi.org/10.1109/TPAMI.2010.125 (visited on 2016-12-20), doi:10.1109/TPAMI.2010.125.">Cecotti and Graser, 2011</a>]</span></p></td>
<td><p>Oddball / attention response using P300 speller, within-subject</p></td>
<td><p>Time, 0.1-20 Hz</p></td>
</tr>
</tbody>
</table>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:left head"><p>Study</p></th>
<th class="head"><p>Decoding problem</p></th>
<th class="head"><p>Input domain</p></th>
<th class="head"><p>Conv/dense layers</p></th>
<th class="head"><p>Design choices</p></th>
<th class="head"><p>Training strategies</p></th>
<th class="head"><p>External baseline</p></th>
<th class="head"><p>Visualization type(s)</p></th>
<th class="head"><p>Visualization findings</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:left"><p>This manuscript, Schirrmeister et. al (2017)</p></td>
<td><p>Imagined and executed movement classes, within subject</p></td>
<td><p>Time,  0–125 Hz</p></td>
<td><p>5/1</p></td>
<td><p>Different ConvNet architectures <br>Nonlinearities and pooling modes <br>Regularization and intermediate normalization layers <br>Factorized convolutions <br>Splitted vs one-step convolutions</p></td>
<td><p>Trial-wise vs. cropped training strategy</p></td>
<td><p>FBCSP + rLDA</p></td>
<td><p>Feature activation correlation <br>Feature-perturbation prediction correlation</p></td>
<td><p>See Section \ref{subsec:results-visualization}</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Single-trial EEG classification of motor imagery using deep convolutional neural networks, citet{tang_single-trial_2017}</p></td>
<td><p>Imagined movement classes, within-subject</p></td>
<td><p>Time,  8–30 Hz</p></td>
<td><p>2/2</p></td>
<td><p></p></td>
<td><p></p></td>
<td><p>FBCSP</p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>EEGNet: A Compact Convolutional Network for EEG-based Brain-Computer Interfaces, <span id="id19">[<a class="reference internal" href="References.html#id150" title="Vernon J. Lawhern, Amelia J. Solon, Nicholas R. Waytowich, Stephen M. Gordon, Chou P. Hung, and Brent J. Lance. EEGNet: A Compact Convolutional Network for EEG-based Brain-Computer Interfaces. arXiv:1611.08024 [cs, q-bio, stat], November 2016. arXiv: 1611.08024. URL: http://arxiv.org/abs/1611.08024 (visited on 2016-12-20).">Lawhern <em>et al.</em>, 2016</a>]</span></p></td>
<td><p>Oddball response (RSVP), error response (ERN), movement classes (voluntarily started and imagined)</p></td>
<td><p>Time, 0.1–40 Hz</p></td>
<td><p>3/1</p></td>
<td><p>Kernel sizes</p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Remembered or Forgotten? –- An EEG-Based Computational Prediction Approach, <span id="id20">[<a class="reference internal" href="References.html#id222" title="Xuyun Sun, Cunle Qian, Zhongqin Chen, Zhaohui Wu, Benyan Luo, and Gang Pan. Remembered or Forgotten?—An EEG-Based Computational Prediction Approach. PLOS ONE, 11(12):e0167497, December 2016. URL: http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0167497 (visited on 2017-02-14), doi:10.1371/journal.pone.0167497.">Sun <em>et al.</em>, 2016</a>]</span></p></td>
<td><p>Memory performance, within-subject</p></td>
<td><p>Time, 0.05–15 Hz</p></td>
<td><p>2/2</p></td>
<td><p></p></td>
<td><p>Different time windows</p></td>
<td><p></p></td>
<td><p>Weights (spatial)</p></td>
<td><p>Largest weights found over p\refrontal and temporal cortex</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Multimodal Neural Network for Rapid Serial Visual Presentation Brain Computer Interface, <span id="id21">[<a class="reference internal" href="References.html#id196" title="Ran Manor, Liran Mishali, and Amir B. Geva. Multimodal Neural Network for Rapid Serial Visual Presentation Brain Computer Interface. Frontiers in Computational Neuroscience, December 2016. URL: http://www.ncbi.nlm.nih.gov/pmc/articles/PMC5168930/ (visited on 2017-02-03), doi:10.3389/fncom.2016.00130.">Manor <em>et al.</em>, 2016</a>]</span></p></td>
<td><p>Oddball response using RSVP and image (combined image-EEG decoding), within-subject</p></td>
<td><p>Time, 0.3–20 Hz</p></td>
<td><p>3/2</p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p>Weights <br> Activations <br> Saliency maps by gradient</p></td>
<td><p>Weights showed typical P300 distribution <br>Activations were high at plausible times (300-500ms) <br>Saliency maps showed plausible spatio-temporal plots</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>A novel deep learning approach for classification of EEG motor imagery signals, <span id="id22">[<a class="reference internal" href="References.html#id215" title="Yousef Rezaei Tabar and Ugur Halici. A novel deep learning approach for classification of EEG motor imagery signals. Journal of Neural Engineering, 14(1):016003, 2017. URL: http://stacks.iop.org/1741-2552/14/i=1/a=016003 (visited on 2017-02-14), doi:10.1088/1741-2560/14/1/016003.">Tabar and Halici, 2017</a>]</span></p></td>
<td><p>Imagined and executed movement classes, within-subject</p></td>
<td><p>Frequency, 6–30 Hz</p></td>
<td><p>1/1</p></td>
<td><p>multicolumn{2}{p{0.285	extwidth}}{Addition of six-layer stacked autoencoder on ConvNet features <br> Kernel sizes}</p></td>
<td><p>FBCSP, Twin SVM, DDFBS, Bi-spectrum, RQNN</p></td>
<td><p>Weights (spatial + frequential)</p></td>
<td><p>Some weights represented difference of values of two electrodes on different sides of head</p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Predicting Seizures from Electroencephalography Recordings: A Knowledge Transfer Strategy, <span id="id23">[<a class="reference internal" href="References.html#id172" title="J. Liang, R. Lu, C. Zhang, and F. Wang. Predicting Seizures from Electroencephalography Recordings: A Knowledge Transfer Strategy. In 2016 IEEE International Conference on Healthcare Informatics (ICHI), 184–191. October 2016. doi:10.1109/ICHI.2016.27.">Liang <em>et al.</em>, 2016</a>]</span></p></td>
<td><p>Seizure prediction, within-subject</p></td>
<td><p>Frequency, 0–200 Hz</p></td>
<td><p>1/2</p></td>
<td><p></p></td>
<td><p>Different subdivisions of frequency range <br>Different lengths of time crops <br>Transfer learning with auxiliary non-epilepsy datasets</p></td>
<td><p></p></td>
<td><p>Weights <br> Clustering of weights</p></td>
<td><p>Clusters of weights showed typical frequency band subdivision (delta, theta, alpha, beta, gamma)</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>EEG-based prediction of driver’s cognitive performance by deep convolutional neural network, <span id="id24">[<a class="reference internal" href="References.html#id180" title="Mehdi Hajinoroozi, Zijing Mao, Tzyy-Ping Jung, Chin-Teng Lin, and Yufei Huang. EEG-based prediction of driver's cognitive performance by deep convolutional neural network. Signal Processing: Image Communication, 47:549–555, September 2016. URL: http://www.sciencedirect.com/science/article/pii/S0923596516300832 (visited on 2016-12-20), doi:10.1016/j.image.2016.05.018.">Hajinoroozi <em>et al.</em>, 2016</a>]</span></p></td>
<td><p>Driver performance, within- and cross-subject</p></td>
<td><p>Time,  1–50 Hz</p></td>
<td><p>1/3</p></td>
<td><p>multicolumn{2}{p{0.285	extwidth}}{Replacement of convolutional layers by restricted Boltzmann machines with slightly varied network architecture}</p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Deep learning for epileptic intracranial EEG data, <span id="id25">[<a class="reference internal" href="References.html#id154" title="A. Antoniades, L. Spyrou, C. C. Took, and S. Sanei. Deep learning for epileptic intracranial EEG data. In 2016 IEEE 26th International Workshop on Machine Learning for Signal Processing (MLSP), 1–6. September 2016. doi:10.1109/MLSP.2016.7738824.">Antoniades <em>et al.</em>, 2016</a>]</span></p></td>
<td><p>Epileptic discharges, cross-subject</p></td>
<td><p>Time,  0–100 HZ</p></td>
<td><p>1–2/2</p></td>
<td><p>1 or 2 convolutional layers</p></td>
<td><p></p></td>
<td><p></p></td>
<td><p>Weights <br>Correlation weights and interictal epileptic discharges (IED) <br>Activations</p></td>
<td><p>Weights increasingly correlated with IED waveforms with increasing number of training iterations <br>Second layer captured more complex and well-defined epileptic shapes than first layer <br>IEDs led to highly synchronized activations for neighbouring electrodes</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Learning Robust Features using Deep Learning for Automatic Seizure Detection, <span id="id26">[<a class="reference internal" href="References.html#id182" title="Pierre Thodoroff, Joelle Pineau, and Andrew Lim. Learning Robust Features using Deep Learning for Automatic Seizure Detection. In JMLR Workshop and Conference Proceedings, volume 56. 2016. URL: http://www.jmlr.org/proceedings/papers/v56/Thodoroff16.pdf (visited on 2017-02-14).">Thodoroff <em>et al.</em>, 2016</a>]</span></p></td>
<td><p>Start of epileptic seizure, within- and cross-subject</p></td>
<td><p>Frequency, mean amplitude for 0–7 Hz, 7–14 Hz, 14–49 Hz</p></td>
<td><p>3/1 (+ LSTM as postprocessor)</p></td>
<td><p></p></td>
<td><p></p></td>
<td><p>Hand crafted features + SVM</p></td>
<td><p>Input occlusion and effect on prediction accuracy</p></td>
<td><p>Allowed to locate areas critical for seizure</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Single-trial EEG RSVP classification using convolutional neural networks, <span id="id27">[<a class="reference internal" href="References.html#id200" title="Jared Shamwell, Hyungtae Lee, Heesung Kwon, Amar R. Marathe, Vernon Lawhern, and William Nothwang. Single-trial EEG RSVP classification using convolutional neural networks. In Thomas George, Achyut K. Dutta, and M. Saif Islam, editors, SPIE Defense+ Security, volume 9836. International Society for Optics and Photonics, May 2016. URL: http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.2224172 (visited on 2017-02-14), doi:10.1117/12.2224172.">Shamwell <em>et al.</em>, 2016</a>]</span></p></td>
<td><p>Oddball response (RSVP), groupwise (ConvNet trained on all subjects)</p></td>
<td><p>Time, 0.5–50 Hz</p></td>
<td><p>4/3</p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p>Weights (spatial)</p></td>
<td><p>Some filter weights had expected topographic distributions for P300 <br>Others filters had large weights on areas not traditionally associated with P300</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Wearable seizure detection using convolutional neural networks with transfer learning, <span id="id28">[<a class="reference internal" href="References.html#id204" title="A. Page, C. Shea, and T. Mohsenin. Wearable seizure detection using convolutional neural networks with transfer learning. In 2016 IEEE International Symposium on Circuits and Systems (ISCAS), 1086–1089. May 2016. doi:10.1109/ISCAS.2016.7527433.">Page <em>et al.</em>, 2016</a>]</span></p></td>
<td><p>Seizure detection, cross-subject, within-subject, groupwise</p></td>
<td><p>Time,  0–128 Hz</p></td>
<td><p>1-3/1-3</p></td>
<td><p></p></td>
<td><p>Cross-subject supervised training, within-subject finetuning of fully connected layers</p></td>
<td><p>Multiple: spectral features, higher order statistics + linear-SVM, RBF-SVM, …</p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Learning Representations from EEG with Deep Recurrent-Convolutional Neural Networks, <span id="id29">[<a class="reference internal" href="References.html#id197" title="Pouya Bashivan, Irina Rish, Mohammed Yeasin, and Noel Codella. Learning Representations from EEG with Deep Recurrent-Convolutional Neural Networks. In arXiv:1511.06448 [cs]. 2016. arXiv: 1511.06448. URL: http://arxiv.org/abs/1511.06448 (visited on 2016-12-20).">Bashivan <em>et al.</em>, 2016</a>]</span></p></td>
<td><p>Cognitive load (number of characters to memorize), cross-subject</p></td>
<td><p>Frequency, mean power for 4–7 Hz, 8–13 Hz, 13–30 Hz</p></td>
<td><p>3–7/2 (+ LSTM or other temporal post-processing (see design choices))</p></td>
<td><p>Number of convolutional layers <br>Temporal processing of ConvNet output by max pooling, temporal convolution, LSTM or temporal convolution + LSTM</p></td>
<td><p></p></td>
<td><p></p></td>
<td><p>Inputs that maximally activate given filter <br>Activations of these inputs <br>”Deconvolution” for these inputs</p></td>
<td><p>Different filters were sensitive to different frequency bands <br>Later layers had more spatially localized activations <br>Learned features had noticeable links to well-known electrophysiological markers of cognitive load <br></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Deep Feature Learning for EEG Recordings, <span id="id30">[<a class="reference internal" href="References.html#id226" title="Sebastian Stober. Learning Discriminative Features from Electroencephalography Recordings by Encoding Similarity Constraints. In Bernstein Conference 2016. 2016. doi:10.12751/nncn.bc2016.0223.">Stober, 2016</a>]</span></p></td>
<td><p>Type of music rhythm, groupwise (ensembles of leave-one-subject-out trained models, evaluated on separate test set of same subjects)</p></td>
<td><p>Time, 0.5–30Hz</p></td>
<td><p>2/1</p></td>
<td><p>Kernel sizes</p></td>
<td><p>Pretraining first layer as convolutional autoencoder with different constraints</p></td>
<td><p></p></td>
<td><p>Weights (spatial+3 timesteps, pretrained as autoencoder)</p></td>
<td><p>Different constraints led to different weights, one type of constraints could enforce weights that are similar across subjects; other type of constraints led to weights that have similar spatial topographies under different architectural configurations and preprocessings</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Convolutional Neural Network for Multi-Category Rapid Serial Visual Presentation BCI, <span id="id31">[<a class="reference internal" href="References.html#id147" title="Ran Manor and Amir B. Geva. Convolutional Neural Network for Multi-Category Rapid Serial Visual Presentation BCI. Frontiers in Computational Neuroscience, 9:146, 2015. doi:10.3389/fncom.2015.00146.">Manor and Geva, 2015</a>]</span></p></td>
<td><p>Oddball response (RSVP), within-subject</p></td>
<td><p>Time, 0.1–50 Hz</p></td>
<td><p>3/3 (Spatio-temporal regularization)</p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p>Weights <br> Mean and single-trial activations</p></td>
<td><p>Spatiotemporal regularization led to softer peaks in weights <br>Spatial weights showed typical P300 distribution <br>Activations mostly had peaks at typical times (300-400ms)</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Parallel Convolutional-Linear Neural Network for Motor Imagery Classification, <span id="id32">[<a class="reference internal" href="References.html#id117" title="S. Sakhavi, C. Guan, and S. Yan. Parallel convolutional-linear neural network for motor imagery classification. In Signal Processing Conference (EUSIPCO), 2015 23rd European, 2736–2740. August 2015. doi:10.1109/EUSIPCO.2015.7362882.">Sakhavi <em>et al.</em>, 2015</a>]</span></p></td>
<td><p>Imagined movement classes, within-subject</p></td>
<td><p>Frequency, 4–40 Hz, using FBCSP</p></td>
<td><p>2/2 (Final fully connected layer uses concatenated output by convolutionaland fully connected layers)</p></td>
<td><p>Combination ConvNet and MLP (trained on different features) vs. only ConvNet vs. only MLP</p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Using Convolutional Neural networks to Recognize Rhythm Stimuli form Electroencephalography Recordings, <span id="id33">[<a class="reference internal" href="References.html#id184" title="Sebastian Stober, Daniel J. Cameron, and Jessica A. Grahn. Using Convolutional Neural Networks to Recognize Rhythm Stimuli from Electroencephalography Recordings. In Proceedings of the 27th International Conference on Neural Information Processing Systems, NIPS'14, 1449–1457. Cambridge, MA, USA, 2014. MIT Press. URL: http://dl.acm.org/citation.cfm?id=2968826.2968988 (visited on 2016-12-20).">Stober <em>et al.</em>, 2014</a>]</span></p></td>
<td><p>Type of music rhythm, within-subject</p></td>
<td><p>Time and frequency evaluated, 0-200 Hz</p></td>
<td><p>1-2/1</p></td>
<td><p>Best values from automatic hyperparameter optimization: frequency cutoff, one vs two layers, kernel sizes, number of channels, pooling width</p></td>
<td><p>Best values from automatic hyperparameter optimization: learning rate, learning rate decay, momentum, final momentum</p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Convolutional deep belief networks for feature extraction of EEG signal, <span id="id34">[<a class="reference internal" href="References.html#id137" title="Y. Ren and Y. Wu. Convolutional deep belief networks for feature extraction of EEG signal. In 2014 International Joint Conference on Neural Networks (IJCNN), 2850–2853. July 2014. doi:10.1109/IJCNN.2014.6889383.">Ren and Wu, 2014</a>]</span></p></td>
<td><p>Imagined movement classes, within-subject</p></td>
<td><p>Frequency, 8–30 Hz</p></td>
<td><p>2/0 (Convolutional deep belief network, separately trained RBF-SVM classifier)</p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Deep feature learning using target priors with applications in ECoG signal decoding for BCI, <span id="id35">[<a class="reference internal" href="References.html#id209" title="Zuoguan Wang, Siwei Lyu, Gerwin Schalk, and Qiang Ji. Deep Feature Learning Using Target Priors with Applications in ECoG Signal Decoding for BCI. In Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence, IJCAI '13, 1785–1791. Beijing, China, 2013. AAAI Press. URL: http://dl.acm.org/citation.cfm?id=2540128.2540384 (visited on 2017-01-16).">Wang <em>et al.</em>, 2013</a>]</span></p></td>
<td><p>Finger flexion trajectory (regression), within-subject</p></td>
<td><p>Time, 0.15–200 Hz</p></td>
<td><p>3/1 (Convolutional layers trained as convolutional stacked autoencoder with target prior)</p></td>
<td><p>Partially supervised CSA</p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Convolutional neural networks for P300 detection with application to brain-computer interfaces, <span id="id36">[<a class="reference internal" href="References.html#id212" title="Hubert Cecotti and Axel Graser. Convolutional Neural Networks for P300 Detection with Application to Brain-Computer Interfaces. IEEE Transactions on Pattern Analysis and Machine Intelligence, 33(3):433–445, March 2011. URL: http://dx.doi.org/10.1109/TPAMI.2010.125 (visited on 2016-12-20), doi:10.1109/TPAMI.2010.125.">Cecotti and Graser, 2011</a>]</span></p></td>
<td><p>Oddball / attention response using P300 speller, within-subject</p></td>
<td><p>Time, 0.1-20 Hz</p></td>
<td><p>2/2</p></td>
<td><p>Electrode subset (fixed or automatically determined) <br>Using only one spatial filter <br>Different ensembling strategies</p></td>
<td><p></p></td>
<td><p>Multiple: Linear SVM, gradient boosting, E-SVM, S-SVM, mLVQ, LDA, …</p></td>
<td><p>Weights</p></td>
<td><p>Spatial filters were similar for different architectures <br>Spatial filters were different (more focal, more diffuse) for different subjects</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">re</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">This manuscript, Schirrmeister et. al (2017) &amp;</span>
<span class="s2">Imagined and executed movement classes, within subject &amp;</span>
<span class="s2">Time, \hspace</span><span class="si">{1cm}</span><span class="s2"> 0--125 Hz &amp; 5/1 &amp;</span>
<span class="s2">Different ConvNet architectures \cellbr</span>
<span class="s2">Nonlinearities and pooling modes \cellbr</span>
<span class="s2">Regularization and intermediate normalization layers \cellbr</span>
<span class="s2">Factorized convolutions \cellbr</span>
<span class="s2">Splitted vs one-step convolutions &amp;</span>
<span class="s2">Trial-wise vs. cropped training strategy &amp;</span>
<span class="s2">FBCSP + rLDA &amp; </span>
<span class="s2">Feature activation correlation \cellbr</span>
<span class="s2">Feature-perturbation prediction correlation &amp;</span>
<span class="s2">See Section \ref{subsec:results-visualization}</span>
<span class="s2">\\</span>
<span class="s2">\hdashline </span>

<span class="s2">Single-trial EEG classification of motor imagery using deep convolutional neural networks, \citet{tang_single-trial_2017} &amp;</span>
<span class="s2">Imagined movement classes, within-subject &amp;</span>
<span class="s2">Time, \hspace</span><span class="si">{1cm}</span><span class="s2"> 8--30 Hz &amp; 2/2 &amp; &amp; </span>
<span class="s2">&amp; FBCSP &amp; &amp;&quot;&quot;&quot;</span>

<span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;&amp;&quot;</span><span class="p">,</span> <span class="s2">&quot;|&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;\cellbr&quot;</span><span class="p">,</span> <span class="s2">&quot;&lt;br&gt;&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;\hdashline&#39;</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">|&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\\</span><span class="s1">&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;ref&quot;</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;\ref&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="s1">&#39;–&#39;</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;hspace{[^}]+}&#39;</span><span class="p">,</span> <span class="sa">r</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="s2">&quot;| &quot;</span> <span class="o">+</span> <span class="n">b</span> <span class="o">+</span> <span class="s2">&quot; |&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>| This manuscript, Schirrmeister et. al (2017) |Imagined and executed movement classes, within subject |Time,  0–125 Hz | 5/1 |Different ConvNet architectures &lt;br&gt;Nonlinearities and pooling modes &lt;br&gt;Regularization and intermediate normalization layers &lt;br&gt;Factorized convolutions &lt;br&gt;Splitted vs one-step convolutions |Trial-wise vs. cropped training strategy |FBCSP + rLDA | Feature activation correlation &lt;br&gt;Feature-perturbation prediction correlation |See Section \ref{subsec:results-visualization}
| Single-trial EEG classification of motor imagery using deep convolutional neural networks, citet{tang_single-trial_2017} |Imagined movement classes, within-subject |Time,  8–30 Hz | 2/2 | | | FBCSP | | |
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">EEGNet: A Compact Convolutional Network for EEG-based Brain-Computer Interfaces, \citet{lawhern_eegnet:_2016} &amp; </span>
<span class="s2">Oddball response (RSVP), error response (ERN), movement classes (voluntarily started and imagined) &amp;</span>
<span class="s2">Time, 0.1--40 Hz &amp; 3/1 &amp;  Kernel sizes &amp; </span>
<span class="s2">&amp;   &amp; &amp;</span>
<span class="se">\\</span><span class="s2"></span>
<span class="s2">\hdashline</span>
<span class="s2"> </span>
<span class="s2">Remembered or Forgotten? --- An EEG-Based Computational Prediction Approach, \citet</span><span class="si">{sun_remembered_2016}</span><span class="s2"> &amp; </span>
<span class="s2">Memory performance, within-subject &amp;</span>
<span class="s2">Time, 0.05--15 Hz &amp; 2/2 &amp; &amp; Different time windows &amp;  &amp;</span>
<span class="s2">Weights (spatial) &amp; </span>
<span class="s2">Largest weights found over prefrontal and temporal cortex</span>
<span class="se">\\</span><span class="s2"></span>
<span class="s2">\hdashline</span>

<span class="s2">Multimodal Neural Network for Rapid Serial Visual Presentation Brain Computer Interface, \citet</span><span class="si">{manor_multimodal_2016}</span><span class="s2"></span>
<span class="s2">&amp;</span>
<span class="s2">Oddball response using RSVP and image (combined image-EEG decoding), within-subject&amp;</span>
<span class="s2">Time, 0.3--20 Hz &amp; 3/2 &amp; &amp; &amp;  &amp; </span>
<span class="s2">Weights \cellbr Activations \cellbr Saliency maps by gradient &amp;</span>
<span class="s2">Weights showed typical P300 distribution \cellbr</span>
<span class="s2">Activations were high at plausible times (300-500ms) \cellbr</span>
<span class="s2">Saliency maps showed plausible spatio-temporal plots</span>
<span class="se">\\</span><span class="s2"></span>
<span class="s2">\hdashline</span>
<span class="s2"> </span>
<span class="s2">A novel deep learning approach for classification of EEG motor imagery signals, \citet</span><span class="si">{tabar_novel_2017}</span><span class="s2"> &amp;</span>
<span class="s2">Imagined and executed movement classes, within-subject &amp;</span>
<span class="s2">Frequency, 6--30 Hz &amp; 1/1 &amp; </span>
<span class="s2">\multicolumn</span><span class="si">{2}</span><span class="s2">{p{0.285</span><span class="se">\t</span><span class="s2">extwidth}}{Addition of six-layer stacked autoencoder on ConvNet features \cellbr Kernel sizes} </span>
<span class="s2">&amp; FBCSP, Twin SVM, DDFBS, Bi-spectrum, RQNN  &amp; Weights (spatial + frequential) &amp;</span>
<span class="s2">Some weights represented difference of values of two electrodes on different sides of head</span>
<span class="se">\\</span><span class="s2"></span>
<span class="s2">\hdashline</span>
<span class="s2"> </span>
<span class="s2">Predicting Seizures from Electroencephalography Recordings: A Knowledge Transfer Strategy, \citet</span><span class="si">{liang_predicting_2016}</span><span class="s2"> &amp;</span>
<span class="s2">Seizure prediction, within-subject &amp; Frequency, 0--200 Hz &amp; 1/2 &amp; &amp; </span>
<span class="s2">Different subdivisions of frequency range \cellbr</span>
<span class="s2">Different lengths of time crops \cellbr</span>
<span class="s2">Transfer learning with auxiliary non-epilepsy datasets &amp;</span>
<span class="s2">&amp; Weights \cellbr Clustering of weights &amp;</span>
<span class="s2">Clusters of weights showed typical frequency band subdivision (delta, theta, alpha, beta, gamma)</span>
<span class="se">\\</span><span class="s2"></span>
<span class="s2">\hdashline</span>
<span class="s2"> </span>
<span class="s2">EEG-based prediction of driver&#39;s cognitive performance by deep convolutional neural network, \citet{hajinoroozi_eeg-based_2016} &amp;</span>
<span class="s2">Driver performance, within- and cross-subject &amp;</span>
<span class="s2">Time, \hspace</span><span class="si">{1cm}</span><span class="s2"> 1--50 Hz &amp; 1/3 &amp;</span>
<span class="s2">\multicolumn</span><span class="si">{2}</span><span class="s2">{p{0.285</span><span class="se">\t</span><span class="s2">extwidth}}{Replacement of convolutional layers by restricted Boltzmann machines with slightly varied network architecture}  &amp; </span>
<span class="s2">&amp;</span>
<span class="se">\\</span><span class="s2"></span>
<span class="s2">\hdashline</span>
<span class="s2"> </span>
<span class="s2">Deep learning for epileptic intracranial EEG data, \citet</span><span class="si">{antoniades_deep_2016}</span><span class="s2"> &amp;</span>
<span class="s2">Epileptic discharges, cross-subject &amp; Time, \hspace</span><span class="si">{1cm}</span><span class="s2"> 0--100 HZ &amp; 1--2/2 &amp; 1 or 2 convolutional layers &amp;  &amp; &amp;</span>
<span class="s2">Weights \cellbr</span>
<span class="s2">Correlation weights and interictal epileptic discharges (IED) \cellbr</span>
<span class="s2">Activations &amp;</span>
<span class="s2">Weights increasingly correlated with IED waveforms with increasing number of training iterations \cellbr</span>
<span class="s2">Second layer captured more complex and well-defined epileptic shapes than first layer \cellbr</span>
<span class="s2">IEDs led to highly synchronized activations for neighbouring electrodes</span>
<span class="se">\\</span><span class="s2"></span>
<span class="s2">\hdashline</span>
<span class="s2"> </span>
<span class="s2">Learning Robust Features using Deep Learning for Automatic Seizure Detection, \citet</span><span class="si">{thodoroff_learning_2016}</span><span class="s2"> &amp;</span>
<span class="s2">Start of epileptic seizure, within- and cross-subject &amp;</span>
<span class="s2">Frequency, mean amplitude for 0--7 Hz, 7--14 Hz, 14--49 Hz &amp; 3/1 (+ LSTM as postprocessor) &amp; &amp;</span>
<span class="s2"> &amp; Hand crafted features + SVM &amp; Input occlusion and effect on prediction accuracy &amp;</span>
<span class="s2">Allowed to locate areas critical for seizure </span>
<span class="se">\\</span><span class="s2"></span>
<span class="s2">\hdashline</span>

<span class="s2">Single-trial EEG RSVP classification using convolutional neural networks, \citet{george_single-trial_2016} &amp;</span>
<span class="s2">Oddball response (RSVP), groupwise (ConvNet trained on all subjects) &amp;</span>
<span class="s2">Time, 0.5--50 Hz &amp; 4/3 &amp; &amp;  &amp;  &amp;</span>
<span class="s2">Weights (spatial) &amp;</span>
<span class="s2">Some filter weights had expected topographic distributions for P300 \cellbr</span>
<span class="s2">Others filters had large weights on areas not traditionally associated with P300</span>
<span class="se">\\</span><span class="s2"></span>
<span class="s2">\hdashline</span>

<span class="s2">Wearable seizure detection using convolutional neural networks with transfer learning, \citet</span><span class="si">{page_wearable_2016}</span><span class="s2"> &amp;</span>
<span class="s2">Seizure detection, cross-subject, within-subject, groupwise &amp;</span>
<span class="s2">Time, \hspace</span><span class="si">{1cm}</span><span class="s2"> 0--128 Hz &amp; 1-3/1-3 &amp; &amp; Cross-subject supervised training, within-subject finetuning of fully connected layers &amp;</span>
<span class="s2">Multiple: spectral features, higher order statistics + linear-SVM, RBF-SVM, ...&amp; &amp; </span>
<span class="se">\\</span><span class="s2"></span>
<span class="s2">\hdashline</span>

<span class="s2">Learning Representations from EEG with Deep Recurrent-Convolutional Neural Networks, \citet</span><span class="si">{bashivan_learning_2016}</span><span class="s2">  &amp;</span>
<span class="s2">Cognitive load (number of characters to memorize), cross-subject &amp; </span>
<span class="s2">Frequency, mean power for 4--7 Hz, 8--13 Hz, 13--30 Hz &amp; 3--7/2 (+ LSTM or other temporal post-processing (see design choices)) &amp;</span>
<span class="s2">Number of convolutional layers \cellbr</span>
<span class="s2">Temporal processing of ConvNet output by max pooling, temporal convolution, LSTM or temporal convolution + LSTM &amp; &amp; &amp;</span>
<span class="s2">Inputs that maximally activate given filter \cellbr</span>
<span class="s2">Activations of these inputs \cellbr</span>
<span class="s2">&quot;Deconvolution&quot; for these inputs &amp;</span>
<span class="s2">Different filters were sensitive to different frequency bands \cellbr</span>
<span class="s2">Later layers had more spatially localized activations \cellbr</span>
<span class="s2">Learned features had noticeable links to well-known electrophysiological markers of cognitive load \cellbr</span>
<span class="se">\\</span><span class="s2"></span>
<span class="s2">\hdashline</span>

<span class="s2">Deep Feature Learning for EEG Recordings, \citet</span><span class="si">{stober_learning_2016}</span><span class="s2"> &amp;</span>
<span class="s2">Type of music rhythm, groupwise (ensembles of leave-one-subject-out trained models, evaluated on separate test set of same subjects) &amp;</span>
<span class="s2">Time, 0.5--30Hz &amp; 2/1 &amp; Kernel sizes &amp; </span>
<span class="s2">Pretraining first layer as convolutional autoencoder with different constraints &amp;  &amp; </span>
<span class="s2">Weights (spatial+3 timesteps, pretrained as autoencoder) &amp; </span>
<span class="s2">Different constraints led to different weights, one type of constraints could enforce weights that are similar across subjects; other type of constraints led to weights that have similar spatial topographies under different architectural configurations and preprocessings</span>
<span class="se">\\</span><span class="s2"></span>
<span class="s2">\hdashline</span>

<span class="s2">Convolutional Neural Network for Multi-Category Rapid Serial Visual Presentation BCI, \citet</span><span class="si">{manor_convolutional_2015}</span><span class="s2"> &amp;</span>
<span class="s2">Oddball response (RSVP), within-subject &amp;</span>
<span class="s2">Time, 0.1--50 Hz &amp; 3/3 (Spatio-temporal regularization) &amp;&amp; &amp;&amp;</span>
<span class="s2">Weights \cellbr Mean and single-trial activations &amp;</span>
<span class="s2">Spatiotemporal regularization led to softer peaks in weights \cellbr</span>
<span class="s2">Spatial weights showed typical P300 distribution \cellbr</span>
<span class="s2">Activations mostly had peaks at typical times (300-400ms)</span>
<span class="se">\\</span><span class="s2"></span>
<span class="s2">\hdashline</span>

<span class="s2">Parallel Convolutional-Linear Neural Network for Motor Imagery Classification, \citet</span><span class="si">{sakhavi_parallel_2015}</span><span class="s2">  &amp;</span>
<span class="s2">Imagined movement classes, within-subject &amp;</span>
<span class="s2">Frequency, 4--40 Hz, using FBCSP &amp; 2/2 (Final fully connected layer uses concatenated output by convolutional</span>
<span class="s2">and fully connected layers) &amp;</span>
<span class="s2">Combination ConvNet and MLP (trained on different features) vs. only ConvNet vs. only MLP &amp; </span>
<span class="s2">&amp; &amp; &amp; </span>
<span class="se">\\</span><span class="s2"></span>
<span class="s2">\hdashline</span>

<span class="s2">Using Convolutional Neural networks to Recognize Rhythm Stimuli form Electroencephalography Recordings, \citet</span><span class="si">{stober_using_2014}</span><span class="s2"> &amp;</span>
<span class="s2">Type of music rhythm, within-subject &amp; Time and frequency evaluated, 0-200 Hz &amp; 1-2/1 &amp;</span>
<span class="s2">Best values from automatic hyperparameter optimization: frequency cutoff, one vs two layers, kernel sizes, number of channels, pooling width &amp;</span>
<span class="s2">Best values from automatic hyperparameter optimization: learning rate, learning rate decay, momentum, final momentum &amp;</span>
<span class="s2">&amp;</span>
<span class="se">\\</span><span class="s2"></span>
<span class="s2">\hdashline</span>

<span class="s2">Convolutional deep belief networks for feature extraction of EEG signal, \citet</span><span class="si">{ren_convolutional_2014}</span><span class="s2">  &amp;</span>
<span class="s2">Imagined movement classes, within-subject &amp;</span>
<span class="s2">Frequency, 8--30 Hz &amp;</span>
<span class="s2">2/0 (Convolutional deep belief network, separately trained RBF-SVM classifier) &amp; &amp; </span>
<span class="se">\\</span><span class="s2"></span>
<span class="s2">\hdashline</span>

<span class="s2">Deep feature learning using target priors with applications in ECoG signal decoding for BCI, \citet</span><span class="si">{wang_deep_2013}</span><span class="s2">  &amp;</span>
<span class="s2">Finger flexion trajectory (regression), within-subject &amp;</span>
<span class="s2">Time, 0.15--200 Hz &amp; 3/1 (Convolutional layers trained as convolutional stacked autoencoder with target prior) &amp;</span>
<span class="s2">Partially supervised CSA &amp; </span>
<span class="se">\\</span><span class="s2"></span>
<span class="s2">\hdashline</span>

<span class="s2">Convolutional neural networks for P300 detection with application to brain-computer interfaces, \citet</span><span class="si">{cecotti_convolutional_2011}</span><span class="s2">  &amp;</span>
<span class="s2">Oddball / attention response using P300 speller, within-subject &amp; Time, 0.1-20 Hz &amp; 2/2 &amp;</span>
<span class="s2">Electrode subset (fixed or automatically determined) \cellbr</span>
<span class="s2">Using only one spatial filter \cellbr</span>
<span class="s2">Different ensembling strategies &amp; </span>
<span class="s2">&amp;</span>
<span class="s2">Multiple: Linear SVM, gradient boosting, E-SVM, S-SVM, mLVQ, LDA, ... &amp;</span>
<span class="s2">Weights &amp;</span>
<span class="s2">Spatial filters were similar for different architectures \cellbr</span>
<span class="s2">Spatial filters were different (more focal, more diffuse) for different subjects</span>
<span class="se">\\</span><span class="s2"></span>
<span class="s2">\hdashline</span>
<span class="s2"> &quot;&quot;&quot;</span>

<span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;&amp;&quot;</span><span class="p">,</span> <span class="s2">&quot;|&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;\cellbr&quot;</span><span class="p">,</span> <span class="s2">&quot;&lt;br&gt;&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;\hdashline&#39;</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">|&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\\</span><span class="s1">&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;ref&quot;</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;\ref&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="s1">&#39;–&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;cite&quot;</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;\cite&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;citet&quot;</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;cite&quot;</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;hspace{[^}]+}&#39;</span><span class="p">,</span> <span class="sa">r</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;</span><span class="se">\\</span><span class="s2">cite{([^}]+)}&quot;</span><span class="p">,</span> <span class="s2">&quot;</span><span class="si">{cite}</span><span class="s2">`\g&lt;1&gt;`&quot;</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="s2">&quot;| &quot;</span> <span class="o">+</span> <span class="n">b</span> <span class="o">+</span> <span class="s2">&quot; |&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>| EEGNet: A Compact Convolutional Network for EEG-based Brain-Computer Interfaces, {cite}`lawhern_eegnet:_2016` | Oddball response (RSVP), error response (ERN), movement classes (voluntarily started and imagined) |Time, 0.1–40 Hz | 3/1 |  Kernel sizes | |   | |
| Remembered or Forgotten? –- An EEG-Based Computational Prediction Approach, {cite}`sun_remembered_2016` | Memory performance, within-subject |Time, 0.05–15 Hz | 2/2 | | Different time windows |  |Weights (spatial) | Largest weights found over p\refrontal and temporal cortex
|Multimodal Neural Network for Rapid Serial Visual Presentation Brain Computer Interface, {cite}`manor_multimodal_2016`|Oddball response using RSVP and image (combined image-EEG decoding), within-subject|Time, 0.3–20 Hz | 3/2 | | |  | Weights &lt;br&gt; Activations &lt;br&gt; Saliency maps by gradient |Weights showed typical P300 distribution &lt;br&gt;Activations were high at plausible times (300-500ms) &lt;br&gt;Saliency maps showed plausible spatio-temporal plots
| A novel deep learning approach for classification of EEG motor imagery signals, {cite}`tabar_novel_2017` |Imagined and executed movement classes, within-subject |Frequency, 6–30 Hz | 1/1 | multicolumn{2}{p{0.285	extwidth}}{Addition of six-layer stacked autoencoder on ConvNet features &lt;br&gt; Kernel sizes} | FBCSP, Twin SVM, DDFBS, Bi-spectrum, RQNN  | Weights (spatial + frequential) |Some weights represented difference of values of two electrodes on different sides of head
| Predicting Seizures from Electroencephalography Recordings: A Knowledge Transfer Strategy, {cite}`liang_predicting_2016` |Seizure prediction, within-subject | Frequency, 0–200 Hz | 1/2 | | Different subdivisions of frequency range &lt;br&gt;Different lengths of time crops &lt;br&gt;Transfer learning with auxiliary non-epilepsy datasets || Weights &lt;br&gt; Clustering of weights |Clusters of weights showed typical frequency band subdivision (delta, theta, alpha, beta, gamma)
| EEG-based prediction of driver&#39;s cognitive performance by deep convolutional neural network, {cite}`hajinoroozi_eeg-based_2016` |Driver performance, within- and cross-subject |Time,  1–50 Hz | 1/3 |multicolumn{2}{p{0.285	extwidth}}{Replacement of convolutional layers by restricted Boltzmann machines with slightly varied network architecture}  | |
| Deep learning for epileptic intracranial EEG data, {cite}`antoniades_deep_2016` |Epileptic discharges, cross-subject | Time,  0–100 HZ | 1–2/2 | 1 or 2 convolutional layers |  | |Weights &lt;br&gt;Correlation weights and interictal epileptic discharges (IED) &lt;br&gt;Activations |Weights increasingly correlated with IED waveforms with increasing number of training iterations &lt;br&gt;Second layer captured more complex and well-defined epileptic shapes than first layer &lt;br&gt;IEDs led to highly synchronized activations for neighbouring electrodes
| Learning Robust Features using Deep Learning for Automatic Seizure Detection, {cite}`thodoroff_learning_2016` |Start of epileptic seizure, within- and cross-subject |Frequency, mean amplitude for 0–7 Hz, 7–14 Hz, 14–49 Hz | 3/1 (+ LSTM as postprocessor) | | | Hand crafted features + SVM | Input occlusion and effect on prediction accuracy |Allowed to locate areas critical for seizure 
|Single-trial EEG RSVP classification using convolutional neural networks, {cite}`george_single-trial_2016` |Oddball response (RSVP), groupwise (ConvNet trained on all subjects) |Time, 0.5–50 Hz | 4/3 | |  |  |Weights (spatial) |Some filter weights had expected topographic distributions for P300 &lt;br&gt;Others filters had large weights on areas not traditionally associated with P300
|Wearable seizure detection using convolutional neural networks with transfer learning, {cite}`page_wearable_2016` |Seizure detection, cross-subject, within-subject, groupwise |Time,  0–128 Hz | 1-3/1-3 | | Cross-subject supervised training, within-subject finetuning of fully connected layers |Multiple: spectral features, higher order statistics + linear-SVM, RBF-SVM, ...| | 
|Learning Representations from EEG with Deep Recurrent-Convolutional Neural Networks, {cite}`bashivan_learning_2016`  |Cognitive load (number of characters to memorize), cross-subject | Frequency, mean power for 4–7 Hz, 8–13 Hz, 13–30 Hz | 3–7/2 (+ LSTM or other temporal post-processing (see design choices)) |Number of convolutional layers &lt;br&gt;Temporal processing of ConvNet output by max pooling, temporal convolution, LSTM or temporal convolution + LSTM | | |Inputs that maximally activate given filter &lt;br&gt;Activations of these inputs &lt;br&gt;&quot;Deconvolution&quot; for these inputs |Different filters were sensitive to different frequency bands &lt;br&gt;Later layers had more spatially localized activations &lt;br&gt;Learned features had noticeable links to well-known electrophysiological markers of cognitive load &lt;br&gt;
|Deep Feature Learning for EEG Recordings, {cite}`stober_learning_2016` |Type of music rhythm, groupwise (ensembles of leave-one-subject-out trained models, evaluated on separate test set of same subjects) |Time, 0.5–30Hz | 2/1 | Kernel sizes | Pretraining first layer as convolutional autoencoder with different constraints |  | Weights (spatial+3 timesteps, pretrained as autoencoder) | Different constraints led to different weights, one type of constraints could enforce weights that are similar across subjects; other type of constraints led to weights that have similar spatial topographies under different architectural configurations and preprocessings
|Convolutional Neural Network for Multi-Category Rapid Serial Visual Presentation BCI, {cite}`manor_convolutional_2015` |Oddball response (RSVP), within-subject |Time, 0.1–50 Hz | 3/3 (Spatio-temporal regularization) || ||Weights &lt;br&gt; Mean and single-trial activations |Spatiotemporal regularization led to softer peaks in weights &lt;br&gt;Spatial weights showed typical P300 distribution &lt;br&gt;Activations mostly had peaks at typical times (300-400ms)
|Parallel Convolutional-Linear Neural Network for Motor Imagery Classification, {cite}`sakhavi_parallel_2015`  |Imagined movement classes, within-subject |Frequency, 4–40 Hz, using FBCSP | 2/2 (Final fully connected layer uses concatenated output by convolutionaland fully connected layers) |Combination ConvNet and MLP (trained on different features) vs. only ConvNet vs. only MLP | | | | 
|Using Convolutional Neural networks to Recognize Rhythm Stimuli form Electroencephalography Recordings, {cite}`stober_using_2014` |Type of music rhythm, within-subject | Time and frequency evaluated, 0-200 Hz | 1-2/1 |Best values from automatic hyperparameter optimization: frequency cutoff, one vs two layers, kernel sizes, number of channels, pooling width |Best values from automatic hyperparameter optimization: learning rate, learning rate decay, momentum, final momentum ||
|Convolutional deep belief networks for feature extraction of EEG signal, {cite}`ren_convolutional_2014`  |Imagined movement classes, within-subject |Frequency, 8–30 Hz |2/0 (Convolutional deep belief network, separately trained RBF-SVM classifier) | | 
|Deep feature learning using target priors with applications in ECoG signal decoding for BCI, {cite}`wang_deep_2013`  |Finger flexion trajectory (regression), within-subject |Time, 0.15–200 Hz | 3/1 (Convolutional layers trained as convolutional stacked autoencoder with target prior) |Partially supervised CSA | 
|Convolutional neural networks for P300 detection with application to brain-computer interfaces, {cite}`cecotti_convolutional_2011`  |Oddball / attention response using P300 speller, within-subject | Time, 0.1-20 Hz | 2/2 |Electrode subset (fixed or automatically determined) &lt;br&gt;Using only one spatial filter &lt;br&gt;Different ensembling strategies | |Multiple: Linear SVM, gradient boosting, E-SVM, S-SVM, mLVQ, LDA, ... |Weights |Spatial filters were similar for different architectures &lt;br&gt;Spatial filters were different (more focal, more diffuse) for different subjects
|  |
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span>  <span class="s2">&quot;network, \cite{hajinoroozi_eeg-based_2016} |D&quot;</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;</span><span class="se">\\</span><span class="s2">cite{([^}]+)}&quot;</span><span class="p">,</span> <span class="s2">&quot;</span><span class="si">{cite}</span><span class="s2">`\g&lt;1&gt;`&quot;</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;network, {cite}`hajinoroozi_eeg-based_2016` |D&#39;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">| This manuscript, Schirrmeister et. al (2017) |Imagined and executed movement classes, within subject |Time,  0–125 Hz | 5/1 |Different ConvNet architectures &lt;br&gt;Nonlinearities and pooling modes &lt;br&gt;Regularization and intermediate normalization layers &lt;br&gt;Factorized convolutions &lt;br&gt;Splitted vs one-step convolutions |Trial-wise vs. cropped training strategy |FBCSP + rLDA | Feature activation correlation &lt;br&gt;Feature-perturbation prediction correlation |See Section </span><span class="se">\r</span><span class="s2">ef{subsec:results-visualization}</span>
<span class="s2">| Single-trial EEG classification of motor imagery using deep convolutional neural networks, citet{tang_single-trial_2017} |Imagined movement classes, within-subject |Time,  8–30 Hz | 2/2 | | | FBCSP | | </span>
<span class="s2">| EEGNet: A Compact Convolutional Network for EEG-based Brain-Computer Interfaces, </span><span class="si">{cite}</span><span class="s2">`lawhern_eegnet:_2016` | Oddball response (RSVP), error response (ERN), movement classes (voluntarily started and imagined) |Time, 0.1–40 Hz | 3/1 |  Kernel sizes | |   | |</span>
<span class="s2">| Remembered or Forgotten? –- An EEG-Based Computational Prediction Approach, </span><span class="si">{cite}</span><span class="s2">`sun_remembered_2016` | Memory performance, within-subject |Time, 0.05–15 Hz | 2/2 | | Different time windows |  |Weights (spatial) | Largest weights found over p</span><span class="se">\r</span><span class="s2">efrontal and temporal cortex</span>
<span class="s2">|Multimodal Neural Network for Rapid Serial Visual Presentation Brain Computer Interface, </span><span class="si">{cite}</span><span class="s2">`manor_multimodal_2016`|Oddball response using RSVP and image (combined image-EEG decoding), within-subject|Time, 0.3–20 Hz | 3/2 | | |  | Weights &lt;br&gt; Activations &lt;br&gt; Saliency maps by gradient |Weights showed typical P300 distribution &lt;br&gt;Activations were high at plausible times (300-500ms) &lt;br&gt;Saliency maps showed plausible spatio-temporal plots</span>
<span class="s2">| A novel deep learning approach for classification of EEG motor imagery signals, </span><span class="si">{cite}</span><span class="s2">`tabar_novel_2017` |Imagined and executed movement classes, within-subject |Frequency, 6–30 Hz | 1/1 |Addition of six-layer stacked autoencoder on ConvNet features &lt;br&gt; Kernel sizes} |  | FBCSP, Twin SVM, DDFBS, Bi-spectrum, RQNN  | Weights (spatial + frequential) |Some weights represented difference of values of two electrodes on different sides of head</span>
<span class="s2">| Predicting Seizures from Electroencephalography Recordings: A Knowledge Transfer Strategy, </span><span class="si">{cite}</span><span class="s2">`liang_predicting_2016` |Seizure prediction, within-subject | Frequency, 0–200 Hz | 1/2 | | Different subdivisions of frequency range &lt;br&gt;Different lengths of time crops &lt;br&gt;Transfer learning with auxiliary non-epilepsy datasets || Weights &lt;br&gt; Clustering of weights |Clusters of weights showed typical frequency band subdivision (delta, theta, alpha, beta, gamma)</span>
<span class="s2">| EEG-based prediction of driver&#39;s cognitive performance by deep convolutional neural network, </span><span class="si">{cite}</span><span class="s2">`hajinoroozi_eeg-based_2016` |Driver performance, within- and cross-subject |Time,  1–50 Hz | 1/3 |Replacement of convolutional layers by restricted Boltzmann machines with slightly varied network architecture}  | | |</span>
<span class="s2">| Deep learning for epileptic intracranial EEG data, </span><span class="si">{cite}</span><span class="s2">`antoniades_deep_2016` |Epileptic discharges, cross-subject | Time,  0–100 HZ | 1–2/2 | 1 or 2 convolutional layers |  | |Weights &lt;br&gt;Correlation weights and interictal epileptic discharges (IED) &lt;br&gt;Activations |Weights increasingly correlated with IED waveforms with increasing number of training iterations &lt;br&gt;Second layer captured more complex and well-defined epileptic shapes than first layer &lt;br&gt;IEDs led to highly synchronized activations for neighbouring electrodes</span>
<span class="s2">| Learning Robust Features using Deep Learning for Automatic Seizure Detection, </span><span class="si">{cite}</span><span class="s2">`thodoroff_learning_2016` |Start of epileptic seizure, within- and cross-subject |Frequency, mean amplitude for 0–7 Hz, 7–14 Hz, 14–49 Hz | 3/1 (+ LSTM as postprocessor) | | | Hand crafted features + SVM | Input occlusion and effect on prediction accuracy |Allowed to locate areas critical for seizure </span>
<span class="s2">|Single-trial EEG RSVP classification using convolutional neural networks, </span><span class="si">{cite}</span><span class="s2">`george_single-trial_2016` |Oddball response (RSVP), groupwise (ConvNet trained on all subjects) |Time, 0.5–50 Hz | 4/3 | |  |  |Weights (spatial) |Some filter weights had expected topographic distributions for P300 &lt;br&gt;Others filters had large weights on areas not traditionally associated with P300</span>
<span class="s2">|Wearable seizure detection using convolutional neural networks with transfer learning, </span><span class="si">{cite}</span><span class="s2">`page_wearable_2016` |Seizure detection, cross-subject, within-subject, groupwise |Time,  0–128 Hz | 1-3/1-3 | | Cross-subject supervised training, within-subject finetuning of fully connected layers |Multiple: spectral features, higher order statistics + linear-SVM, RBF-SVM, ...| | </span>
<span class="s2">|Learning Representations from EEG with Deep Recurrent-Convolutional Neural Networks, </span><span class="si">{cite}</span><span class="s2">`bashivan_learning_2016`  |Cognitive load (number of characters to memorize), cross-subject | Frequency, mean power for 4–7 Hz, 8–13 Hz, 13–30 Hz | 3–7/2 (+ LSTM or other temporal post-processing (see design choices)) |Number of convolutional layers &lt;br&gt;Temporal processing of ConvNet output by max pooling, temporal convolution, LSTM or temporal convolution + LSTM | | |Inputs that maximally activate given filter &lt;br&gt;Activations of these inputs &lt;br&gt;&quot;Deconvolution&quot; for these inputs |Different filters were sensitive to different frequency bands &lt;br&gt;Later layers had more spatially localized activations &lt;br&gt;Learned features had noticeable links to well-known electrophysiological markers of cognitive load &lt;br&gt;</span>
<span class="s2">|Deep Feature Learning for EEG Recordings, </span><span class="si">{cite}</span><span class="s2">`stober_learning_2016` |Type of music rhythm, groupwise (ensembles of leave-one-subject-out trained models, evaluated on separate test set of same subjects) |Time, 0.5–30Hz | 2/1 | Kernel sizes | Pretraining first layer as convolutional autoencoder with different constraints |  | Weights (spatial+3 timesteps, pretrained as autoencoder) | Different constraints led to different weights, one type of constraints could enforce weights that are similar across subjects; other type of constraints led to weights that have similar spatial topographies under different architectural configurations and preprocessings</span>
<span class="s2">|Convolutional Neural Network for Multi-Category Rapid Serial Visual Presentation BCI, </span><span class="si">{cite}</span><span class="s2">`manor_convolutional_2015` |Oddball response (RSVP), within-subject |Time, 0.1–50 Hz | 3/3 (Spatio-temporal regularization) || ||Weights &lt;br&gt; Mean and single-trial activations |Spatiotemporal regularization led to softer peaks in weights &lt;br&gt;Spatial weights showed typical P300 distribution &lt;br&gt;Activations mostly had peaks at typical times (300-400ms)</span>
<span class="s2">|Parallel Convolutional-Linear Neural Network for Motor Imagery Classification, </span><span class="si">{cite}</span><span class="s2">`sakhavi_parallel_2015`  |Imagined movement classes, within-subject |Frequency, 4–40 Hz, using FBCSP | 2/2 (Final fully connected layer uses concatenated output by convolutionaland fully connected layers) |Combination ConvNet and MLP (trained on different features) vs. only ConvNet vs. only MLP | | | | </span>
<span class="s2">|Using Convolutional Neural networks to Recognize Rhythm Stimuli form Electroencephalography Recordings, </span><span class="si">{cite}</span><span class="s2">`stober_using_2014` |Type of music rhythm, within-subject | Time and frequency evaluated, 0-200 Hz | 1-2/1 |Best values from automatic hyperparameter optimization: frequency cutoff, one vs two layers, kernel sizes, number of channels, pooling width |Best values from automatic hyperparameter optimization: learning rate, learning rate decay, momentum, final momentum ||</span>
<span class="s2">|Convolutional deep belief networks for feature extraction of EEG signal, </span><span class="si">{cite}</span><span class="s2">`ren_convolutional_2014`  |Imagined movement classes, within-subject |Frequency, 8–30 Hz |2/0 (Convolutional deep belief network, separately trained RBF-SVM classifier) | | </span>
<span class="s2">|Deep feature learning using target priors with applications in ECoG signal decoding for BCI, </span><span class="si">{cite}</span><span class="s2">`wang_deep_2013`  |Finger flexion trajectory (regression), within-subject |Time, 0.15–200 Hz | 3/1 (Convolutional layers trained as convolutional stacked autoencoder with target prior) |Partially supervised CSA | </span>
<span class="s2">|Convolutional neural networks for P300 detection with application to brain-computer interfaces, </span><span class="si">{cite}</span><span class="s2">`cecotti_convolutional_2011`  |Oddball / attention response using P300 speller, within-subject | Time, 0.1-20 Hz | 2/2 |Electrode subset (fixed or automatically determined) &lt;br&gt;Using only one spatial filter &lt;br&gt;Different ensembling strategies | |Multiple: Linear SVM, gradient boosting, E-SVM, S-SVM, mLVQ, LDA, ... |Weights |Spatial filters were similar for different architectures &lt;br&gt;Spatial filters were different (more focal, more diffuse) for different subjects</span>
<span class="s2">&quot;&quot;&quot;</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">parts</span> <span class="o">=</span> <span class="p">[</span><span class="n">l</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;|&#39;</span><span class="p">)[</span><span class="mi">1</span><span class="p">:]</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">a</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)[</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parts</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[9, 9, 9, 9, 9, 9, 9, 8, 9, 9, 9, 9, 9, 9, 9, 9, 8, 6, 6, 9]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">headings</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Study&#39;</span><span class="p">,</span> <span class="s1">&#39;Decoding problem&#39;</span><span class="p">,</span> <span class="s1">&#39;Input domain&#39;</span><span class="p">,</span> <span class="s1">&#39;Conv/dense layers&#39;</span><span class="p">,</span> <span class="s1">&#39;Design choices&#39;</span><span class="p">,</span> 
<span class="s1">&#39;Training strategies&#39;</span><span class="p">,</span> <span class="s1">&#39;External baseline&#39;</span><span class="p">,</span> <span class="s1">&#39;Visualization type(s)&#39;</span><span class="p">,</span> <span class="s1">&#39;Visualization findings&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">parts</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span> <span class="o">+</span> <span class="p">[</span><span class="s1">&#39;&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="mi">9</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">p</span><span class="p">))</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parts</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">headings</span><span class="p">,</span> <span class="n">parts</span><span class="p">[</span><span class="mi">3</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(&#39;Study&#39;,
  &#39; Remembered or Forgotten? –- An EEG-Based Computational Prediction Approach, {cite}`sun_remembered_2016` &#39;),
 (&#39;Decoding problem&#39;, &#39; Memory performance, within-subject &#39;),
 (&#39;Input domain&#39;, &#39;Time, 0.05–15 Hz &#39;),
 (&#39;Conv/dense layers&#39;, &#39; 2/2 &#39;),
 (&#39;Design choices&#39;, &#39; &#39;),
 (&#39;Training strategies&#39;, &#39; Different time windows &#39;),
 (&#39;External baseline&#39;, &#39;  &#39;),
 (&#39;Visualization type(s)&#39;, &#39;Weights (spatial) &#39;),
 (&#39;Visualization findings&#39;,
  &#39; Largest weights found over p\refrontal and temporal cortex&#39;)]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parts</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">part_arr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">parts</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">re</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span>  <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">part_arr</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">headings</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="p">[</span><span class="s1">&#39;Study&#39;</span><span class="p">,</span> <span class="s1">&#39;Design choices&#39;</span><span class="p">,</span> <span class="s1">&#39;Training strategies&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">to_markdown</span><span class="p">(</span><span class="n">showindex</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>| Study                                                                                                                              | Design choices                                                                                                                                                                                  | Training strategies                                                                                                                      |
|:-----------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------|
| This manuscript, Schirrmeister et. al (2017)                                                                                       | Different ConvNet architectures &lt;br&gt;Nonlinearities and pooling modes &lt;br&gt;Regularization and intermediate normalization layers &lt;br&gt;Factorized convolutions &lt;br&gt;Splitted vs one-step convolutions | Trial-wise vs. cropped training strategy                                                                                                 |
| Single-trial EEG classification of motor imagery using deep convolutional neural networks, citet{tang_single-trial_2017}           |                                                                                                                                                                                                 |                                                                                                                                          |
| EEGNet: A Compact Convolutional Network for EEG-based Brain-Computer Interfaces, {cite}`lawhern_eegnet:_2016`                      | Kernel sizes                                                                                                                                                                                    |                                                                                                                                          |
| Remembered or Forgotten? –- An EEG-Based Computational Prediction Approach, {cite}`sun_remembered_2016`                            |                                                                                                                                                                                                 | Different time windows                                                                                                                   |
| Multimodal Neural Network for Rapid Serial Visual Presentation Brain Computer Interface, {cite}`manor_multimodal_2016`             |                                                                                                                                                                                                 |                                                                                                                                          |
| A novel deep learning approach for classification of EEG motor imagery signals, {cite}`tabar_novel_2017`                           | Addition of six-layer stacked autoencoder on ConvNet features &lt;br&gt; Kernel sizes}                                                                                                                |                                                                                                                                          |
| Predicting Seizures from Electroencephalography Recordings: A Knowledge Transfer Strategy, {cite}`liang_predicting_2016`           |                                                                                                                                                                                                 | Different subdivisions of frequency range &lt;br&gt;Different lengths of time crops &lt;br&gt;Transfer learning with auxiliary non-epilepsy datasets |
| EEG-based prediction of driver&#39;s cognitive performance by deep convolutional neural network, {cite}`hajinoroozi_eeg-based_2016`    | Replacement of convolutional layers by restricted Boltzmann machines with slightly varied network architecture}                                                                                 |                                                                                                                                          |
| Deep learning for epileptic intracranial EEG data, {cite}`antoniades_deep_2016`                                                    | 1 or 2 convolutional layers                                                                                                                                                                     |                                                                                                                                          |
| Learning Robust Features using Deep Learning for Automatic Seizure Detection, {cite}`thodoroff_learning_2016`                      |                                                                                                                                                                                                 |                                                                                                                                          |
| Single-trial EEG RSVP classification using convolutional neural networks, {cite}`george_single-trial_2016`                         |                                                                                                                                                                                                 |                                                                                                                                          |
| Wearable seizure detection using convolutional neural networks with transfer learning, {cite}`page_wearable_2016`                  |                                                                                                                                                                                                 | Cross-subject supervised training, within-subject finetuning of fully connected layers                                                   |
| Learning Representations from EEG with Deep Recurrent-Convolutional Neural Networks, {cite}`bashivan_learning_2016`                | Number of convolutional layers &lt;br&gt;Temporal processing of ConvNet output by max pooling, temporal convolution, LSTM or temporal convolution + LSTM                                              |                                                                                                                                          |
| Deep Feature Learning for EEG Recordings, {cite}`stober_learning_2016`                                                             | Kernel sizes                                                                                                                                                                                    | Pretraining first layer as convolutional autoencoder with different constraints                                                          |
| Convolutional Neural Network for Multi-Category Rapid Serial Visual Presentation BCI, {cite}`manor_convolutional_2015`             |                                                                                                                                                                                                 |                                                                                                                                          |
| Parallel Convolutional-Linear Neural Network for Motor Imagery Classification, {cite}`sakhavi_parallel_2015`                       | Combination ConvNet and MLP (trained on different features) vs. only ConvNet vs. only MLP                                                                                                       |                                                                                                                                          |
| Using Convolutional Neural networks to Recognize Rhythm Stimuli form Electroencephalography Recordings, {cite}`stober_using_2014`  | Best values from automatic hyperparameter optimization: frequency cutoff, one vs two layers, kernel sizes, number of channels, pooling width                                                    | Best values from automatic hyperparameter optimization: learning rate, learning rate decay, momentum, final momentum                     |
| Convolutional deep belief networks for feature extraction of EEG signal, {cite}`ren_convolutional_2014`                            |                                                                                                                                                                                                 |                                                                                                                                          |
| Deep feature learning using target priors with applications in ECoG signal decoding for BCI, {cite}`wang_deep_2013`                | Partially supervised CSA                                                                                                                                                                        |                                                                                                                                          |
| Convolutional neural networks for P300 detection with application to brain-computer interfaces, {cite}`cecotti_convolutional_2011` | Electrode subset (fixed or automatically determined) &lt;br&gt;Using only one spatial filter &lt;br&gt;Different ensembling strategies                                                                      |                                                                                                                                          |
</pre></div>
</div>
</div>
</div>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:left head"><p>Study</p></th>
<th class="text-align:left head"><p>Design choices</p></th>
<th class="text-align:left head"><p>Training strategies</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:left"><p>EEGNet: A Compact Convolutional Network for EEG-based Brain-Computer Interfaces, <span id="id37">[<a class="reference internal" href="References.html#id150" title="Vernon J. Lawhern, Amelia J. Solon, Nicholas R. Waytowich, Stephen M. Gordon, Chou P. Hung, and Brent J. Lance. EEGNet: A Compact Convolutional Network for EEG-based Brain-Computer Interfaces. arXiv:1611.08024 [cs, q-bio, stat], November 2016. arXiv: 1611.08024. URL: http://arxiv.org/abs/1611.08024 (visited on 2016-12-20).">Lawhern <em>et al.</em>, 2016</a>]</span></p></td>
<td class="text-align:left"><p>Kernel sizes</p></td>
<td class="text-align:left"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Remembered or Forgotten? –- An EEG-Based Computational Prediction Approach, <span id="id38">[<a class="reference internal" href="References.html#id222" title="Xuyun Sun, Cunle Qian, Zhongqin Chen, Zhaohui Wu, Benyan Luo, and Gang Pan. Remembered or Forgotten?—An EEG-Based Computational Prediction Approach. PLOS ONE, 11(12):e0167497, December 2016. URL: http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0167497 (visited on 2017-02-14), doi:10.1371/journal.pone.0167497.">Sun <em>et al.</em>, 2016</a>]</span></p></td>
<td class="text-align:left"><p></p></td>
<td class="text-align:left"><p>Different time windows</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>A novel deep learning approach for classification of EEG motor imagery signals, <span id="id39">[<a class="reference internal" href="References.html#id215" title="Yousef Rezaei Tabar and Ugur Halici. A novel deep learning approach for classification of EEG motor imagery signals. Journal of Neural Engineering, 14(1):016003, 2017. URL: http://stacks.iop.org/1741-2552/14/i=1/a=016003 (visited on 2017-02-14), doi:10.1088/1741-2560/14/1/016003.">Tabar and Halici, 2017</a>]</span></p></td>
<td class="text-align:left"><p>Addition of six-layer stacked autoencoder on ConvNet features <br> Kernel sizes}</p></td>
<td class="text-align:left"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Predicting Seizures from Electroencephalography Recordings: A Knowledge Transfer Strategy, <span id="id40">[<a class="reference internal" href="References.html#id172" title="J. Liang, R. Lu, C. Zhang, and F. Wang. Predicting Seizures from Electroencephalography Recordings: A Knowledge Transfer Strategy. In 2016 IEEE International Conference on Healthcare Informatics (ICHI), 184–191. October 2016. doi:10.1109/ICHI.2016.27.">Liang <em>et al.</em>, 2016</a>]</span></p></td>
<td class="text-align:left"><p></p></td>
<td class="text-align:left"><p>Different subdivisions of frequency range <br>Different lengths of time crops <br>Transfer learning with auxiliary non-epilepsy datasets</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>EEG-based prediction of driver’s cognitive performance by deep convolutional neural network, <span id="id41">[<a class="reference internal" href="References.html#id180" title="Mehdi Hajinoroozi, Zijing Mao, Tzyy-Ping Jung, Chin-Teng Lin, and Yufei Huang. EEG-based prediction of driver's cognitive performance by deep convolutional neural network. Signal Processing: Image Communication, 47:549–555, September 2016. URL: http://www.sciencedirect.com/science/article/pii/S0923596516300832 (visited on 2016-12-20), doi:10.1016/j.image.2016.05.018.">Hajinoroozi <em>et al.</em>, 2016</a>]</span></p></td>
<td class="text-align:left"><p>Replacement of convolutional layers by restricted Boltzmann machines with slightly varied network architecture}</p></td>
<td class="text-align:left"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Deep learning for epileptic intracranial EEG data, <span id="id42">[<a class="reference internal" href="References.html#id154" title="A. Antoniades, L. Spyrou, C. C. Took, and S. Sanei. Deep learning for epileptic intracranial EEG data. In 2016 IEEE 26th International Workshop on Machine Learning for Signal Processing (MLSP), 1–6. September 2016. doi:10.1109/MLSP.2016.7738824.">Antoniades <em>et al.</em>, 2016</a>]</span></p></td>
<td class="text-align:left"><p>1 or 2 convolutional layers</p></td>
<td class="text-align:left"><p></p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Wearable seizure detection using convolutional neural networks with transfer learning, <span id="id43">[<a class="reference internal" href="References.html#id204" title="A. Page, C. Shea, and T. Mohsenin. Wearable seizure detection using convolutional neural networks with transfer learning. In 2016 IEEE International Symposium on Circuits and Systems (ISCAS), 1086–1089. May 2016. doi:10.1109/ISCAS.2016.7527433.">Page <em>et al.</em>, 2016</a>]</span></p></td>
<td class="text-align:left"><p></p></td>
<td class="text-align:left"><p>Cross-subject supervised training, within-subject finetuning of fully connected layers</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Learning Representations from EEG with Deep Recurrent-Convolutional Neural Networks, <span id="id44">[<a class="reference internal" href="References.html#id197" title="Pouya Bashivan, Irina Rish, Mohammed Yeasin, and Noel Codella. Learning Representations from EEG with Deep Recurrent-Convolutional Neural Networks. In arXiv:1511.06448 [cs]. 2016. arXiv: 1511.06448. URL: http://arxiv.org/abs/1511.06448 (visited on 2016-12-20).">Bashivan <em>et al.</em>, 2016</a>]</span></p></td>
<td class="text-align:left"><p>Number of convolutional layers <br>Temporal processing of ConvNet output by max pooling, temporal convolution, LSTM or temporal convolution + LSTM</p></td>
<td class="text-align:left"><p></p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Deep Feature Learning for EEG Recordings, <span id="id45">[<a class="reference internal" href="References.html#id226" title="Sebastian Stober. Learning Discriminative Features from Electroencephalography Recordings by Encoding Similarity Constraints. In Bernstein Conference 2016. 2016. doi:10.12751/nncn.bc2016.0223.">Stober, 2016</a>]</span></p></td>
<td class="text-align:left"><p>Kernel sizes</p></td>
<td class="text-align:left"><p>Pretraining first layer as convolutional autoencoder with different constraints</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Parallel Convolutional-Linear Neural Network for Motor Imagery Classification, <span id="id46">[<a class="reference internal" href="References.html#id117" title="S. Sakhavi, C. Guan, and S. Yan. Parallel convolutional-linear neural network for motor imagery classification. In Signal Processing Conference (EUSIPCO), 2015 23rd European, 2736–2740. August 2015. doi:10.1109/EUSIPCO.2015.7362882.">Sakhavi <em>et al.</em>, 2015</a>]</span></p></td>
<td class="text-align:left"><p>Combination ConvNet and MLP (trained on different features) vs. only ConvNet vs. only MLP</p></td>
<td class="text-align:left"><p></p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Using Convolutional Neural networks to Recognize Rhythm Stimuli form Electroencephalography Recordings, <span id="id47">[<a class="reference internal" href="References.html#id184" title="Sebastian Stober, Daniel J. Cameron, and Jessica A. Grahn. Using Convolutional Neural Networks to Recognize Rhythm Stimuli from Electroencephalography Recordings. In Proceedings of the 27th International Conference on Neural Information Processing Systems, NIPS'14, 1449–1457. Cambridge, MA, USA, 2014. MIT Press. URL: http://dl.acm.org/citation.cfm?id=2968826.2968988 (visited on 2016-12-20).">Stober <em>et al.</em>, 2014</a>]</span></p></td>
<td class="text-align:left"><p>Best values from automatic hyperparameter optimization: frequency cutoff, one vs two layers, kernel sizes, number of channels, pooling width</p></td>
<td class="text-align:left"><p>Best values from automatic hyperparameter optimization: learning rate, learning rate decay, momentum, final momentum</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Deep feature learning using target priors with applications in ECoG signal decoding for BCI, <span id="id48">[<a class="reference internal" href="References.html#id209" title="Zuoguan Wang, Siwei Lyu, Gerwin Schalk, and Qiang Ji. Deep Feature Learning Using Target Priors with Applications in ECoG Signal Decoding for BCI. In Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence, IJCAI '13, 1785–1791. Beijing, China, 2013. AAAI Press. URL: http://dl.acm.org/citation.cfm?id=2540128.2540384 (visited on 2017-01-16).">Wang <em>et al.</em>, 2013</a>]</span></p></td>
<td class="text-align:left"><p>Partially supervised CSA</p></td>
<td class="text-align:left"><p></p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Convolutional neural networks for P300 detection with application to brain-computer interfaces, <span id="id49">[<a class="reference internal" href="References.html#id212" title="Hubert Cecotti and Axel Graser. Convolutional Neural Networks for P300 Detection with Application to Brain-Computer Interfaces. IEEE Transactions on Pattern Analysis and Machine Intelligence, 33(3):433–445, March 2011. URL: http://dx.doi.org/10.1109/TPAMI.2010.125 (visited on 2016-12-20), doi:10.1109/TPAMI.2010.125.">Cecotti and Graser, 2011</a>]</span></p></td>
<td class="text-align:left"><p>Electrode subset (fixed or automatically determined) <br>Using only one spatial filter <br>Different ensembling strategies</p></td>
<td class="text-align:left"><p></p></td>
</tr>
</tbody>
</table>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Study</th>
      <th>Decoding problem</th>
      <th>Input domain</th>
      <th>Conv/dense layers</th>
      <th>Design choices</th>
      <th>Training strategies</th>
      <th>External baseline</th>
      <th>Visualization type(s)</th>
      <th>Visualization findings</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>This manuscript, Schirrmeister et. al (2017)</td>
      <td>Imagined and executed movement classes, within...</td>
      <td>Time,  0–125 Hz</td>
      <td>5/1</td>
      <td>Different ConvNet architectures &lt;br&gt;Nonlineari...</td>
      <td>Trial-wise vs. cropped training strategy</td>
      <td>FBCSP + rLDA</td>
      <td>Feature activation correlation &lt;br&gt;Feature-pe...</td>
      <td></td>
    </tr>
    <tr>
      <th>1</th>
      <td>Single-trial EEG classification of motor imag...</td>
      <td>Imagined movement classes, within-subject</td>
      <td>Time,  8–30 Hz</td>
      <td>2/2</td>
      <td></td>
      <td></td>
      <td>FBCSP</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <th>2</th>
      <td>EEGNet: A Compact Convolutional Network for E...</td>
      <td>Oddball response (RSVP), error response (ERN)...</td>
      <td>Time, 0.1–40 Hz</td>
      <td>3/1</td>
      <td>Kernel sizes</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <th>3</th>
      <td>Remembered or Forgotten? –- An EEG-Based Comp...</td>
      <td>Memory performance, within-subject</td>
      <td>Time, 0.05–15 Hz</td>
      <td>2/2</td>
      <td></td>
      <td>Different time windows</td>
      <td></td>
      <td>Weights (spatial)</td>
      <td></td>
    </tr>
    <tr>
      <th>4</th>
      <td>Multimodal Neural Network for Rapid Serial Vis...</td>
      <td>Oddball response using RSVP and image (combine...</td>
      <td>Time, 0.3–20 Hz</td>
      <td>3/2</td>
      <td></td>
      <td></td>
      <td></td>
      <td>Weights &lt;br&gt; Activations &lt;br&gt; Saliency maps b...</td>
      <td></td>
    </tr>
    <tr>
      <th>5</th>
      <td>A novel deep learning approach for classifica...</td>
      <td>Imagined and executed movement classes, within...</td>
      <td>Frequency, 6–30 Hz</td>
      <td>1/1</td>
      <td>Addition of six-layer stacked autoencoder on C...</td>
      <td></td>
      <td>FBCSP, Twin SVM, DDFBS, Bi-spectrum, RQNN</td>
      <td>Weights (spatial + frequential)</td>
      <td></td>
    </tr>
    <tr>
      <th>6</th>
      <td>Predicting Seizures from Electroencephalograp...</td>
      <td>Seizure prediction, within-subject</td>
      <td>Frequency, 0–200 Hz</td>
      <td>1/2</td>
      <td></td>
      <td>Different subdivisions of frequency range &lt;br...</td>
      <td></td>
      <td>Weights &lt;br&gt; Clustering of weights</td>
      <td></td>
    </tr>
    <tr>
      <th>7</th>
      <td>EEG-based prediction of driver's cognitive pe...</td>
      <td>Driver performance, within- and cross-subject</td>
      <td>Time,  1–50 Hz</td>
      <td>1/3</td>
      <td>Replacement of convolutional layers by restric...</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <th>8</th>
      <td>Deep learning for epileptic intracranial EEG ...</td>
      <td>Epileptic discharges, cross-subject</td>
      <td>Time,  0–100 HZ</td>
      <td>1–2/2</td>
      <td>1 or 2 convolutional layers</td>
      <td></td>
      <td></td>
      <td>Weights &lt;br&gt;Correlation weights and interictal...</td>
      <td></td>
    </tr>
    <tr>
      <th>9</th>
      <td>Learning Robust Features using Deep Learning ...</td>
      <td>Start of epileptic seizure, within- and cross-...</td>
      <td>Frequency, mean amplitude for 0–7 Hz, 7–14 Hz,...</td>
      <td>3/1 (+ LSTM as postprocessor)</td>
      <td></td>
      <td></td>
      <td>Hand crafted features + SVM</td>
      <td>Input occlusion and effect on prediction accu...</td>
      <td></td>
    </tr>
    <tr>
      <th>10</th>
      <td>Single-trial EEG RSVP classification using con...</td>
      <td>Oddball response (RSVP), groupwise (ConvNet tr...</td>
      <td>Time, 0.5–50 Hz</td>
      <td>4/3</td>
      <td></td>
      <td></td>
      <td></td>
      <td>Weights (spatial)</td>
      <td></td>
    </tr>
    <tr>
      <th>11</th>
      <td>Wearable seizure detection using convolutional...</td>
      <td>Seizure detection, cross-subject, within-subje...</td>
      <td>Time,  0–128 Hz</td>
      <td>1-3/1-3</td>
      <td></td>
      <td>Cross-subject supervised training, within-sub...</td>
      <td>Multiple: spectral features, higher order stat...</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <th>12</th>
      <td>Learning Representations from EEG with Deep Re...</td>
      <td>Cognitive load (number of characters to memori...</td>
      <td>Frequency, mean power for 4–7 Hz, 8–13 Hz, 13...</td>
      <td>3–7/2 (+ LSTM or other temporal post-processi...</td>
      <td>Number of convolutional layers &lt;br&gt;Temporal pr...</td>
      <td></td>
      <td></td>
      <td>Inputs that maximally activate given filter &lt;b...</td>
      <td></td>
    </tr>
    <tr>
      <th>13</th>
      <td>Deep Feature Learning for EEG Recordings, {cit...</td>
      <td>Type of music rhythm, groupwise (ensembles of ...</td>
      <td>Time, 0.5–30Hz</td>
      <td>2/1</td>
      <td>Kernel sizes</td>
      <td>Pretraining first layer as convolutional auto...</td>
      <td></td>
      <td>Weights (spatial+3 timesteps, pretrained as a...</td>
      <td></td>
    </tr>
    <tr>
      <th>14</th>
      <td>Convolutional Neural Network for Multi-Categor...</td>
      <td>Oddball response (RSVP), within-subject</td>
      <td>Time, 0.1–50 Hz</td>
      <td>3/3 (Spatio-temporal regularization)</td>
      <td></td>
      <td></td>
      <td></td>
      <td>Weights &lt;br&gt; Mean and single-trial activations</td>
      <td></td>
    </tr>
    <tr>
      <th>15</th>
      <td>Parallel Convolutional-Linear Neural Network f...</td>
      <td>Imagined movement classes, within-subject</td>
      <td>Frequency, 4–40 Hz, using FBCSP</td>
      <td>2/2 (Final fully connected layer uses concate...</td>
      <td>Combination ConvNet and MLP (trained on differ...</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <th>16</th>
      <td>Using Convolutional Neural networks to Recogni...</td>
      <td>Type of music rhythm, within-subject</td>
      <td>Time and frequency evaluated, 0-200 Hz</td>
      <td>1-2/1</td>
      <td>Best values from automatic hyperparameter opti...</td>
      <td>Best values from automatic hyperparameter opti...</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <th>17</th>
      <td>Convolutional deep belief networks for feature...</td>
      <td>Imagined movement classes, within-subject</td>
      <td>Frequency, 8–30 Hz</td>
      <td>2/0 (Convolutional deep belief network, separa...</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <th>18</th>
      <td>Deep feature learning using target priors with...</td>
      <td>Finger flexion trajectory (regression), within...</td>
      <td>Time, 0.15–200 Hz</td>
      <td>3/1 (Convolutional layers trained as convolut...</td>
      <td>Partially supervised CSA</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <th>19</th>
      <td>Convolutional neural networks for P300 detecti...</td>
      <td>Oddball / attention response using P300 spelle...</td>
      <td>Time, 0.1-20 Hz</td>
      <td>2/2</td>
      <td>Electrode subset (fixed or automatically deter...</td>
      <td></td>
      <td>Multiple: Linear SVM, gradient boosting, E-SVM...</td>
      <td>Weights</td>
      <td></td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="mi">1</span><span class="p">:,</span> <span class="p">[</span><span class="s1">&#39;Study&#39;</span><span class="p">,</span> <span class="s1">&#39;Visualization type(s)&#39;</span><span class="p">,</span> <span class="s1">&#39;Visualization findings&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">to_markdown</span><span class="p">(</span><span class="n">showindex</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>| Study                                                                                                                              | Visualization type(s)                                                                                            | Visualization findings                                                                                                                                                                                                                                                          |
|:-----------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Single-trial EEG classification of motor imagery using deep convolutional neural networks, citet{tang_single-trial_2017}           |                                                                                                                  |                                                                                                                                                                                                                                                                                 |
| EEGNet: A Compact Convolutional Network for EEG-based Brain-Computer Interfaces, {cite}`lawhern_eegnet:_2016`                      |                                                                                                                  |                                                                                                                                                                                                                                                                                 |
| Remembered or Forgotten? –- An EEG-Based Computational Prediction Approach, {cite}`sun_remembered_2016`                            | Weights (spatial)                                                                                                | Largest weights found over p                                                                                                                                                                                                                                                    |
|                                                                                                                                    |                                                                                                                  | efrontal and temporal cortex                                                                                                                                                                                                                                                    |
| Multimodal Neural Network for Rapid Serial Visual Presentation Brain Computer Interface, {cite}`manor_multimodal_2016`             | Weights &lt;br&gt; Activations &lt;br&gt; Saliency maps by gradient                                                          | Weights showed typical P300 distribution &lt;br&gt;Activations were high at plausible times (300-500ms) &lt;br&gt;Saliency maps showed plausible spatio-temporal plots                                                                                                                      |
| A novel deep learning approach for classification of EEG motor imagery signals, {cite}`tabar_novel_2017`                           | Weights (spatial + frequential)                                                                                  | Some weights represented difference of values of two electrodes on different sides of head                                                                                                                                                                                      |
| Predicting Seizures from Electroencephalography Recordings: A Knowledge Transfer Strategy, {cite}`liang_predicting_2016`           | Weights &lt;br&gt; Clustering of weights                                                                               | Clusters of weights showed typical frequency band subdivision (delta, theta, alpha, beta, gamma)                                                                                                                                                                                |
| EEG-based prediction of driver&#39;s cognitive performance by deep convolutional neural network, {cite}`hajinoroozi_eeg-based_2016`    |                                                                                                                  |                                                                                                                                                                                                                                                                                 |
| Deep learning for epileptic intracranial EEG data, {cite}`antoniades_deep_2016`                                                    | Weights &lt;br&gt;Correlation weights and interictal epileptic discharges (IED) &lt;br&gt;Activations                        | Weights increasingly correlated with IED waveforms with increasing number of training iterations &lt;br&gt;Second layer captured more complex and well-defined epileptic shapes than first layer &lt;br&gt;IEDs led to highly synchronized activations for neighbouring electrodes          |
| Learning Robust Features using Deep Learning for Automatic Seizure Detection, {cite}`thodoroff_learning_2016`                      | Input occlusion and effect on prediction accuracy                                                                | Allowed to locate areas critical for seizure                                                                                                                                                                                                                                    |
| Single-trial EEG RSVP classification using convolutional neural networks, {cite}`george_single-trial_2016`                         | Weights (spatial)                                                                                                | Some filter weights had expected topographic distributions for P300 &lt;br&gt;Others filters had large weights on areas not traditionally associated with P300                                                                                                                        |
| Wearable seizure detection using convolutional neural networks with transfer learning, {cite}`page_wearable_2016`                  |                                                                                                                  |                                                                                                                                                                                                                                                                                 |
| Learning Representations from EEG with Deep Recurrent-Convolutional Neural Networks, {cite}`bashivan_learning_2016`                | Inputs that maximally activate given filter &lt;br&gt;Activations of these inputs &lt;br&gt;&quot;Deconvolution&quot; for these inputs | Different filters were sensitive to different frequency bands &lt;br&gt;Later layers had more spatially localized activations &lt;br&gt;Learned features had noticeable links to well-known electrophysiological markers of cognitive load &lt;br&gt;                                             |
| Deep Feature Learning for EEG Recordings, {cite}`stober_learning_2016`                                                             | Weights (spatial+3 timesteps, pretrained as autoencoder)                                                         | Different constraints led to different weights, one type of constraints could enforce weights that are similar across subjects; other type of constraints led to weights that have similar spatial topographies under different architectural configurations and preprocessings |
| Convolutional Neural Network for Multi-Category Rapid Serial Visual Presentation BCI, {cite}`manor_convolutional_2015`             | Weights &lt;br&gt; Mean and single-trial activations                                                                   | Spatiotemporal regularization led to softer peaks in weights &lt;br&gt;Spatial weights showed typical P300 distribution &lt;br&gt;Activations mostly had peaks at typical times (300-400ms)                                                                                                 |
| Parallel Convolutional-Linear Neural Network for Motor Imagery Classification, {cite}`sakhavi_parallel_2015`                       |                                                                                                                  |                                                                                                                                                                                                                                                                                 |
| Using Convolutional Neural networks to Recognize Rhythm Stimuli form Electroencephalography Recordings, {cite}`stober_using_2014`  |                                                                                                                  |                                                                                                                                                                                                                                                                                 |
| Convolutional deep belief networks for feature extraction of EEG signal, {cite}`ren_convolutional_2014`                            |                                                                                                                  |                                                                                                                                                                                                                                                                                 |
| Deep feature learning using target priors with applications in ECoG signal decoding for BCI, {cite}`wang_deep_2013`                |                                                                                                                  |                                                                                                                                                                                                                                                                                 |
| Convolutional neural networks for P300 detection with application to brain-computer interfaces, {cite}`cecotti_convolutional_2011` | Weights                                                                                                          | Spatial filters were similar for different architectures &lt;br&gt;Spatial filters were different (more focal, more diffuse) for different subjects                                                                                                                                   |
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">b</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;| Study                                                                                                                              | Visualization type(s)                                                                                            | Visualization findings                                                                                                                                                                                                                                                          |</span>
<span class="s2">|:-----------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|</span>
<span class="s2">| Single-trial EEG classification of motor imagery using deep convolutional neural networks, citet{tang_single-trial_2017}           |                                                                                                                  |                                                                                                                                                                                                                                                                                 |</span>
<span class="s2">| EEGNet: A Compact Convolutional Network for EEG-based Brain-Computer Interfaces, </span><span class="si">{cite}</span><span class="s2">`lawhern_eegnet:_2016`                      |                                                                                                                  |                                                                                                                                                                                                                                                                                 |</span>
<span class="s2">| Remembered or Forgotten? –- An EEG-Based Computational Prediction Approach, </span><span class="si">{cite}</span><span class="s2">`sun_remembered_2016`                            | Weights (spatial)                                                                                                | Largest weights found over prefrontal and temporal cortex                                                                                                                                                                                                                                                                    |</span>
<span class="s2">| Multimodal Neural Network for Rapid Serial Visual Presentation Brain Computer Interface, </span><span class="si">{cite}</span><span class="s2">`manor_multimodal_2016`             | Weights &lt;br&gt; Activations &lt;br&gt; Saliency maps by gradient                                                          | Weights showed typical P300 distribution &lt;br&gt;Activations were high at plausible times (300-500ms) &lt;br&gt;Saliency maps showed plausible spatio-temporal plots                                                                                                                      |</span>
<span class="s2">| A novel deep learning approach for classification of EEG motor imagery signals, </span><span class="si">{cite}</span><span class="s2">`tabar_novel_2017`                           | Weights (spatial + frequential)                                                                                  | Some weights represented difference of values of two electrodes on different sides of head                                                                                                                                                                                      |</span>
<span class="s2">| Predicting Seizures from Electroencephalography Recordings: A Knowledge Transfer Strategy, </span><span class="si">{cite}</span><span class="s2">`liang_predicting_2016`           | Weights &lt;br&gt; Clustering of weights                                                                               | Clusters of weights showed typical frequency band subdivision (delta, theta, alpha, beta, gamma)                                                                                                                                                                                |</span>
<span class="s2">| EEG-based prediction of driver&#39;s cognitive performance by deep convolutional neural network, </span><span class="si">{cite}</span><span class="s2">`hajinoroozi_eeg-based_2016`    |                                                                                                                  |                                                                                                                                                                                                                                                                                 |</span>
<span class="s2">| Deep learning for epileptic intracranial EEG data, </span><span class="si">{cite}</span><span class="s2">`antoniades_deep_2016`                                                    | Weights &lt;br&gt;Correlation weights and interictal epileptic discharges (IED) &lt;br&gt;Activations                        | Weights increasingly correlated with IED waveforms with increasing number of training iterations &lt;br&gt;Second layer captured more complex and well-defined epileptic shapes than first layer &lt;br&gt;IEDs led to highly synchronized activations for neighbouring electrodes          |</span>
<span class="s2">| Learning Robust Features using Deep Learning for Automatic Seizure Detection, </span><span class="si">{cite}</span><span class="s2">`thodoroff_learning_2016`                      | Input occlusion and effect on prediction accuracy                                                                | Allowed to locate areas critical for seizure                                                                                                                                                                                                                                    |</span>
<span class="s2">| Single-trial EEG RSVP classification using convolutional neural networks, </span><span class="si">{cite}</span><span class="s2">`george_single-trial_2016`                         | Weights (spatial)                                                                                                | Some filter weights had expected topographic distributions for P300 &lt;br&gt;Others filters had large weights on areas not traditionally associated with P300                                                                                                                        |</span>
<span class="s2">| Wearable seizure detection using convolutional neural networks with transfer learning, </span><span class="si">{cite}</span><span class="s2">`page_wearable_2016`                  |                                                                                                                  |                                                                                                                                                                                                                                                                                 |</span>
<span class="s2">| Learning Representations from EEG with Deep Recurrent-Convolutional Neural Networks, </span><span class="si">{cite}</span><span class="s2">`bashivan_learning_2016`                | Inputs that maximally activate given filter &lt;br&gt;Activations of these inputs &lt;br&gt;&quot;Deconvolution&quot; for these inputs | Different filters were sensitive to different frequency bands &lt;br&gt;Later layers had more spatially localized activations &lt;br&gt;Learned features had noticeable links to well-known electrophysiological markers of cognitive load &lt;br&gt;                                             |</span>
<span class="s2">| Deep Feature Learning for EEG Recordings, </span><span class="si">{cite}</span><span class="s2">`stober_learning_2016`                                                             | Weights (spatial+3 timesteps, pretrained as autoencoder)                                                         | Different constraints led to different weights, one type of constraints could enforce weights that are similar across subjects; other type of constraints led to weights that have similar spatial topographies under different architectural configurations and preprocessings |</span>
<span class="s2">| Convolutional Neural Network for Multi-Category Rapid Serial Visual Presentation BCI, </span><span class="si">{cite}</span><span class="s2">`manor_convolutional_2015`             | Weights &lt;br&gt; Mean and single-trial activations                                                                   | Spatiotemporal regularization led to softer peaks in weights &lt;br&gt;Spatial weights showed typical P300 distribution &lt;br&gt;Activations mostly had peaks at typical times (300-400ms)                                                                                                 |</span>
<span class="s2">| Parallel Convolutional-Linear Neural Network for Motor Imagery Classification, </span><span class="si">{cite}</span><span class="s2">`sakhavi_parallel_2015`                       |                                                                                                                  |                                                                                                                                                                                                                                                                                 |</span>
<span class="s2">| Using Convolutional Neural networks to Recognize Rhythm Stimuli form Electroencephalography Recordings, </span><span class="si">{cite}</span><span class="s2">`stober_using_2014`  |                                                                                                                  |                                                                                                                                                                                                                                                                                 |</span>
<span class="s2">| Convolutional deep belief networks for feature extraction of EEG signal, </span><span class="si">{cite}</span><span class="s2">`ren_convolutional_2014`                            |                                                                                                                  |                                                                                                                                                                                                                                                                                 |</span>
<span class="s2">| Deep feature learning using target priors with applications in ECoG signal decoding for BCI, </span><span class="si">{cite}</span><span class="s2">`wang_deep_2013`                |                                                                                                                  |                                                                                                                                                                                                                                                                                 |</span>
<span class="s2">| Convolutional neural networks for P300 detection with application to brain-computer interfaces, </span><span class="si">{cite}</span><span class="s2">`cecotti_convolutional_2011` | Weights                                                                                                          | Spatial filters were similar for different architectures &lt;br&gt;Spatial filters were different (more focal, more diffuse) for different subjects                                                                                                                                   |&quot;&quot;&quot;</span>


<span class="n">c</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)[</span><span class="mi">2</span><span class="p">:]</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="s1">&#39;|&#39;</span> <span class="o">+</span> <span class="n">l</span><span class="p">[</span><span class="n">l</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="s1">&#39;cite&#39;</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:]</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">c</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>| citet{tang_single-trial_2017}           |                                                                                                                  |                                                                                                                                                                                                                                                                                 |
|{cite}`lawhern_eegnet:_2016`                      |                                                                                                                  |                                                                                                                                                                                                                                                                                 |
|{cite}`sun_remembered_2016`                            | Weights (spatial)                                                                                                | Largest weights found over prefrontal and temporal cortex                                                                                                                                                                                                                                                                    |
|{cite}`manor_multimodal_2016`             | Weights &lt;br&gt; Activations &lt;br&gt; Saliency maps by gradient                                                          | Weights showed typical P300 distribution &lt;br&gt;Activations were high at plausible times (300-500ms) &lt;br&gt;Saliency maps showed plausible spatio-temporal plots                                                                                                                      |
|{cite}`tabar_novel_2017`                           | Weights (spatial + frequential)                                                                                  | Some weights represented difference of values of two electrodes on different sides of head                                                                                                                                                                                      |
|{cite}`liang_predicting_2016`           | Weights &lt;br&gt; Clustering of weights                                                                               | Clusters of weights showed typical frequency band subdivision (delta, theta, alpha, beta, gamma)                                                                                                                                                                                |
|{cite}`hajinoroozi_eeg-based_2016`    |                                                                                                                  |                                                                                                                                                                                                                                                                                 |
|{cite}`antoniades_deep_2016`                                                    | Weights &lt;br&gt;Correlation weights and interictal epileptic discharges (IED) &lt;br&gt;Activations                        | Weights increasingly correlated with IED waveforms with increasing number of training iterations &lt;br&gt;Second layer captured more complex and well-defined epileptic shapes than first layer &lt;br&gt;IEDs led to highly synchronized activations for neighbouring electrodes          |
|{cite}`thodoroff_learning_2016`                      | Input occlusion and effect on prediction accuracy                                                                | Allowed to locate areas critical for seizure                                                                                                                                                                                                                                    |
|{cite}`george_single-trial_2016`                         | Weights (spatial)                                                                                                | Some filter weights had expected topographic distributions for P300 &lt;br&gt;Others filters had large weights on areas not traditionally associated with P300                                                                                                                        |
|{cite}`page_wearable_2016`                  |                                                                                                                  |                                                                                                                                                                                                                                                                                 |
|{cite}`bashivan_learning_2016`                | Inputs that maximally activate given filter &lt;br&gt;Activations of these inputs &lt;br&gt;&quot;Deconvolution&quot; for these inputs | Different filters were sensitive to different frequency bands &lt;br&gt;Later layers had more spatially localized activations &lt;br&gt;Learned features had noticeable links to well-known electrophysiological markers of cognitive load &lt;br&gt;                                             |
|{cite}`stober_learning_2016`                                                             | Weights (spatial+3 timesteps, pretrained as autoencoder)                                                         | Different constraints led to different weights, one type of constraints could enforce weights that are similar across subjects; other type of constraints led to weights that have similar spatial topographies under different architectural configurations and preprocessings |
|{cite}`manor_convolutional_2015`             | Weights &lt;br&gt; Mean and single-trial activations                                                                   | Spatiotemporal regularization led to softer peaks in weights &lt;br&gt;Spatial weights showed typical P300 distribution &lt;br&gt;Activations mostly had peaks at typical times (300-400ms)                                                                                                 |
|{cite}`sakhavi_parallel_2015`                       |                                                                                                                  |                                                                                                                                                                                                                                                                                 |
|{cite}`stober_using_2014`  |                                                                                                                  |                                                                                                                                                                                                                                                                                 |
|{cite}`ren_convolutional_2014`                            |                                                                                                                  |                                                                                                                                                                                                                                                                                 |
|{cite}`wang_deep_2013`                |                                                                                                                  |                                                                                                                                                                                                                                                                                 |
|{cite}`cecotti_convolutional_2011` | Weights                                                                                                          | Spatial filters were similar for different architectures &lt;br&gt;Spatial filters were different (more focal, more diffuse) for different subjects                                                                                                                                   |
</pre></div>
</div>
</div>
</div>
<div class="section" id="visulization-table-with-empty-papers">
<h2>visulization table with empty papers<a class="headerlink" href="#visulization-table-with-empty-papers" title="Permalink to this headline">#</a></h2>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:left head"><p>Study</p></th>
<th class="text-align:left head"><p>Visualization type(s)</p></th>
<th class="text-align:left head"><p>Visualization findings</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:left"><p><span id="id50">[<a class="reference internal" href="References.html#id213" title="Zhichuan Tang, Chao Li, and Shouqian Sun. Single-trial EEG classification of motor imagery using deep convolutional neural networks. Optik - International Journal for Light and Electron Optics, 130:11–18, February 2017. URL: http://www.sciencedirect.com/science/article/pii/S0030402616312980 (visited on 2017-02-14), doi:10.1016/j.ijleo.2016.10.117.">Tang <em>et al.</em>, 2017</a>]</span></p></td>
<td class="text-align:left"><p></p></td>
<td class="text-align:left"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p><span id="id51">[<a class="reference internal" href="References.html#id150" title="Vernon J. Lawhern, Amelia J. Solon, Nicholas R. Waytowich, Stephen M. Gordon, Chou P. Hung, and Brent J. Lance. EEGNet: A Compact Convolutional Network for EEG-based Brain-Computer Interfaces. arXiv:1611.08024 [cs, q-bio, stat], November 2016. arXiv: 1611.08024. URL: http://arxiv.org/abs/1611.08024 (visited on 2016-12-20).">Lawhern <em>et al.</em>, 2016</a>]</span></p></td>
<td class="text-align:left"><p></p></td>
<td class="text-align:left"><p></p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p><span id="id52">[<a class="reference internal" href="References.html#id222" title="Xuyun Sun, Cunle Qian, Zhongqin Chen, Zhaohui Wu, Benyan Luo, and Gang Pan. Remembered or Forgotten?—An EEG-Based Computational Prediction Approach. PLOS ONE, 11(12):e0167497, December 2016. URL: http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0167497 (visited on 2017-02-14), doi:10.1371/journal.pone.0167497.">Sun <em>et al.</em>, 2016</a>]</span></p></td>
<td class="text-align:left"><p>Weights (spatial)</p></td>
<td class="text-align:left"><p>Largest weights found over prefrontal and temporal cortex</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p><span id="id53">[<a class="reference internal" href="References.html#id196" title="Ran Manor, Liran Mishali, and Amir B. Geva. Multimodal Neural Network for Rapid Serial Visual Presentation Brain Computer Interface. Frontiers in Computational Neuroscience, December 2016. URL: http://www.ncbi.nlm.nih.gov/pmc/articles/PMC5168930/ (visited on 2017-02-03), doi:10.3389/fncom.2016.00130.">Manor <em>et al.</em>, 2016</a>]</span></p></td>
<td class="text-align:left"><p>Weights <br> Activations <br> Saliency maps by gradient</p></td>
<td class="text-align:left"><p>Weights showed typical P300 distribution <br>Activations were high at plausible times (300-500ms) <br>Saliency maps showed plausible spatio-temporal plots</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p><span id="id54">[<a class="reference internal" href="References.html#id215" title="Yousef Rezaei Tabar and Ugur Halici. A novel deep learning approach for classification of EEG motor imagery signals. Journal of Neural Engineering, 14(1):016003, 2017. URL: http://stacks.iop.org/1741-2552/14/i=1/a=016003 (visited on 2017-02-14), doi:10.1088/1741-2560/14/1/016003.">Tabar and Halici, 2017</a>]</span></p></td>
<td class="text-align:left"><p>Weights (spatial + frequential)</p></td>
<td class="text-align:left"><p>Some weights represented difference of values of two electrodes on different sides of head</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p><span id="id55">[<a class="reference internal" href="References.html#id172" title="J. Liang, R. Lu, C. Zhang, and F. Wang. Predicting Seizures from Electroencephalography Recordings: A Knowledge Transfer Strategy. In 2016 IEEE International Conference on Healthcare Informatics (ICHI), 184–191. October 2016. doi:10.1109/ICHI.2016.27.">Liang <em>et al.</em>, 2016</a>]</span></p></td>
<td class="text-align:left"><p>Weights <br> Clustering of weights</p></td>
<td class="text-align:left"><p>Clusters of weights showed typical frequency band subdivision (delta, theta, alpha, beta, gamma)</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p><span id="id56">[<a class="reference internal" href="References.html#id180" title="Mehdi Hajinoroozi, Zijing Mao, Tzyy-Ping Jung, Chin-Teng Lin, and Yufei Huang. EEG-based prediction of driver's cognitive performance by deep convolutional neural network. Signal Processing: Image Communication, 47:549–555, September 2016. URL: http://www.sciencedirect.com/science/article/pii/S0923596516300832 (visited on 2016-12-20), doi:10.1016/j.image.2016.05.018.">Hajinoroozi <em>et al.</em>, 2016</a>]</span></p></td>
<td class="text-align:left"><p></p></td>
<td class="text-align:left"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p><span id="id57">[<a class="reference internal" href="References.html#id154" title="A. Antoniades, L. Spyrou, C. C. Took, and S. Sanei. Deep learning for epileptic intracranial EEG data. In 2016 IEEE 26th International Workshop on Machine Learning for Signal Processing (MLSP), 1–6. September 2016. doi:10.1109/MLSP.2016.7738824.">Antoniades <em>et al.</em>, 2016</a>]</span></p></td>
<td class="text-align:left"><p>Weights <br>Correlation weights and interictal epileptic discharges (IED) <br>Activations</p></td>
<td class="text-align:left"><p>Weights increasingly correlated with IED waveforms with increasing number of training iterations <br>Second layer captured more complex and well-defined epileptic shapes than first layer <br>IEDs led to highly synchronized activations for neighbouring electrodes</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p><span id="id58">[<a class="reference internal" href="References.html#id182" title="Pierre Thodoroff, Joelle Pineau, and Andrew Lim. Learning Robust Features using Deep Learning for Automatic Seizure Detection. In JMLR Workshop and Conference Proceedings, volume 56. 2016. URL: http://www.jmlr.org/proceedings/papers/v56/Thodoroff16.pdf (visited on 2017-02-14).">Thodoroff <em>et al.</em>, 2016</a>]</span></p></td>
<td class="text-align:left"><p>Input occlusion and effect on prediction accuracy</p></td>
<td class="text-align:left"><p>Allowed to locate areas critical for seizure</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p><span id="id59">[<a class="reference internal" href="References.html#id200" title="Jared Shamwell, Hyungtae Lee, Heesung Kwon, Amar R. Marathe, Vernon Lawhern, and William Nothwang. Single-trial EEG RSVP classification using convolutional neural networks. In Thomas George, Achyut K. Dutta, and M. Saif Islam, editors, SPIE Defense+ Security, volume 9836. International Society for Optics and Photonics, May 2016. URL: http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.2224172 (visited on 2017-02-14), doi:10.1117/12.2224172.">Shamwell <em>et al.</em>, 2016</a>]</span></p></td>
<td class="text-align:left"><p>Weights (spatial)</p></td>
<td class="text-align:left"><p>Some filter weights had expected topographic distributions for P300 <br>Others filters had large weights on areas not traditionally associated with P300</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p><span id="id60">[<a class="reference internal" href="References.html#id204" title="A. Page, C. Shea, and T. Mohsenin. Wearable seizure detection using convolutional neural networks with transfer learning. In 2016 IEEE International Symposium on Circuits and Systems (ISCAS), 1086–1089. May 2016. doi:10.1109/ISCAS.2016.7527433.">Page <em>et al.</em>, 2016</a>]</span></p></td>
<td class="text-align:left"><p></p></td>
<td class="text-align:left"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p><span id="id61">[<a class="reference internal" href="References.html#id197" title="Pouya Bashivan, Irina Rish, Mohammed Yeasin, and Noel Codella. Learning Representations from EEG with Deep Recurrent-Convolutional Neural Networks. In arXiv:1511.06448 [cs]. 2016. arXiv: 1511.06448. URL: http://arxiv.org/abs/1511.06448 (visited on 2016-12-20).">Bashivan <em>et al.</em>, 2016</a>]</span></p></td>
<td class="text-align:left"><p>Inputs that maximally activate given filter <br>Activations of these inputs <br>”Deconvolution” for these inputs</p></td>
<td class="text-align:left"><p>Different filters were sensitive to different frequency bands <br>Later layers had more spatially localized activations <br>Learned features had noticeable links to well-known electrophysiological markers of cognitive load <br></p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p><span id="id62">[<a class="reference internal" href="References.html#id226" title="Sebastian Stober. Learning Discriminative Features from Electroencephalography Recordings by Encoding Similarity Constraints. In Bernstein Conference 2016. 2016. doi:10.12751/nncn.bc2016.0223.">Stober, 2016</a>]</span></p></td>
<td class="text-align:left"><p>Weights (spatial+3 timesteps, pretrained as autoencoder)</p></td>
<td class="text-align:left"><p>Different constraints led to different weights, one type of constraints could enforce weights that are similar across subjects; other type of constraints led to weights that have similar spatial topographies under different architectural configurations and preprocessings</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p><span id="id63">[<a class="reference internal" href="References.html#id147" title="Ran Manor and Amir B. Geva. Convolutional Neural Network for Multi-Category Rapid Serial Visual Presentation BCI. Frontiers in Computational Neuroscience, 9:146, 2015. doi:10.3389/fncom.2015.00146.">Manor and Geva, 2015</a>]</span></p></td>
<td class="text-align:left"><p>Weights <br> Mean and single-trial activations</p></td>
<td class="text-align:left"><p>Spatiotemporal regularization led to softer peaks in weights <br>Spatial weights showed typical P300 distribution <br>Activations mostly had peaks at typical times (300-400ms)</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p><span id="id64">[<a class="reference internal" href="References.html#id117" title="S. Sakhavi, C. Guan, and S. Yan. Parallel convolutional-linear neural network for motor imagery classification. In Signal Processing Conference (EUSIPCO), 2015 23rd European, 2736–2740. August 2015. doi:10.1109/EUSIPCO.2015.7362882.">Sakhavi <em>et al.</em>, 2015</a>]</span></p></td>
<td class="text-align:left"><p></p></td>
<td class="text-align:left"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p><span id="id65">[<a class="reference internal" href="References.html#id184" title="Sebastian Stober, Daniel J. Cameron, and Jessica A. Grahn. Using Convolutional Neural Networks to Recognize Rhythm Stimuli from Electroencephalography Recordings. In Proceedings of the 27th International Conference on Neural Information Processing Systems, NIPS'14, 1449–1457. Cambridge, MA, USA, 2014. MIT Press. URL: http://dl.acm.org/citation.cfm?id=2968826.2968988 (visited on 2016-12-20).">Stober <em>et al.</em>, 2014</a>]</span></p></td>
<td class="text-align:left"><p></p></td>
<td class="text-align:left"><p></p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p><span id="id66">[<a class="reference internal" href="References.html#id137" title="Y. Ren and Y. Wu. Convolutional deep belief networks for feature extraction of EEG signal. In 2014 International Joint Conference on Neural Networks (IJCNN), 2850–2853. July 2014. doi:10.1109/IJCNN.2014.6889383.">Ren and Wu, 2014</a>]</span></p></td>
<td class="text-align:left"><p></p></td>
<td class="text-align:left"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p><span id="id67">[<a class="reference internal" href="References.html#id209" title="Zuoguan Wang, Siwei Lyu, Gerwin Schalk, and Qiang Ji. Deep Feature Learning Using Target Priors with Applications in ECoG Signal Decoding for BCI. In Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence, IJCAI '13, 1785–1791. Beijing, China, 2013. AAAI Press. URL: http://dl.acm.org/citation.cfm?id=2540128.2540384 (visited on 2017-01-16).">Wang <em>et al.</em>, 2013</a>]</span></p></td>
<td class="text-align:left"><p></p></td>
<td class="text-align:left"><p></p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p><span id="id68">[<a class="reference internal" href="References.html#id212" title="Hubert Cecotti and Axel Graser. Convolutional Neural Networks for P300 Detection with Application to Brain-Computer Interfaces. IEEE Transactions on Pattern Analysis and Machine Intelligence, 33(3):433–445, March 2011. URL: http://dx.doi.org/10.1109/TPAMI.2010.125 (visited on 2016-12-20), doi:10.1109/TPAMI.2010.125.">Cecotti and Graser, 2011</a>]</span></p></td>
<td class="text-align:left"><p>Weights</p></td>
<td class="text-align:left"><p>Spatial filters were similar for different architectures <br>Spatial filters were different (more focal, more diffuse) for different subjects</p></td>
</tr>
</tbody>
</table>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span><span class="s1">&#39;Input domain&#39;</span><span class="p">]</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">a</span><span class="p">)[</span><span class="mi">1</span><span class="p">:]</span> <span class="c1"># exclude our own study</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="s1">&#39;Time,  8–30 Hz &#39;</span><span class="p">,</span> <span class="s1">&#39;Time, 0.1–40 Hz &#39;</span><span class="p">,</span> <span class="s1">&#39;Time, 0.05–15 Hz &#39;</span><span class="p">,</span>
       <span class="s1">&#39;Time, 0.3–20 Hz &#39;</span><span class="p">,</span> <span class="s1">&#39;Frequency, 6–30 Hz &#39;</span><span class="p">,</span> <span class="s1">&#39; Frequency, 0–200 Hz &#39;</span><span class="p">,</span>
       <span class="s1">&#39;Time,  1–50 Hz &#39;</span><span class="p">,</span> <span class="s1">&#39; Time,  0–100 HZ &#39;</span><span class="p">,</span>
       <span class="s1">&#39;Frequency, mean amplitude for 0–7 Hz, 7–14 Hz, 14–49 Hz &#39;</span><span class="p">,</span>
       <span class="s1">&#39;Time, 0.5–50 Hz &#39;</span><span class="p">,</span> <span class="s1">&#39;Time,  0–128 Hz &#39;</span><span class="p">,</span>
       <span class="s1">&#39; Frequency, mean power for 4–7 Hz, 8–13 Hz, 13–30 Hz &#39;</span><span class="p">,</span>
       <span class="s1">&#39;Time, 0.5–30Hz &#39;</span><span class="p">,</span> <span class="s1">&#39;Time, 0.1–50 Hz &#39;</span><span class="p">,</span>
       <span class="s1">&#39;Frequency, 4–40 Hz, using FBCSP &#39;</span><span class="p">,</span>
       <span class="s1">&#39; Time and frequency evaluated, 0-200 Hz &#39;</span><span class="p">,</span> <span class="s1">&#39;Frequency, 8–30 Hz &#39;</span><span class="p">,</span>
       <span class="s1">&#39;Time, 0.15–200 Hz &#39;</span><span class="p">,</span> <span class="s1">&#39; Time, 0.1-20 Hz &#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">re</span>
<span class="n">domain_strings</span> <span class="o">=</span> <span class="p">[</span><span class="n">s</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;,&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">a</span><span class="p">]</span>
<span class="n">start_fs</span> <span class="o">=</span> <span class="p">[</span><span class="nb">float</span><span class="p">(</span><span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;[a-z ]+&#39;</span><span class="p">,</span><span class="sa">r</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">re</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;[–-–-]&#39;</span><span class="p">,</span><span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;,&#39;</span><span class="p">)[</span><span class="mi">1</span><span class="p">:]))[</span><span class="mi">0</span><span class="p">]))</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">a</span><span class="p">]</span>
<span class="n">end_fs</span> <span class="o">=</span> <span class="p">[</span><span class="nb">float</span><span class="p">(</span><span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;[a-z HZFBCSP]+&#39;</span><span class="p">,</span><span class="sa">r</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">re</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;[–-–-]&#39;</span><span class="p">,</span><span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;,&#39;</span><span class="p">)[</span><span class="mi">1</span><span class="p">:]))[</span><span class="mi">1</span><span class="p">]))</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">a</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">cm</span>
<span class="kn">import</span> <span class="nn">seaborn</span>
<span class="n">seaborn</span><span class="o">.</span><span class="n">set_palette</span><span class="p">(</span><span class="s1">&#39;colorblind&#39;</span><span class="p">)</span>
<span class="n">seaborn</span><span class="o">.</span><span class="n">set_style</span><span class="p">(</span><span class="s1">&#39;darkgrid&#39;</span><span class="p">)</span>

<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="o">%</span><span class="k">config</span> InlineBackend.figure_format = &#39;png&#39;
<span class="c1">#matplotlib.rcParams[&#39;figure.figsize&#39;] = (12.0, 1.0)</span>
<span class="n">matplotlib</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;font.size&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">14</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">freq_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="s1">&#39;freq&#39;</span> <span class="ow">in</span> <span class="n">s</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">domain_strings</span><span class="p">])</span>
<span class="n">time_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="s1">&#39;time&#39;</span> <span class="ow">in</span> <span class="n">s</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">domain_strings</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">98349384</span><span class="p">)</span>
<span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;grey&#39;</span>
<span class="n">domain_strings</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">domain_strings</span><span class="p">)</span>
<span class="n">start_fs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">start_fs</span><span class="p">)</span>
<span class="n">end_fs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">end_fs</span><span class="p">)</span>
<span class="n">i_sort</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">flatnonzero</span><span class="p">(</span><span class="n">time_mask</span><span class="p">)[</span><span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">end_fs</span><span class="p">[</span><span class="n">time_mask</span><span class="p">])]</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">d</span><span class="p">,</span><span class="n">s</span><span class="p">,</span><span class="n">e</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span>
        <span class="n">domain_strings</span><span class="p">[</span><span class="n">i_sort</span><span class="p">],</span> <span class="n">start_fs</span><span class="p">[</span><span class="n">i_sort</span><span class="p">],</span> <span class="n">end_fs</span><span class="p">[</span><span class="n">i_sort</span><span class="p">])):</span>
    <span class="n">offset</span> <span class="o">=</span> <span class="mf">0.6</span><span class="o">*</span><span class="n">i</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">i_sort</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.3</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">offset</span><span class="p">,</span><span class="n">offset</span><span class="p">]</span> <span class="p">,</span> <span class="p">[</span><span class="n">s</span><span class="p">,</span> <span class="n">e</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">)</span>
<span class="n">i_sort</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">flatnonzero</span><span class="p">(</span><span class="n">freq_mask</span><span class="p">)[</span><span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">end_fs</span><span class="p">[</span><span class="n">freq_mask</span><span class="p">])]</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">d</span><span class="p">,</span><span class="n">s</span><span class="p">,</span><span class="n">e</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span>
        <span class="n">domain_strings</span><span class="p">[</span><span class="n">i_sort</span><span class="p">],</span> <span class="n">start_fs</span><span class="p">[</span><span class="n">i_sort</span><span class="p">],</span> <span class="n">end_fs</span><span class="p">[</span><span class="n">i_sort</span><span class="p">])):</span>
    <span class="n">offset</span> <span class="o">=</span> <span class="mf">0.6</span><span class="o">*</span><span class="n">i</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">i_sort</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.7</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">offset</span><span class="p">,</span><span class="n">offset</span><span class="p">]</span> <span class="p">,</span> <span class="p">[</span><span class="n">s</span><span class="p">,</span> <span class="n">e</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">1.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Input domain&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Frequency [Hz]&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;Time&quot;</span><span class="p">,</span> <span class="s2">&quot;Frequency&quot;</span><span class="p">],</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">45</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Input domains and frequency ranges in prior work&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.05</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0.5, 1.05, &#39;Input domains and frequency ranges in prior work&#39;)
</pre></div>
</div>
<img alt="_images/ConvertLatex_31_1.png" src="_images/ConvertLatex_31_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="mi">1</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">98349384</span><span class="p">)</span>
<span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;grey&#39;</span>
<span class="k">for</span> <span class="n">low_c</span><span class="p">,</span> <span class="n">high_c</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">low_conv_ls</span><span class="p">,</span> <span class="n">high_conv_ls</span><span class="p">):</span>
    <span class="n">offset</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.1</span>
    <span class="n">tried_cs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">low_c</span><span class="p">,</span> <span class="n">high_c</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">offset</span><span class="p">,]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">tried_cs</span><span class="p">),</span> <span class="n">tried_cs</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">)</span>
    
<span class="k">for</span> <span class="n">i_c</span><span class="p">,</span> <span class="n">n_c</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">bincount_conv</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">i_c</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="n">n_c</span><span class="o">*</span><span class="mi">40</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.535</span><span class="p">,</span> <span class="n">i_c</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">n_c</span><span class="p">)</span><span class="o">+</span> <span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;left&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">low_c</span><span class="p">,</span> <span class="n">high_c</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">low_dense_ls</span><span class="p">,</span> <span class="n">high_dense_ls</span><span class="p">):</span>
    <span class="n">offset</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">rng</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.1</span>
    <span class="n">tried_cs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">low_c</span><span class="p">,</span> <span class="n">high_c</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">offset</span><span class="p">,]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">tried_cs</span><span class="p">),</span> <span class="n">tried_cs</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">)</span>
    
<span class="k">for</span> <span class="n">i_c</span><span class="p">,</span> <span class="n">n_c</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">bincount_dense</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="mf">1.4</span><span class="p">,</span> <span class="n">i_c</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="n">n_c</span><span class="o">*</span><span class="mi">40</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">1.535</span><span class="p">,</span> <span class="n">i_c</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">n_c</span><span class="p">)</span><span class="o">+</span> <span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;left&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Type of layer&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Number of layers&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;Convolutional&quot;</span><span class="p">,</span> <span class="s2">&quot;Dense&quot;</span><span class="p">],</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">45</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">7</span><span class="p">]);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Number of layers in prior works&#39; architectures&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.05</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="layer-stuff">
<h1>layer stuff<a class="headerlink" href="#layer-stuff" title="Permalink to this headline">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ls</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s1">&#39;Conv/dense layers&#39;</span><span class="p">])[</span><span class="mi">1</span><span class="p">:]</span> <span class="c1"># exclude ourstudy</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ls</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="s1">&#39; 2/2 &#39;</span><span class="p">,</span> <span class="s1">&#39; 3/1 &#39;</span><span class="p">,</span> <span class="s1">&#39; 2/2 &#39;</span><span class="p">,</span> <span class="s1">&#39; 3/2 &#39;</span><span class="p">,</span> <span class="s1">&#39; 1/1 &#39;</span><span class="p">,</span> <span class="s1">&#39; 1/2 &#39;</span><span class="p">,</span> <span class="s1">&#39; 1/3 &#39;</span><span class="p">,</span>
       <span class="s1">&#39; 1–2/2 &#39;</span><span class="p">,</span> <span class="s1">&#39; 3/1 (+ LSTM as postprocessor) &#39;</span><span class="p">,</span> <span class="s1">&#39; 4/3 &#39;</span><span class="p">,</span> <span class="s1">&#39; 1-3/1-3 &#39;</span><span class="p">,</span>
       <span class="s1">&#39; 3–7/2 (+ LSTM or other temporal post-processing (see design choices)) &#39;</span><span class="p">,</span>
       <span class="s1">&#39; 2/1 &#39;</span><span class="p">,</span> <span class="s1">&#39; 3/3 (Spatio-temporal regularization) &#39;</span><span class="p">,</span>
       <span class="s1">&#39; 2/2 (Final fully connected layer uses concatenated output by convolutionaland fully connected layers) &#39;</span><span class="p">,</span>
       <span class="s1">&#39; 1-2/1 &#39;</span><span class="p">,</span>
       <span class="s1">&#39;2/0 (Convolutional deep belief network, separately trained RBF-SVM classifier) &#39;</span><span class="p">,</span>
       <span class="s1">&#39; 3/1 (Convolutional layers trained as convolutional stacked autoencoder with target prior) &#39;</span><span class="p">,</span>
       <span class="s1">&#39; 2/2 &#39;</span><span class="p">])</span>

<span class="n">conv_ls</span> <span class="o">=</span> <span class="p">[</span><span class="n">l</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;/&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">ls</span><span class="p">]</span>
<span class="n">low_conv_ls</span> <span class="o">=</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">re</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;[–-]&#39;</span><span class="p">,</span> <span class="n">c</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span><span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">conv_ls</span><span class="p">]</span>
<span class="n">high_conv_ls</span> <span class="o">=</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">re</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;[–-]&#39;</span><span class="p">,</span> <span class="n">c</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span><span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">conv_ls</span><span class="p">]</span>
<span class="n">dense_ls</span> <span class="o">=</span> <span class="p">[</span><span class="n">l</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;/&#39;</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">ls</span><span class="p">]</span>
<span class="n">low_dense_ls</span> <span class="o">=</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">re</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;[–-]&#39;</span><span class="p">,</span> <span class="n">c</span><span class="p">[:</span><span class="mi">8</span><span class="p">])[</span><span class="mi">0</span><span class="p">][:</span><span class="mi">2</span><span class="p">])</span><span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">dense_ls</span><span class="p">]</span>
<span class="n">high_dense_ls</span> <span class="o">=</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">re</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;[–-]&#39;</span><span class="p">,</span> <span class="n">c</span><span class="p">[:</span><span class="mi">8</span><span class="p">])[</span><span class="o">-</span><span class="mi">1</span><span class="p">][:</span><span class="mi">2</span><span class="p">])</span><span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">dense_ls</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">cm</span>
<span class="kn">import</span> <span class="nn">seaborn</span>
<span class="n">seaborn</span><span class="o">.</span><span class="n">set_palette</span><span class="p">(</span><span class="s1">&#39;colorblind&#39;</span><span class="p">)</span>
<span class="n">seaborn</span><span class="o">.</span><span class="n">set_style</span><span class="p">(</span><span class="s1">&#39;darkgrid&#39;</span><span class="p">)</span>

<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="o">%</span><span class="k">config</span> InlineBackend.figure_format = &#39;png&#39;
<span class="c1">#matplotlib.rcParams[&#39;figure.figsize&#39;] = (12.0, 1.0)</span>
<span class="n">matplotlib</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;font.size&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">14</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">all_conv_ls</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">low_c</span><span class="p">,</span> <span class="n">high_c</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">low_c</span><span class="p">,</span> <span class="n">high_c</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">low_conv_ls</span><span class="p">,</span> <span class="n">high_conv_ls</span><span class="p">)])</span>
<span class="n">all_dense_ls</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">low_c</span><span class="p">,</span> <span class="n">high_c</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">low_c</span><span class="p">,</span> <span class="n">high_c</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">low_dense_ls</span><span class="p">,</span> <span class="n">high_dense_ls</span><span class="p">)])</span>
<span class="n">bincount_conv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">all_conv_ls</span><span class="p">)</span>
<span class="n">bincount_dense</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">all_dense_ls</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">98349384</span><span class="p">)</span>
<span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;grey&#39;</span>
<span class="k">for</span> <span class="n">low_c</span><span class="p">,</span> <span class="n">high_c</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">low_conv_ls</span><span class="p">,</span> <span class="n">high_conv_ls</span><span class="p">):</span>
    <span class="n">offset</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.1</span>
    <span class="n">tried_cs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">low_c</span><span class="p">,</span> <span class="n">high_c</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">offset</span><span class="p">,]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">tried_cs</span><span class="p">),</span> <span class="n">tried_cs</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">)</span>
    
<span class="k">for</span> <span class="n">i_c</span><span class="p">,</span> <span class="n">n_c</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">bincount_conv</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">i_c</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="n">n_c</span><span class="o">*</span><span class="mi">40</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.535</span><span class="p">,</span> <span class="n">i_c</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">n_c</span><span class="p">)</span><span class="o">+</span> <span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;left&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">low_c</span><span class="p">,</span> <span class="n">high_c</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">low_dense_ls</span><span class="p">,</span> <span class="n">high_dense_ls</span><span class="p">):</span>
    <span class="n">offset</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">rng</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.1</span>
    <span class="n">tried_cs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">low_c</span><span class="p">,</span> <span class="n">high_c</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">offset</span><span class="p">,]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">tried_cs</span><span class="p">),</span> <span class="n">tried_cs</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">)</span>
    
<span class="k">for</span> <span class="n">i_c</span><span class="p">,</span> <span class="n">n_c</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">bincount_dense</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="mf">1.4</span><span class="p">,</span> <span class="n">i_c</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="n">n_c</span><span class="o">*</span><span class="mi">40</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">1.535</span><span class="p">,</span> <span class="n">i_c</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">n_c</span><span class="p">)</span><span class="o">+</span> <span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;left&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Type of layer&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Number of layers&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;Convolutional&quot;</span><span class="p">,</span> <span class="s2">&quot;Dense&quot;</span><span class="p">],</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">45</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">7</span><span class="p">]);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Number of layers in prior works&#39; architectures&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.05</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="p">[</span><span class="s1">&#39;Study&#39;</span><span class="p">,</span> <span class="s1">&#39;Decoding problem&#39;</span><span class="p">,</span> <span class="s1">&#39;External baseline&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">to_markdown</span><span class="p">(</span><span class="n">showindex</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:left head"><p>Study</p></th>
<th class="text-align:left head"><p>Decoding problem</p></th>
<th class="text-align:left head"><p>External baseline</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:left"><p>This manuscript, Schirrmeister et. al (2017)</p></td>
<td class="text-align:left"><p>Imagined and executed movement classes, within subject</p></td>
<td class="text-align:left"><p>FBCSP + rLDA</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Single-trial EEG classification of motor imagery using deep convolutional neural networks, citet{tang_single-trial_2017}</p></td>
<td class="text-align:left"><p>Imagined movement classes, within-subject</p></td>
<td class="text-align:left"><p>FBCSP</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>EEGNet: A Compact Convolutional Network for EEG-based Brain-Computer Interfaces, <span id="id69">[<a class="reference internal" href="References.html#id150" title="Vernon J. Lawhern, Amelia J. Solon, Nicholas R. Waytowich, Stephen M. Gordon, Chou P. Hung, and Brent J. Lance. EEGNet: A Compact Convolutional Network for EEG-based Brain-Computer Interfaces. arXiv:1611.08024 [cs, q-bio, stat], November 2016. arXiv: 1611.08024. URL: http://arxiv.org/abs/1611.08024 (visited on 2016-12-20).">Lawhern <em>et al.</em>, 2016</a>]</span></p></td>
<td class="text-align:left"><p>Oddball response (RSVP), error response (ERN), movement classes (voluntarily started and imagined)</p></td>
<td class="text-align:left"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Remembered or Forgotten? –- An EEG-Based Computational Prediction Approach, <span id="id70">[<a class="reference internal" href="References.html#id222" title="Xuyun Sun, Cunle Qian, Zhongqin Chen, Zhaohui Wu, Benyan Luo, and Gang Pan. Remembered or Forgotten?—An EEG-Based Computational Prediction Approach. PLOS ONE, 11(12):e0167497, December 2016. URL: http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0167497 (visited on 2017-02-14), doi:10.1371/journal.pone.0167497.">Sun <em>et al.</em>, 2016</a>]</span></p></td>
<td class="text-align:left"><p>Memory performance, within-subject</p></td>
<td class="text-align:left"><p></p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Multimodal Neural Network for Rapid Serial Visual Presentation Brain Computer Interface, <span id="id71">[<a class="reference internal" href="References.html#id196" title="Ran Manor, Liran Mishali, and Amir B. Geva. Multimodal Neural Network for Rapid Serial Visual Presentation Brain Computer Interface. Frontiers in Computational Neuroscience, December 2016. URL: http://www.ncbi.nlm.nih.gov/pmc/articles/PMC5168930/ (visited on 2017-02-03), doi:10.3389/fncom.2016.00130.">Manor <em>et al.</em>, 2016</a>]</span></p></td>
<td class="text-align:left"><p>Oddball response using RSVP and image (combined image-EEG decoding), within-subject</p></td>
<td class="text-align:left"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>A novel deep learning approach for classification of EEG motor imagery signals, <span id="id72">[<a class="reference internal" href="References.html#id215" title="Yousef Rezaei Tabar and Ugur Halici. A novel deep learning approach for classification of EEG motor imagery signals. Journal of Neural Engineering, 14(1):016003, 2017. URL: http://stacks.iop.org/1741-2552/14/i=1/a=016003 (visited on 2017-02-14), doi:10.1088/1741-2560/14/1/016003.">Tabar and Halici, 2017</a>]</span></p></td>
<td class="text-align:left"><p>Imagined and executed movement classes, within-subject</p></td>
<td class="text-align:left"><p>Weights (spatial + frequential)</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Predicting Seizures from Electroencephalography Recordings: A Knowledge Transfer Strategy, <span id="id73">[<a class="reference internal" href="References.html#id172" title="J. Liang, R. Lu, C. Zhang, and F. Wang. Predicting Seizures from Electroencephalography Recordings: A Knowledge Transfer Strategy. In 2016 IEEE International Conference on Healthcare Informatics (ICHI), 184–191. October 2016. doi:10.1109/ICHI.2016.27.">Liang <em>et al.</em>, 2016</a>]</span></p></td>
<td class="text-align:left"><p>Seizure prediction, within-subject</p></td>
<td class="text-align:left"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>EEG-based prediction of driver’s cognitive performance by deep convolutional neural network, <span id="id74">[<a class="reference internal" href="References.html#id180" title="Mehdi Hajinoroozi, Zijing Mao, Tzyy-Ping Jung, Chin-Teng Lin, and Yufei Huang. EEG-based prediction of driver's cognitive performance by deep convolutional neural network. Signal Processing: Image Communication, 47:549–555, September 2016. URL: http://www.sciencedirect.com/science/article/pii/S0923596516300832 (visited on 2016-12-20), doi:10.1016/j.image.2016.05.018.">Hajinoroozi <em>et al.</em>, 2016</a>]</span></p></td>
<td class="text-align:left"><p>Driver performance, within- and cross-subject</p></td>
<td class="text-align:left"><p></p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Deep learning for epileptic intracranial EEG data, <span id="id75">[<a class="reference internal" href="References.html#id154" title="A. Antoniades, L. Spyrou, C. C. Took, and S. Sanei. Deep learning for epileptic intracranial EEG data. In 2016 IEEE 26th International Workshop on Machine Learning for Signal Processing (MLSP), 1–6. September 2016. doi:10.1109/MLSP.2016.7738824.">Antoniades <em>et al.</em>, 2016</a>]</span></p></td>
<td class="text-align:left"><p>Epileptic discharges, cross-subject</p></td>
<td class="text-align:left"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Learning Robust Features using Deep Learning for Automatic Seizure Detection, <span id="id76">[<a class="reference internal" href="References.html#id182" title="Pierre Thodoroff, Joelle Pineau, and Andrew Lim. Learning Robust Features using Deep Learning for Automatic Seizure Detection. In JMLR Workshop and Conference Proceedings, volume 56. 2016. URL: http://www.jmlr.org/proceedings/papers/v56/Thodoroff16.pdf (visited on 2017-02-14).">Thodoroff <em>et al.</em>, 2016</a>]</span></p></td>
<td class="text-align:left"><p>Start of epileptic seizure, within- and cross-subject</p></td>
<td class="text-align:left"><p>Hand crafted features + SVM</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Single-trial EEG RSVP classification using convolutional neural networks, <span id="id77">[<a class="reference internal" href="References.html#id200" title="Jared Shamwell, Hyungtae Lee, Heesung Kwon, Amar R. Marathe, Vernon Lawhern, and William Nothwang. Single-trial EEG RSVP classification using convolutional neural networks. In Thomas George, Achyut K. Dutta, and M. Saif Islam, editors, SPIE Defense+ Security, volume 9836. International Society for Optics and Photonics, May 2016. URL: http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.2224172 (visited on 2017-02-14), doi:10.1117/12.2224172.">Shamwell <em>et al.</em>, 2016</a>]</span></p></td>
<td class="text-align:left"><p>Oddball response (RSVP), groupwise (ConvNet trained on all subjects)</p></td>
<td class="text-align:left"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Wearable seizure detection using convolutional neural networks with transfer learning, <span id="id78">[<a class="reference internal" href="References.html#id204" title="A. Page, C. Shea, and T. Mohsenin. Wearable seizure detection using convolutional neural networks with transfer learning. In 2016 IEEE International Symposium on Circuits and Systems (ISCAS), 1086–1089. May 2016. doi:10.1109/ISCAS.2016.7527433.">Page <em>et al.</em>, 2016</a>]</span></p></td>
<td class="text-align:left"><p>Seizure detection, cross-subject, within-subject, groupwise</p></td>
<td class="text-align:left"><p>Multiple: spectral features, higher order statistics + linear-SVM, RBF-SVM, …</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Learning Representations from EEG with Deep Recurrent-Convolutional Neural Networks, <span id="id79">[<a class="reference internal" href="References.html#id197" title="Pouya Bashivan, Irina Rish, Mohammed Yeasin, and Noel Codella. Learning Representations from EEG with Deep Recurrent-Convolutional Neural Networks. In arXiv:1511.06448 [cs]. 2016. arXiv: 1511.06448. URL: http://arxiv.org/abs/1511.06448 (visited on 2016-12-20).">Bashivan <em>et al.</em>, 2016</a>]</span></p></td>
<td class="text-align:left"><p>Cognitive load (number of characters to memorize), cross-subject</p></td>
<td class="text-align:left"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Deep Feature Learning for EEG Recordings, <span id="id80">[<a class="reference internal" href="References.html#id226" title="Sebastian Stober. Learning Discriminative Features from Electroencephalography Recordings by Encoding Similarity Constraints. In Bernstein Conference 2016. 2016. doi:10.12751/nncn.bc2016.0223.">Stober, 2016</a>]</span></p></td>
<td class="text-align:left"><p>Type of music rhythm, groupwise (ensembles of leave-one-subject-out trained models, evaluated on separate test set of same subjects)</p></td>
<td class="text-align:left"><p></p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Convolutional Neural Network for Multi-Category Rapid Serial Visual Presentation BCI, <span id="id81">[<a class="reference internal" href="References.html#id147" title="Ran Manor and Amir B. Geva. Convolutional Neural Network for Multi-Category Rapid Serial Visual Presentation BCI. Frontiers in Computational Neuroscience, 9:146, 2015. doi:10.3389/fncom.2015.00146.">Manor and Geva, 2015</a>]</span></p></td>
<td class="text-align:left"><p>Oddball response (RSVP), within-subject</p></td>
<td class="text-align:left"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Parallel Convolutional-Linear Neural Network for Motor Imagery Classification, <span id="id82">[<a class="reference internal" href="References.html#id117" title="S. Sakhavi, C. Guan, and S. Yan. Parallel convolutional-linear neural network for motor imagery classification. In Signal Processing Conference (EUSIPCO), 2015 23rd European, 2736–2740. August 2015. doi:10.1109/EUSIPCO.2015.7362882.">Sakhavi <em>et al.</em>, 2015</a>]</span></p></td>
<td class="text-align:left"><p>Imagined movement classes, within-subject</p></td>
<td class="text-align:left"><p></p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Using Convolutional Neural networks to Recognize Rhythm Stimuli form Electroencephalography Recordings, <span id="id83">[<a class="reference internal" href="References.html#id184" title="Sebastian Stober, Daniel J. Cameron, and Jessica A. Grahn. Using Convolutional Neural Networks to Recognize Rhythm Stimuli from Electroencephalography Recordings. In Proceedings of the 27th International Conference on Neural Information Processing Systems, NIPS'14, 1449–1457. Cambridge, MA, USA, 2014. MIT Press. URL: http://dl.acm.org/citation.cfm?id=2968826.2968988 (visited on 2016-12-20).">Stober <em>et al.</em>, 2014</a>]</span></p></td>
<td class="text-align:left"><p>Type of music rhythm, within-subject</p></td>
<td class="text-align:left"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Convolutional deep belief networks for feature extraction of EEG signal, <span id="id84">[<a class="reference internal" href="References.html#id137" title="Y. Ren and Y. Wu. Convolutional deep belief networks for feature extraction of EEG signal. In 2014 International Joint Conference on Neural Networks (IJCNN), 2850–2853. July 2014. doi:10.1109/IJCNN.2014.6889383.">Ren and Wu, 2014</a>]</span></p></td>
<td class="text-align:left"><p>Imagined movement classes, within-subject</p></td>
<td class="text-align:left"><p></p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Deep feature learning using target priors with applications in ECoG signal decoding for BCI, <span id="id85">[<a class="reference internal" href="References.html#id209" title="Zuoguan Wang, Siwei Lyu, Gerwin Schalk, and Qiang Ji. Deep Feature Learning Using Target Priors with Applications in ECoG Signal Decoding for BCI. In Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence, IJCAI '13, 1785–1791. Beijing, China, 2013. AAAI Press. URL: http://dl.acm.org/citation.cfm?id=2540128.2540384 (visited on 2017-01-16).">Wang <em>et al.</em>, 2013</a>]</span></p></td>
<td class="text-align:left"><p>Finger flexion trajectory (regression), within-subject</p></td>
<td class="text-align:left"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Convolutional neural networks for P300 detection with application to brain-computer interfaces, <span id="id86">[<a class="reference internal" href="References.html#id212" title="Hubert Cecotti and Axel Graser. Convolutional Neural Networks for P300 Detection with Application to Brain-Computer Interfaces. IEEE Transactions on Pattern Analysis and Machine Intelligence, 33(3):433–445, March 2011. URL: http://dx.doi.org/10.1109/TPAMI.2010.125 (visited on 2016-12-20), doi:10.1109/TPAMI.2010.125.">Cecotti and Graser, 2011</a>]</span></p></td>
<td class="text-align:left"><p>Oddball / attention response using P300 speller, within-subject</p></td>
<td class="text-align:left"><p>Multiple: Linear SVM, gradient boosting, E-SVM, S-SVM, mLVQ, LDA, …</p></td>
</tr>
</tbody>
</table>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By The Jupyter Book community<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>