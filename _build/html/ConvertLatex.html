
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>old, copied &#8212; Deep Learning for Brain-Signal Decoding from Electroencephalography (EEG)</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script async="async" kind="hypothesis" src="https://hypothes.is/embed.js"></script>
    <script kind="utterances">

    var commentsRunWhenDOMLoaded = cb => {
    if (document.readyState != 'loading') {
        cb()
    } else if (document.addEventListener) {
        document.addEventListener('DOMContentLoaded', cb)
    } else {
        document.attachEvent('onreadystatechange', function() {
        if (document.readyState == 'complete') cb()
        })
    }
}

var addUtterances = () => {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src = "https://utteranc.es/client.js";
    script.async = "async";

    script.setAttribute("repo", "robintibor/eeg-deep-learning-phd-thesis");
    script.setAttribute("issue-term", "pathname");
    script.setAttribute("theme", "github-light");
    script.setAttribute("label", "üí¨ comment");
    script.setAttribute("crossorigin", "anonymous");

    sections = document.querySelectorAll("div.section");
    if (sections !== null) {
        section = sections[sections.length-1];
        section.appendChild(script);
    }
}
commentsRunWhenDOMLoaded(addUtterances);
</script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/braindecode-logo.svg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Deep Learning for Brain-Signal Decoding from Electroencephalography (EEG)</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Abstract.html">
   Deep Learning for Brain-Signal Decoding from Electroencephalography (EEG)
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="PriorWork.html">
   Prior Work
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="DeepArchitectures.html">
   Neural Network Architectures for EEG-Decoding
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="CroppedTraining.html">
   Cropped Training
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="PerturbationVisualization.html">
   Perturbation Visualization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="MovementDecoding.html">
   Decoding Movement-Related Brain Activity
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="TaskDecoding.html">
   Further Task-Related Decoding
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Pathology.html">
   Decoding Pathology
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Invertible.html">
   Invertible Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="FutureWork.html">
   Future Work
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="References.html">
   References
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/ConvertLatex.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/ConvertLatex.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   old, copied
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#visulization-table-with-empty-papers">
     visulization table with empty papers
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#layer-stuff">
   layer stuff
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="old-copied">
<h1>old, copied<a class="headerlink" href="#old-copied" title="Permalink to this headline">¬∂</a></h1>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:left head"><p>Study</p></th>
<th class="head"><p>Decoding problem</p></th>
<th class="head"><p>External Baseline</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:left"><p>This manuscript, Schirrmeister et. al (2017)</p></td>
<td><p>Imagined and executed movement classes, within subject</p></td>
<td><p>FBCSP + rLDA</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Single-trial EEG classification of motor imagery using deep convolutional neural networks, citet{tang_single-trial_2017}</p></td>
<td><p>Imagined movement classes, within-subject</p></td>
<td><p>FBCSP</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>EEGNet: A Compact Convolutional Network for EEG-based Brain-Computer Interfaces, <span id="id1">[<a class="reference internal" href="References.html#id122">LSW+16</a>]</span></p></td>
<td><p>Oddball response (RSVP), error response (ERN), movement classes (voluntarily started and imagined)</p></td>
<td><p>[recheck]</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Remembered or Forgotten? ‚Äì- An EEG-Based Computational Prediction Approach, <span id="id2">[<a class="reference internal" href="References.html#id195">SQC+16</a>]</span></p></td>
<td><p>Memory performance, within-subject</p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Multimodal Neural Network for Rapid Serial Visual Presentation Brain Computer Interface, <span id="id3">[<a class="reference internal" href="References.html#id169">MMG16</a>]</span></p></td>
<td><p>Oddball response using RSVP and image (combined image-EEG decoding), within-subject</p></td>
<td><p>Time, 0.3‚Äì20 Hz</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>A novel deep learning approach for classification of EEG motor imagery signals, <span id="id4">[<a class="reference internal" href="References.html#id188">TH17</a>]</span></p></td>
<td><p>Imagined and executed movement classes, within-subject</p></td>
<td><p>Frequency, 6‚Äì30 Hz</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Predicting Seizures from Electroencephalography Recordings: A Knowledge Transfer Strategy, <span id="id5">[<a class="reference internal" href="References.html#id144">LLZW16</a>]</span></p></td>
<td><p>Seizure prediction, within-subject</p></td>
<td><p>Frequency, 0‚Äì200 Hz</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>EEG-based prediction of driver‚Äôs cognitive performance by deep convolutional neural network, <span id="id6">[<a class="reference internal" href="References.html#id152">HMJ+16</a>]</span></p></td>
<td><p>Driver performance, within- and cross-subject</p></td>
<td><p>Time,  1‚Äì50 Hz</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Deep learning for epileptic intracranial EEG data, <span id="id7">[<a class="reference internal" href="References.html#id126">ASTS16</a>]</span></p></td>
<td><p>Epileptic discharges, cross-subject</p></td>
<td><p>Time,  0‚Äì100 HZ</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Learning Robust Features using Deep Learning for Automatic Seizure Detection, <span id="id8">[<a class="reference internal" href="References.html#id154">TPL16</a>]</span></p></td>
<td><p>Start of epileptic seizure, within- and cross-subject</p></td>
<td><p>Frequency, mean amplitude for 0‚Äì7 Hz, 7‚Äì14 Hz, 14‚Äì49 Hz</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Single-trial EEG RSVP classification using convolutional neural networks, <span id="id9">[<a class="reference internal" href="References.html#id173">SLK+16</a>]</span></p></td>
<td><p>Oddball response (RSVP), groupwise (ConvNet trained on all subjects)</p></td>
<td><p>Time, 0.5‚Äì50 Hz</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Wearable seizure detection using convolutional neural networks with transfer learning, <span id="id10">[<a class="reference internal" href="References.html#id177">PSM16</a>]</span></p></td>
<td><p>Seizure detection, cross-subject, within-subject, groupwise</p></td>
<td><p>Time,  0‚Äì128 Hz</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Learning Representations from EEG with Deep Recurrent-Convolutional Neural Networks, <span id="id11">[<a class="reference internal" href="References.html#id170">BRYC16</a>]</span></p></td>
<td><p>Cognitive load (number of characters to memorize), cross-subject</p></td>
<td><p>Frequency, mean power for 4‚Äì7 Hz, 8‚Äì13 Hz, 13‚Äì30 Hz</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Deep Feature Learning for EEG Recordings, <span id="id12">[<a class="reference internal" href="References.html#id199">Sto16</a>]</span></p></td>
<td><p>Type of music rhythm, groupwise (ensembles of leave-one-subject-out trained models, evaluated on separate test set of same subjects)</p></td>
<td><p>Time, 0.5‚Äì30Hz</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Convolutional Neural Network for Multi-Category Rapid Serial Visual Presentation BCI, <span id="id13">[<a class="reference internal" href="References.html#id119">MG15</a>]</span></p></td>
<td><p>Oddball response (RSVP), within-subject</p></td>
<td><p>Time, 0.1‚Äì50 Hz</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Parallel Convolutional-Linear Neural Network for Motor Imagery Classification, <span id="id14">[<a class="reference internal" href="References.html#id89">SGY15</a>]</span></p></td>
<td><p>Imagined movement classes, within-subject</p></td>
<td><p>Frequency, 4‚Äì40 Hz, using FBCSP</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Using Convolutional Neural networks to Recognize Rhythm Stimuli form Electroencephalography Recordings, <span id="id15">[<a class="reference internal" href="References.html#id156">SCG14</a>]</span></p></td>
<td><p>Type of music rhythm, within-subject</p></td>
<td><p>Time and frequency evaluated, 0-200 Hz</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Convolutional deep belief networks for feature extraction of EEG signal, <span id="id16">[<a class="reference internal" href="References.html#id109">RW14</a>]</span></p></td>
<td><p>Imagined movement classes, within-subject</p></td>
<td><p>Frequency, 8‚Äì30 Hz</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Deep feature learning using target priors with applications in ECoG signal decoding for BCI, <span id="id17">[<a class="reference internal" href="References.html#id182">WLSJ13</a>]</span></p></td>
<td><p>Finger flexion trajectory (regression), within-subject</p></td>
<td><p>Time, 0.15‚Äì200 Hz</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Convolutional neural networks for P300 detection with application to brain-computer interfaces, <span id="id18">[<a class="reference internal" href="References.html#id185">CG11</a>]</span></p></td>
<td><p>Oddball / attention response using P300 speller, within-subject</p></td>
<td><p>Time, 0.1-20 Hz</p></td>
</tr>
</tbody>
</table>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:left head"><p>Study</p></th>
<th class="head"><p>Decoding problem</p></th>
<th class="head"><p>Input domain</p></th>
<th class="head"><p>Conv/dense layers</p></th>
<th class="head"><p>Design choices</p></th>
<th class="head"><p>Training strategies</p></th>
<th class="head"><p>External baseline</p></th>
<th class="head"><p>Visualization type(s)</p></th>
<th class="head"><p>Visualization findings</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:left"><p>This manuscript, Schirrmeister et. al (2017)</p></td>
<td><p>Imagined and executed movement classes, within subject</p></td>
<td><p>Time,  0‚Äì125 Hz</p></td>
<td><p>5/1</p></td>
<td><p>Different ConvNet architectures <br>Nonlinearities and pooling modes <br>Regularization and intermediate normalization layers <br>Factorized convolutions <br>Splitted vs one-step convolutions</p></td>
<td><p>Trial-wise vs. cropped training strategy</p></td>
<td><p>FBCSP + rLDA</p></td>
<td><p>Feature activation correlation <br>Feature-perturbation prediction correlation</p></td>
<td><p>See Section \ref{subsec:results-visualization}</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Single-trial EEG classification of motor imagery using deep convolutional neural networks, citet{tang_single-trial_2017}</p></td>
<td><p>Imagined movement classes, within-subject</p></td>
<td><p>Time,  8‚Äì30 Hz</p></td>
<td><p>2/2</p></td>
<td><p></p></td>
<td><p></p></td>
<td><p>FBCSP</p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>EEGNet: A Compact Convolutional Network for EEG-based Brain-Computer Interfaces, <span id="id19">[<a class="reference internal" href="References.html#id122">LSW+16</a>]</span></p></td>
<td><p>Oddball response (RSVP), error response (ERN), movement classes (voluntarily started and imagined)</p></td>
<td><p>Time, 0.1‚Äì40 Hz</p></td>
<td><p>3/1</p></td>
<td><p>Kernel sizes</p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Remembered or Forgotten? ‚Äì- An EEG-Based Computational Prediction Approach, <span id="id20">[<a class="reference internal" href="References.html#id195">SQC+16</a>]</span></p></td>
<td><p>Memory performance, within-subject</p></td>
<td><p>Time, 0.05‚Äì15 Hz</p></td>
<td><p>2/2</p></td>
<td><p></p></td>
<td><p>Different time windows</p></td>
<td><p></p></td>
<td><p>Weights (spatial)</p></td>
<td><p>Largest weights found over p\refrontal and temporal cortex</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Multimodal Neural Network for Rapid Serial Visual Presentation Brain Computer Interface, <span id="id21">[<a class="reference internal" href="References.html#id169">MMG16</a>]</span></p></td>
<td><p>Oddball response using RSVP and image (combined image-EEG decoding), within-subject</p></td>
<td><p>Time, 0.3‚Äì20 Hz</p></td>
<td><p>3/2</p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p>Weights <br> Activations <br> Saliency maps by gradient</p></td>
<td><p>Weights showed typical P300 distribution <br>Activations were high at plausible times (300-500ms) <br>Saliency maps showed plausible spatio-temporal plots</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>A novel deep learning approach for classification of EEG motor imagery signals, <span id="id22">[<a class="reference internal" href="References.html#id188">TH17</a>]</span></p></td>
<td><p>Imagined and executed movement classes, within-subject</p></td>
<td><p>Frequency, 6‚Äì30 Hz</p></td>
<td><p>1/1</p></td>
<td><p>multicolumn{2}{p{0.285	extwidth}}{Addition of six-layer stacked autoencoder on ConvNet features <br> Kernel sizes}</p></td>
<td><p>FBCSP, Twin SVM, DDFBS, Bi-spectrum, RQNN</p></td>
<td><p>Weights (spatial + frequential)</p></td>
<td><p>Some weights represented difference of values of two electrodes on different sides of head</p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Predicting Seizures from Electroencephalography Recordings: A Knowledge Transfer Strategy, <span id="id23">[<a class="reference internal" href="References.html#id144">LLZW16</a>]</span></p></td>
<td><p>Seizure prediction, within-subject</p></td>
<td><p>Frequency, 0‚Äì200 Hz</p></td>
<td><p>1/2</p></td>
<td><p></p></td>
<td><p>Different subdivisions of frequency range <br>Different lengths of time crops <br>Transfer learning with auxiliary non-epilepsy datasets</p></td>
<td><p></p></td>
<td><p>Weights <br> Clustering of weights</p></td>
<td><p>Clusters of weights showed typical frequency band subdivision (delta, theta, alpha, beta, gamma)</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>EEG-based prediction of driver‚Äôs cognitive performance by deep convolutional neural network, <span id="id24">[<a class="reference internal" href="References.html#id152">HMJ+16</a>]</span></p></td>
<td><p>Driver performance, within- and cross-subject</p></td>
<td><p>Time,  1‚Äì50 Hz</p></td>
<td><p>1/3</p></td>
<td><p>multicolumn{2}{p{0.285	extwidth}}{Replacement of convolutional layers by restricted Boltzmann machines with slightly varied network architecture}</p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Deep learning for epileptic intracranial EEG data, <span id="id25">[<a class="reference internal" href="References.html#id126">ASTS16</a>]</span></p></td>
<td><p>Epileptic discharges, cross-subject</p></td>
<td><p>Time,  0‚Äì100 HZ</p></td>
<td><p>1‚Äì2/2</p></td>
<td><p>1 or 2 convolutional layers</p></td>
<td><p></p></td>
<td><p></p></td>
<td><p>Weights <br>Correlation weights and interictal epileptic discharges (IED) <br>Activations</p></td>
<td><p>Weights increasingly correlated with IED waveforms with increasing number of training iterations <br>Second layer captured more complex and well-defined epileptic shapes than first layer <br>IEDs led to highly synchronized activations for neighbouring electrodes</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Learning Robust Features using Deep Learning for Automatic Seizure Detection, <span id="id26">[<a class="reference internal" href="References.html#id154">TPL16</a>]</span></p></td>
<td><p>Start of epileptic seizure, within- and cross-subject</p></td>
<td><p>Frequency, mean amplitude for 0‚Äì7 Hz, 7‚Äì14 Hz, 14‚Äì49 Hz</p></td>
<td><p>3/1 (+ LSTM as postprocessor)</p></td>
<td><p></p></td>
<td><p></p></td>
<td><p>Hand crafted features + SVM</p></td>
<td><p>Input occlusion and effect on prediction accuracy</p></td>
<td><p>Allowed to locate areas critical for seizure</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Single-trial EEG RSVP classification using convolutional neural networks, <span id="id27">[<a class="reference internal" href="References.html#id173">SLK+16</a>]</span></p></td>
<td><p>Oddball response (RSVP), groupwise (ConvNet trained on all subjects)</p></td>
<td><p>Time, 0.5‚Äì50 Hz</p></td>
<td><p>4/3</p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p>Weights (spatial)</p></td>
<td><p>Some filter weights had expected topographic distributions for P300 <br>Others filters had large weights on areas not traditionally associated with P300</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Wearable seizure detection using convolutional neural networks with transfer learning, <span id="id28">[<a class="reference internal" href="References.html#id177">PSM16</a>]</span></p></td>
<td><p>Seizure detection, cross-subject, within-subject, groupwise</p></td>
<td><p>Time,  0‚Äì128 Hz</p></td>
<td><p>1-3/1-3</p></td>
<td><p></p></td>
<td><p>Cross-subject supervised training, within-subject finetuning of fully connected layers</p></td>
<td><p>Multiple: spectral features, higher order statistics + linear-SVM, RBF-SVM, ‚Ä¶</p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Learning Representations from EEG with Deep Recurrent-Convolutional Neural Networks, <span id="id29">[<a class="reference internal" href="References.html#id170">BRYC16</a>]</span></p></td>
<td><p>Cognitive load (number of characters to memorize), cross-subject</p></td>
<td><p>Frequency, mean power for 4‚Äì7 Hz, 8‚Äì13 Hz, 13‚Äì30 Hz</p></td>
<td><p>3‚Äì7/2 (+ LSTM or other temporal post-processing (see design choices))</p></td>
<td><p>Number of convolutional layers <br>Temporal processing of ConvNet output by max pooling, temporal convolution, LSTM or temporal convolution + LSTM</p></td>
<td><p></p></td>
<td><p></p></td>
<td><p>Inputs that maximally activate given filter <br>Activations of these inputs <br>‚ÄùDeconvolution‚Äù for these inputs</p></td>
<td><p>Different filters were sensitive to different frequency bands <br>Later layers had more spatially localized activations <br>Learned features had noticeable links to well-known electrophysiological markers of cognitive load <br></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Deep Feature Learning for EEG Recordings, <span id="id30">[<a class="reference internal" href="References.html#id199">Sto16</a>]</span></p></td>
<td><p>Type of music rhythm, groupwise (ensembles of leave-one-subject-out trained models, evaluated on separate test set of same subjects)</p></td>
<td><p>Time, 0.5‚Äì30Hz</p></td>
<td><p>2/1</p></td>
<td><p>Kernel sizes</p></td>
<td><p>Pretraining first layer as convolutional autoencoder with different constraints</p></td>
<td><p></p></td>
<td><p>Weights (spatial+3 timesteps, pretrained as autoencoder)</p></td>
<td><p>Different constraints led to different weights, one type of constraints could enforce weights that are similar across subjects; other type of constraints led to weights that have similar spatial topographies under different architectural configurations and preprocessings</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Convolutional Neural Network for Multi-Category Rapid Serial Visual Presentation BCI, <span id="id31">[<a class="reference internal" href="References.html#id119">MG15</a>]</span></p></td>
<td><p>Oddball response (RSVP), within-subject</p></td>
<td><p>Time, 0.1‚Äì50 Hz</p></td>
<td><p>3/3 (Spatio-temporal regularization)</p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p>Weights <br> Mean and single-trial activations</p></td>
<td><p>Spatiotemporal regularization led to softer peaks in weights <br>Spatial weights showed typical P300 distribution <br>Activations mostly had peaks at typical times (300-400ms)</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Parallel Convolutional-Linear Neural Network for Motor Imagery Classification, <span id="id32">[<a class="reference internal" href="References.html#id89">SGY15</a>]</span></p></td>
<td><p>Imagined movement classes, within-subject</p></td>
<td><p>Frequency, 4‚Äì40 Hz, using FBCSP</p></td>
<td><p>2/2 (Final fully connected layer uses concatenated output by convolutionaland fully connected layers)</p></td>
<td><p>Combination ConvNet and MLP (trained on different features) vs. only ConvNet vs. only MLP</p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Using Convolutional Neural networks to Recognize Rhythm Stimuli form Electroencephalography Recordings, <span id="id33">[<a class="reference internal" href="References.html#id156">SCG14</a>]</span></p></td>
<td><p>Type of music rhythm, within-subject</p></td>
<td><p>Time and frequency evaluated, 0-200 Hz</p></td>
<td><p>1-2/1</p></td>
<td><p>Best values from automatic hyperparameter optimization: frequency cutoff, one vs two layers, kernel sizes, number of channels, pooling width</p></td>
<td><p>Best values from automatic hyperparameter optimization: learning rate, learning rate decay, momentum, final momentum</p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Convolutional deep belief networks for feature extraction of EEG signal, <span id="id34">[<a class="reference internal" href="References.html#id109">RW14</a>]</span></p></td>
<td><p>Imagined movement classes, within-subject</p></td>
<td><p>Frequency, 8‚Äì30 Hz</p></td>
<td><p>2/0 (Convolutional deep belief network, separately trained RBF-SVM classifier)</p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Deep feature learning using target priors with applications in ECoG signal decoding for BCI, <span id="id35">[<a class="reference internal" href="References.html#id182">WLSJ13</a>]</span></p></td>
<td><p>Finger flexion trajectory (regression), within-subject</p></td>
<td><p>Time, 0.15‚Äì200 Hz</p></td>
<td><p>3/1 (Convolutional layers trained as convolutional stacked autoencoder with target prior)</p></td>
<td><p>Partially supervised CSA</p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Convolutional neural networks for P300 detection with application to brain-computer interfaces, <span id="id36">[<a class="reference internal" href="References.html#id185">CG11</a>]</span></p></td>
<td><p>Oddball / attention response using P300 speller, within-subject</p></td>
<td><p>Time, 0.1-20 Hz</p></td>
<td><p>2/2</p></td>
<td><p>Electrode subset (fixed or automatically determined) <br>Using only one spatial filter <br>Different ensembling strategies</p></td>
<td><p></p></td>
<td><p>Multiple: Linear SVM, gradient boosting, E-SVM, S-SVM, mLVQ, LDA, ‚Ä¶</p></td>
<td><p>Weights</p></td>
<td><p>Spatial filters were similar for different architectures <br>Spatial filters were different (more focal, more diffuse) for different subjects</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">re</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">This manuscript, Schirrmeister et. al (2017) &amp;</span>
<span class="s2">Imagined and executed movement classes, within subject &amp;</span>
<span class="s2">Time, \hspace</span><span class="si">{1cm}</span><span class="s2"> 0--125 Hz &amp; 5/1 &amp;</span>
<span class="s2">Different ConvNet architectures \cellbr</span>
<span class="s2">Nonlinearities and pooling modes \cellbr</span>
<span class="s2">Regularization and intermediate normalization layers \cellbr</span>
<span class="s2">Factorized convolutions \cellbr</span>
<span class="s2">Splitted vs one-step convolutions &amp;</span>
<span class="s2">Trial-wise vs. cropped training strategy &amp;</span>
<span class="s2">FBCSP + rLDA &amp; </span>
<span class="s2">Feature activation correlation \cellbr</span>
<span class="s2">Feature-perturbation prediction correlation &amp;</span>
<span class="s2">See Section \ref{subsec:results-visualization}</span>
<span class="s2">\\</span>
<span class="s2">\hdashline </span>

<span class="s2">Single-trial EEG classification of motor imagery using deep convolutional neural networks, \citet{tang_single-trial_2017} &amp;</span>
<span class="s2">Imagined movement classes, within-subject &amp;</span>
<span class="s2">Time, \hspace</span><span class="si">{1cm}</span><span class="s2"> 8--30 Hz &amp; 2/2 &amp; &amp; </span>
<span class="s2">&amp; FBCSP &amp; &amp;&quot;&quot;&quot;</span>

<span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;&amp;&quot;</span><span class="p">,</span> <span class="s2">&quot;|&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;\cellbr&quot;</span><span class="p">,</span> <span class="s2">&quot;&lt;br&gt;&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;\hdashline&#39;</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">|&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\\</span><span class="s1">&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;ref&quot;</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;\ref&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="s1">&#39;‚Äì&#39;</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;hspace{[^}]+}&#39;</span><span class="p">,</span> <span class="sa">r</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="s2">&quot;| &quot;</span> <span class="o">+</span> <span class="n">b</span> <span class="o">+</span> <span class="s2">&quot; |&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>| This manuscript, Schirrmeister et. al (2017) |Imagined and executed movement classes, within subject |Time,  0‚Äì125 Hz | 5/1 |Different ConvNet architectures &lt;br&gt;Nonlinearities and pooling modes &lt;br&gt;Regularization and intermediate normalization layers &lt;br&gt;Factorized convolutions &lt;br&gt;Splitted vs one-step convolutions |Trial-wise vs. cropped training strategy |FBCSP + rLDA | Feature activation correlation &lt;br&gt;Feature-perturbation prediction correlation |See Section \ref{subsec:results-visualization}
| Single-trial EEG classification of motor imagery using deep convolutional neural networks, citet{tang_single-trial_2017} |Imagined movement classes, within-subject |Time,  8‚Äì30 Hz | 2/2 | | | FBCSP | | |
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">EEGNet: A Compact Convolutional Network for EEG-based Brain-Computer Interfaces, \citet{lawhern_eegnet:_2016} &amp; </span>
<span class="s2">Oddball response (RSVP), error response (ERN), movement classes (voluntarily started and imagined) &amp;</span>
<span class="s2">Time, 0.1--40 Hz &amp; 3/1 &amp;  Kernel sizes &amp; </span>
<span class="s2">&amp;   &amp; &amp;</span>
<span class="se">\\</span><span class="s2"></span>
<span class="s2">\hdashline</span>
<span class="s2"> </span>
<span class="s2">Remembered or Forgotten? --- An EEG-Based Computational Prediction Approach, \citet</span><span class="si">{sun_remembered_2016}</span><span class="s2"> &amp; </span>
<span class="s2">Memory performance, within-subject &amp;</span>
<span class="s2">Time, 0.05--15 Hz &amp; 2/2 &amp; &amp; Different time windows &amp;  &amp;</span>
<span class="s2">Weights (spatial) &amp; </span>
<span class="s2">Largest weights found over prefrontal and temporal cortex</span>
<span class="se">\\</span><span class="s2"></span>
<span class="s2">\hdashline</span>

<span class="s2">Multimodal Neural Network for Rapid Serial Visual Presentation Brain Computer Interface, \citet</span><span class="si">{manor_multimodal_2016}</span><span class="s2"></span>
<span class="s2">&amp;</span>
<span class="s2">Oddball response using RSVP and image (combined image-EEG decoding), within-subject&amp;</span>
<span class="s2">Time, 0.3--20 Hz &amp; 3/2 &amp; &amp; &amp;  &amp; </span>
<span class="s2">Weights \cellbr Activations \cellbr Saliency maps by gradient &amp;</span>
<span class="s2">Weights showed typical P300 distribution \cellbr</span>
<span class="s2">Activations were high at plausible times (300-500ms) \cellbr</span>
<span class="s2">Saliency maps showed plausible spatio-temporal plots</span>
<span class="se">\\</span><span class="s2"></span>
<span class="s2">\hdashline</span>
<span class="s2"> </span>
<span class="s2">A novel deep learning approach for classification of EEG motor imagery signals, \citet</span><span class="si">{tabar_novel_2017}</span><span class="s2"> &amp;</span>
<span class="s2">Imagined and executed movement classes, within-subject &amp;</span>
<span class="s2">Frequency, 6--30 Hz &amp; 1/1 &amp; </span>
<span class="s2">\multicolumn</span><span class="si">{2}</span><span class="s2">{p{0.285</span><span class="se">\t</span><span class="s2">extwidth}}{Addition of six-layer stacked autoencoder on ConvNet features \cellbr Kernel sizes} </span>
<span class="s2">&amp; FBCSP, Twin SVM, DDFBS, Bi-spectrum, RQNN  &amp; Weights (spatial + frequential) &amp;</span>
<span class="s2">Some weights represented difference of values of two electrodes on different sides of head</span>
<span class="se">\\</span><span class="s2"></span>
<span class="s2">\hdashline</span>
<span class="s2"> </span>
<span class="s2">Predicting Seizures from Electroencephalography Recordings: A Knowledge Transfer Strategy, \citet</span><span class="si">{liang_predicting_2016}</span><span class="s2"> &amp;</span>
<span class="s2">Seizure prediction, within-subject &amp; Frequency, 0--200 Hz &amp; 1/2 &amp; &amp; </span>
<span class="s2">Different subdivisions of frequency range \cellbr</span>
<span class="s2">Different lengths of time crops \cellbr</span>
<span class="s2">Transfer learning with auxiliary non-epilepsy datasets &amp;</span>
<span class="s2">&amp; Weights \cellbr Clustering of weights &amp;</span>
<span class="s2">Clusters of weights showed typical frequency band subdivision (delta, theta, alpha, beta, gamma)</span>
<span class="se">\\</span><span class="s2"></span>
<span class="s2">\hdashline</span>
<span class="s2"> </span>
<span class="s2">EEG-based prediction of driver&#39;s cognitive performance by deep convolutional neural network, \citet{hajinoroozi_eeg-based_2016} &amp;</span>
<span class="s2">Driver performance, within- and cross-subject &amp;</span>
<span class="s2">Time, \hspace</span><span class="si">{1cm}</span><span class="s2"> 1--50 Hz &amp; 1/3 &amp;</span>
<span class="s2">\multicolumn</span><span class="si">{2}</span><span class="s2">{p{0.285</span><span class="se">\t</span><span class="s2">extwidth}}{Replacement of convolutional layers by restricted Boltzmann machines with slightly varied network architecture}  &amp; </span>
<span class="s2">&amp;</span>
<span class="se">\\</span><span class="s2"></span>
<span class="s2">\hdashline</span>
<span class="s2"> </span>
<span class="s2">Deep learning for epileptic intracranial EEG data, \citet</span><span class="si">{antoniades_deep_2016}</span><span class="s2"> &amp;</span>
<span class="s2">Epileptic discharges, cross-subject &amp; Time, \hspace</span><span class="si">{1cm}</span><span class="s2"> 0--100 HZ &amp; 1--2/2 &amp; 1 or 2 convolutional layers &amp;  &amp; &amp;</span>
<span class="s2">Weights \cellbr</span>
<span class="s2">Correlation weights and interictal epileptic discharges (IED) \cellbr</span>
<span class="s2">Activations &amp;</span>
<span class="s2">Weights increasingly correlated with IED waveforms with increasing number of training iterations \cellbr</span>
<span class="s2">Second layer captured more complex and well-defined epileptic shapes than first layer \cellbr</span>
<span class="s2">IEDs led to highly synchronized activations for neighbouring electrodes</span>
<span class="se">\\</span><span class="s2"></span>
<span class="s2">\hdashline</span>
<span class="s2"> </span>
<span class="s2">Learning Robust Features using Deep Learning for Automatic Seizure Detection, \citet</span><span class="si">{thodoroff_learning_2016}</span><span class="s2"> &amp;</span>
<span class="s2">Start of epileptic seizure, within- and cross-subject &amp;</span>
<span class="s2">Frequency, mean amplitude for 0--7 Hz, 7--14 Hz, 14--49 Hz &amp; 3/1 (+ LSTM as postprocessor) &amp; &amp;</span>
<span class="s2"> &amp; Hand crafted features + SVM &amp; Input occlusion and effect on prediction accuracy &amp;</span>
<span class="s2">Allowed to locate areas critical for seizure </span>
<span class="se">\\</span><span class="s2"></span>
<span class="s2">\hdashline</span>

<span class="s2">Single-trial EEG RSVP classification using convolutional neural networks, \citet{george_single-trial_2016} &amp;</span>
<span class="s2">Oddball response (RSVP), groupwise (ConvNet trained on all subjects) &amp;</span>
<span class="s2">Time, 0.5--50 Hz &amp; 4/3 &amp; &amp;  &amp;  &amp;</span>
<span class="s2">Weights (spatial) &amp;</span>
<span class="s2">Some filter weights had expected topographic distributions for P300 \cellbr</span>
<span class="s2">Others filters had large weights on areas not traditionally associated with P300</span>
<span class="se">\\</span><span class="s2"></span>
<span class="s2">\hdashline</span>

<span class="s2">Wearable seizure detection using convolutional neural networks with transfer learning, \citet</span><span class="si">{page_wearable_2016}</span><span class="s2"> &amp;</span>
<span class="s2">Seizure detection, cross-subject, within-subject, groupwise &amp;</span>
<span class="s2">Time, \hspace</span><span class="si">{1cm}</span><span class="s2"> 0--128 Hz &amp; 1-3/1-3 &amp; &amp; Cross-subject supervised training, within-subject finetuning of fully connected layers &amp;</span>
<span class="s2">Multiple: spectral features, higher order statistics + linear-SVM, RBF-SVM, ...&amp; &amp; </span>
<span class="se">\\</span><span class="s2"></span>
<span class="s2">\hdashline</span>

<span class="s2">Learning Representations from EEG with Deep Recurrent-Convolutional Neural Networks, \citet</span><span class="si">{bashivan_learning_2016}</span><span class="s2">  &amp;</span>
<span class="s2">Cognitive load (number of characters to memorize), cross-subject &amp; </span>
<span class="s2">Frequency, mean power for 4--7 Hz, 8--13 Hz, 13--30 Hz &amp; 3--7/2 (+ LSTM or other temporal post-processing (see design choices)) &amp;</span>
<span class="s2">Number of convolutional layers \cellbr</span>
<span class="s2">Temporal processing of ConvNet output by max pooling, temporal convolution, LSTM or temporal convolution + LSTM &amp; &amp; &amp;</span>
<span class="s2">Inputs that maximally activate given filter \cellbr</span>
<span class="s2">Activations of these inputs \cellbr</span>
<span class="s2">&quot;Deconvolution&quot; for these inputs &amp;</span>
<span class="s2">Different filters were sensitive to different frequency bands \cellbr</span>
<span class="s2">Later layers had more spatially localized activations \cellbr</span>
<span class="s2">Learned features had noticeable links to well-known electrophysiological markers of cognitive load \cellbr</span>
<span class="se">\\</span><span class="s2"></span>
<span class="s2">\hdashline</span>

<span class="s2">Deep Feature Learning for EEG Recordings, \citet</span><span class="si">{stober_learning_2016}</span><span class="s2"> &amp;</span>
<span class="s2">Type of music rhythm, groupwise (ensembles of leave-one-subject-out trained models, evaluated on separate test set of same subjects) &amp;</span>
<span class="s2">Time, 0.5--30Hz &amp; 2/1 &amp; Kernel sizes &amp; </span>
<span class="s2">Pretraining first layer as convolutional autoencoder with different constraints &amp;  &amp; </span>
<span class="s2">Weights (spatial+3 timesteps, pretrained as autoencoder) &amp; </span>
<span class="s2">Different constraints led to different weights, one type of constraints could enforce weights that are similar across subjects; other type of constraints led to weights that have similar spatial topographies under different architectural configurations and preprocessings</span>
<span class="se">\\</span><span class="s2"></span>
<span class="s2">\hdashline</span>

<span class="s2">Convolutional Neural Network for Multi-Category Rapid Serial Visual Presentation BCI, \citet</span><span class="si">{manor_convolutional_2015}</span><span class="s2"> &amp;</span>
<span class="s2">Oddball response (RSVP), within-subject &amp;</span>
<span class="s2">Time, 0.1--50 Hz &amp; 3/3 (Spatio-temporal regularization) &amp;&amp; &amp;&amp;</span>
<span class="s2">Weights \cellbr Mean and single-trial activations &amp;</span>
<span class="s2">Spatiotemporal regularization led to softer peaks in weights \cellbr</span>
<span class="s2">Spatial weights showed typical P300 distribution \cellbr</span>
<span class="s2">Activations mostly had peaks at typical times (300-400ms)</span>
<span class="se">\\</span><span class="s2"></span>
<span class="s2">\hdashline</span>

<span class="s2">Parallel Convolutional-Linear Neural Network for Motor Imagery Classification, \citet</span><span class="si">{sakhavi_parallel_2015}</span><span class="s2">  &amp;</span>
<span class="s2">Imagined movement classes, within-subject &amp;</span>
<span class="s2">Frequency, 4--40 Hz, using FBCSP &amp; 2/2 (Final fully connected layer uses concatenated output by convolutional</span>
<span class="s2">and fully connected layers) &amp;</span>
<span class="s2">Combination ConvNet and MLP (trained on different features) vs. only ConvNet vs. only MLP &amp; </span>
<span class="s2">&amp; &amp; &amp; </span>
<span class="se">\\</span><span class="s2"></span>
<span class="s2">\hdashline</span>

<span class="s2">Using Convolutional Neural networks to Recognize Rhythm Stimuli form Electroencephalography Recordings, \citet</span><span class="si">{stober_using_2014}</span><span class="s2"> &amp;</span>
<span class="s2">Type of music rhythm, within-subject &amp; Time and frequency evaluated, 0-200 Hz &amp; 1-2/1 &amp;</span>
<span class="s2">Best values from automatic hyperparameter optimization: frequency cutoff, one vs two layers, kernel sizes, number of channels, pooling width &amp;</span>
<span class="s2">Best values from automatic hyperparameter optimization: learning rate, learning rate decay, momentum, final momentum &amp;</span>
<span class="s2">&amp;</span>
<span class="se">\\</span><span class="s2"></span>
<span class="s2">\hdashline</span>

<span class="s2">Convolutional deep belief networks for feature extraction of EEG signal, \citet</span><span class="si">{ren_convolutional_2014}</span><span class="s2">  &amp;</span>
<span class="s2">Imagined movement classes, within-subject &amp;</span>
<span class="s2">Frequency, 8--30 Hz &amp;</span>
<span class="s2">2/0 (Convolutional deep belief network, separately trained RBF-SVM classifier) &amp; &amp; </span>
<span class="se">\\</span><span class="s2"></span>
<span class="s2">\hdashline</span>

<span class="s2">Deep feature learning using target priors with applications in ECoG signal decoding for BCI, \citet</span><span class="si">{wang_deep_2013}</span><span class="s2">  &amp;</span>
<span class="s2">Finger flexion trajectory (regression), within-subject &amp;</span>
<span class="s2">Time, 0.15--200 Hz &amp; 3/1 (Convolutional layers trained as convolutional stacked autoencoder with target prior) &amp;</span>
<span class="s2">Partially supervised CSA &amp; </span>
<span class="se">\\</span><span class="s2"></span>
<span class="s2">\hdashline</span>

<span class="s2">Convolutional neural networks for P300 detection with application to brain-computer interfaces, \citet</span><span class="si">{cecotti_convolutional_2011}</span><span class="s2">  &amp;</span>
<span class="s2">Oddball / attention response using P300 speller, within-subject &amp; Time, 0.1-20 Hz &amp; 2/2 &amp;</span>
<span class="s2">Electrode subset (fixed or automatically determined) \cellbr</span>
<span class="s2">Using only one spatial filter \cellbr</span>
<span class="s2">Different ensembling strategies &amp; </span>
<span class="s2">&amp;</span>
<span class="s2">Multiple: Linear SVM, gradient boosting, E-SVM, S-SVM, mLVQ, LDA, ... &amp;</span>
<span class="s2">Weights &amp;</span>
<span class="s2">Spatial filters were similar for different architectures \cellbr</span>
<span class="s2">Spatial filters were different (more focal, more diffuse) for different subjects</span>
<span class="se">\\</span><span class="s2"></span>
<span class="s2">\hdashline</span>
<span class="s2"> &quot;&quot;&quot;</span>

<span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;&amp;&quot;</span><span class="p">,</span> <span class="s2">&quot;|&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;\cellbr&quot;</span><span class="p">,</span> <span class="s2">&quot;&lt;br&gt;&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;\hdashline&#39;</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">|&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\\</span><span class="s1">&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;ref&quot;</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;\ref&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="s1">&#39;‚Äì&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;cite&quot;</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;\cite&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;citet&quot;</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;cite&quot;</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;hspace{[^}]+}&#39;</span><span class="p">,</span> <span class="sa">r</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;</span><span class="se">\\</span><span class="s2">cite{([^}]+)}&quot;</span><span class="p">,</span> <span class="s2">&quot;</span><span class="si">{cite}</span><span class="s2">`\g&lt;1&gt;`&quot;</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="s2">&quot;| &quot;</span> <span class="o">+</span> <span class="n">b</span> <span class="o">+</span> <span class="s2">&quot; |&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>| EEGNet: A Compact Convolutional Network for EEG-based Brain-Computer Interfaces, {cite}`lawhern_eegnet:_2016` | Oddball response (RSVP), error response (ERN), movement classes (voluntarily started and imagined) |Time, 0.1‚Äì40 Hz | 3/1 |  Kernel sizes | |   | |
| Remembered or Forgotten? ‚Äì- An EEG-Based Computational Prediction Approach, {cite}`sun_remembered_2016` | Memory performance, within-subject |Time, 0.05‚Äì15 Hz | 2/2 | | Different time windows |  |Weights (spatial) | Largest weights found over p\refrontal and temporal cortex
|Multimodal Neural Network for Rapid Serial Visual Presentation Brain Computer Interface, {cite}`manor_multimodal_2016`|Oddball response using RSVP and image (combined image-EEG decoding), within-subject|Time, 0.3‚Äì20 Hz | 3/2 | | |  | Weights &lt;br&gt; Activations &lt;br&gt; Saliency maps by gradient |Weights showed typical P300 distribution &lt;br&gt;Activations were high at plausible times (300-500ms) &lt;br&gt;Saliency maps showed plausible spatio-temporal plots
| A novel deep learning approach for classification of EEG motor imagery signals, {cite}`tabar_novel_2017` |Imagined and executed movement classes, within-subject |Frequency, 6‚Äì30 Hz | 1/1 | multicolumn{2}{p{0.285	extwidth}}{Addition of six-layer stacked autoencoder on ConvNet features &lt;br&gt; Kernel sizes} | FBCSP, Twin SVM, DDFBS, Bi-spectrum, RQNN  | Weights (spatial + frequential) |Some weights represented difference of values of two electrodes on different sides of head
| Predicting Seizures from Electroencephalography Recordings: A Knowledge Transfer Strategy, {cite}`liang_predicting_2016` |Seizure prediction, within-subject | Frequency, 0‚Äì200 Hz | 1/2 | | Different subdivisions of frequency range &lt;br&gt;Different lengths of time crops &lt;br&gt;Transfer learning with auxiliary non-epilepsy datasets || Weights &lt;br&gt; Clustering of weights |Clusters of weights showed typical frequency band subdivision (delta, theta, alpha, beta, gamma)
| EEG-based prediction of driver&#39;s cognitive performance by deep convolutional neural network, {cite}`hajinoroozi_eeg-based_2016` |Driver performance, within- and cross-subject |Time,  1‚Äì50 Hz | 1/3 |multicolumn{2}{p{0.285	extwidth}}{Replacement of convolutional layers by restricted Boltzmann machines with slightly varied network architecture}  | |
| Deep learning for epileptic intracranial EEG data, {cite}`antoniades_deep_2016` |Epileptic discharges, cross-subject | Time,  0‚Äì100 HZ | 1‚Äì2/2 | 1 or 2 convolutional layers |  | |Weights &lt;br&gt;Correlation weights and interictal epileptic discharges (IED) &lt;br&gt;Activations |Weights increasingly correlated with IED waveforms with increasing number of training iterations &lt;br&gt;Second layer captured more complex and well-defined epileptic shapes than first layer &lt;br&gt;IEDs led to highly synchronized activations for neighbouring electrodes
| Learning Robust Features using Deep Learning for Automatic Seizure Detection, {cite}`thodoroff_learning_2016` |Start of epileptic seizure, within- and cross-subject |Frequency, mean amplitude for 0‚Äì7 Hz, 7‚Äì14 Hz, 14‚Äì49 Hz | 3/1 (+ LSTM as postprocessor) | | | Hand crafted features + SVM | Input occlusion and effect on prediction accuracy |Allowed to locate areas critical for seizure 
|Single-trial EEG RSVP classification using convolutional neural networks, {cite}`george_single-trial_2016` |Oddball response (RSVP), groupwise (ConvNet trained on all subjects) |Time, 0.5‚Äì50 Hz | 4/3 | |  |  |Weights (spatial) |Some filter weights had expected topographic distributions for P300 &lt;br&gt;Others filters had large weights on areas not traditionally associated with P300
|Wearable seizure detection using convolutional neural networks with transfer learning, {cite}`page_wearable_2016` |Seizure detection, cross-subject, within-subject, groupwise |Time,  0‚Äì128 Hz | 1-3/1-3 | | Cross-subject supervised training, within-subject finetuning of fully connected layers |Multiple: spectral features, higher order statistics + linear-SVM, RBF-SVM, ...| | 
|Learning Representations from EEG with Deep Recurrent-Convolutional Neural Networks, {cite}`bashivan_learning_2016`  |Cognitive load (number of characters to memorize), cross-subject | Frequency, mean power for 4‚Äì7 Hz, 8‚Äì13 Hz, 13‚Äì30 Hz | 3‚Äì7/2 (+ LSTM or other temporal post-processing (see design choices)) |Number of convolutional layers &lt;br&gt;Temporal processing of ConvNet output by max pooling, temporal convolution, LSTM or temporal convolution + LSTM | | |Inputs that maximally activate given filter &lt;br&gt;Activations of these inputs &lt;br&gt;&quot;Deconvolution&quot; for these inputs |Different filters were sensitive to different frequency bands &lt;br&gt;Later layers had more spatially localized activations &lt;br&gt;Learned features had noticeable links to well-known electrophysiological markers of cognitive load &lt;br&gt;
|Deep Feature Learning for EEG Recordings, {cite}`stober_learning_2016` |Type of music rhythm, groupwise (ensembles of leave-one-subject-out trained models, evaluated on separate test set of same subjects) |Time, 0.5‚Äì30Hz | 2/1 | Kernel sizes | Pretraining first layer as convolutional autoencoder with different constraints |  | Weights (spatial+3 timesteps, pretrained as autoencoder) | Different constraints led to different weights, one type of constraints could enforce weights that are similar across subjects; other type of constraints led to weights that have similar spatial topographies under different architectural configurations and preprocessings
|Convolutional Neural Network for Multi-Category Rapid Serial Visual Presentation BCI, {cite}`manor_convolutional_2015` |Oddball response (RSVP), within-subject |Time, 0.1‚Äì50 Hz | 3/3 (Spatio-temporal regularization) || ||Weights &lt;br&gt; Mean and single-trial activations |Spatiotemporal regularization led to softer peaks in weights &lt;br&gt;Spatial weights showed typical P300 distribution &lt;br&gt;Activations mostly had peaks at typical times (300-400ms)
|Parallel Convolutional-Linear Neural Network for Motor Imagery Classification, {cite}`sakhavi_parallel_2015`  |Imagined movement classes, within-subject |Frequency, 4‚Äì40 Hz, using FBCSP | 2/2 (Final fully connected layer uses concatenated output by convolutionaland fully connected layers) |Combination ConvNet and MLP (trained on different features) vs. only ConvNet vs. only MLP | | | | 
|Using Convolutional Neural networks to Recognize Rhythm Stimuli form Electroencephalography Recordings, {cite}`stober_using_2014` |Type of music rhythm, within-subject | Time and frequency evaluated, 0-200 Hz | 1-2/1 |Best values from automatic hyperparameter optimization: frequency cutoff, one vs two layers, kernel sizes, number of channels, pooling width |Best values from automatic hyperparameter optimization: learning rate, learning rate decay, momentum, final momentum ||
|Convolutional deep belief networks for feature extraction of EEG signal, {cite}`ren_convolutional_2014`  |Imagined movement classes, within-subject |Frequency, 8‚Äì30 Hz |2/0 (Convolutional deep belief network, separately trained RBF-SVM classifier) | | 
|Deep feature learning using target priors with applications in ECoG signal decoding for BCI, {cite}`wang_deep_2013`  |Finger flexion trajectory (regression), within-subject |Time, 0.15‚Äì200 Hz | 3/1 (Convolutional layers trained as convolutional stacked autoencoder with target prior) |Partially supervised CSA | 
|Convolutional neural networks for P300 detection with application to brain-computer interfaces, {cite}`cecotti_convolutional_2011`  |Oddball / attention response using P300 speller, within-subject | Time, 0.1-20 Hz | 2/2 |Electrode subset (fixed or automatically determined) &lt;br&gt;Using only one spatial filter &lt;br&gt;Different ensembling strategies | |Multiple: Linear SVM, gradient boosting, E-SVM, S-SVM, mLVQ, LDA, ... |Weights |Spatial filters were similar for different architectures &lt;br&gt;Spatial filters were different (more focal, more diffuse) for different subjects
|  |
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span>  <span class="s2">&quot;network, \cite{hajinoroozi_eeg-based_2016} |D&quot;</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;</span><span class="se">\\</span><span class="s2">cite{([^}]+)}&quot;</span><span class="p">,</span> <span class="s2">&quot;</span><span class="si">{cite}</span><span class="s2">`\g&lt;1&gt;`&quot;</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;network, {cite}`hajinoroozi_eeg-based_2016` |D&#39;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">| This manuscript, Schirrmeister et. al (2017) |Imagined and executed movement classes, within subject |Time,  0‚Äì125 Hz | 5/1 |Different ConvNet architectures &lt;br&gt;Nonlinearities and pooling modes &lt;br&gt;Regularization and intermediate normalization layers &lt;br&gt;Factorized convolutions &lt;br&gt;Splitted vs one-step convolutions |Trial-wise vs. cropped training strategy |FBCSP + rLDA | Feature activation correlation &lt;br&gt;Feature-perturbation prediction correlation |See Section </span><span class="se">\r</span><span class="s2">ef{subsec:results-visualization}</span>
<span class="s2">| Single-trial EEG classification of motor imagery using deep convolutional neural networks, citet{tang_single-trial_2017} |Imagined movement classes, within-subject |Time,  8‚Äì30 Hz | 2/2 | | | FBCSP | | </span>
<span class="s2">| EEGNet: A Compact Convolutional Network for EEG-based Brain-Computer Interfaces, </span><span class="si">{cite}</span><span class="s2">`lawhern_eegnet:_2016` | Oddball response (RSVP), error response (ERN), movement classes (voluntarily started and imagined) |Time, 0.1‚Äì40 Hz | 3/1 |  Kernel sizes | |   | |</span>
<span class="s2">| Remembered or Forgotten? ‚Äì- An EEG-Based Computational Prediction Approach, </span><span class="si">{cite}</span><span class="s2">`sun_remembered_2016` | Memory performance, within-subject |Time, 0.05‚Äì15 Hz | 2/2 | | Different time windows |  |Weights (spatial) | Largest weights found over p</span><span class="se">\r</span><span class="s2">efrontal and temporal cortex</span>
<span class="s2">|Multimodal Neural Network for Rapid Serial Visual Presentation Brain Computer Interface, </span><span class="si">{cite}</span><span class="s2">`manor_multimodal_2016`|Oddball response using RSVP and image (combined image-EEG decoding), within-subject|Time, 0.3‚Äì20 Hz | 3/2 | | |  | Weights &lt;br&gt; Activations &lt;br&gt; Saliency maps by gradient |Weights showed typical P300 distribution &lt;br&gt;Activations were high at plausible times (300-500ms) &lt;br&gt;Saliency maps showed plausible spatio-temporal plots</span>
<span class="s2">| A novel deep learning approach for classification of EEG motor imagery signals, </span><span class="si">{cite}</span><span class="s2">`tabar_novel_2017` |Imagined and executed movement classes, within-subject |Frequency, 6‚Äì30 Hz | 1/1 |Addition of six-layer stacked autoencoder on ConvNet features &lt;br&gt; Kernel sizes} |  | FBCSP, Twin SVM, DDFBS, Bi-spectrum, RQNN  | Weights (spatial + frequential) |Some weights represented difference of values of two electrodes on different sides of head</span>
<span class="s2">| Predicting Seizures from Electroencephalography Recordings: A Knowledge Transfer Strategy, </span><span class="si">{cite}</span><span class="s2">`liang_predicting_2016` |Seizure prediction, within-subject | Frequency, 0‚Äì200 Hz | 1/2 | | Different subdivisions of frequency range &lt;br&gt;Different lengths of time crops &lt;br&gt;Transfer learning with auxiliary non-epilepsy datasets || Weights &lt;br&gt; Clustering of weights |Clusters of weights showed typical frequency band subdivision (delta, theta, alpha, beta, gamma)</span>
<span class="s2">| EEG-based prediction of driver&#39;s cognitive performance by deep convolutional neural network, </span><span class="si">{cite}</span><span class="s2">`hajinoroozi_eeg-based_2016` |Driver performance, within- and cross-subject |Time,  1‚Äì50 Hz | 1/3 |Replacement of convolutional layers by restricted Boltzmann machines with slightly varied network architecture}  | | |</span>
<span class="s2">| Deep learning for epileptic intracranial EEG data, </span><span class="si">{cite}</span><span class="s2">`antoniades_deep_2016` |Epileptic discharges, cross-subject | Time,  0‚Äì100 HZ | 1‚Äì2/2 | 1 or 2 convolutional layers |  | |Weights &lt;br&gt;Correlation weights and interictal epileptic discharges (IED) &lt;br&gt;Activations |Weights increasingly correlated with IED waveforms with increasing number of training iterations &lt;br&gt;Second layer captured more complex and well-defined epileptic shapes than first layer &lt;br&gt;IEDs led to highly synchronized activations for neighbouring electrodes</span>
<span class="s2">| Learning Robust Features using Deep Learning for Automatic Seizure Detection, </span><span class="si">{cite}</span><span class="s2">`thodoroff_learning_2016` |Start of epileptic seizure, within- and cross-subject |Frequency, mean amplitude for 0‚Äì7 Hz, 7‚Äì14 Hz, 14‚Äì49 Hz | 3/1 (+ LSTM as postprocessor) | | | Hand crafted features + SVM | Input occlusion and effect on prediction accuracy |Allowed to locate areas critical for seizure </span>
<span class="s2">|Single-trial EEG RSVP classification using convolutional neural networks, </span><span class="si">{cite}</span><span class="s2">`george_single-trial_2016` |Oddball response (RSVP), groupwise (ConvNet trained on all subjects) |Time, 0.5‚Äì50 Hz | 4/3 | |  |  |Weights (spatial) |Some filter weights had expected topographic distributions for P300 &lt;br&gt;Others filters had large weights on areas not traditionally associated with P300</span>
<span class="s2">|Wearable seizure detection using convolutional neural networks with transfer learning, </span><span class="si">{cite}</span><span class="s2">`page_wearable_2016` |Seizure detection, cross-subject, within-subject, groupwise |Time,  0‚Äì128 Hz | 1-3/1-3 | | Cross-subject supervised training, within-subject finetuning of fully connected layers |Multiple: spectral features, higher order statistics + linear-SVM, RBF-SVM, ...| | </span>
<span class="s2">|Learning Representations from EEG with Deep Recurrent-Convolutional Neural Networks, </span><span class="si">{cite}</span><span class="s2">`bashivan_learning_2016`  |Cognitive load (number of characters to memorize), cross-subject | Frequency, mean power for 4‚Äì7 Hz, 8‚Äì13 Hz, 13‚Äì30 Hz | 3‚Äì7/2 (+ LSTM or other temporal post-processing (see design choices)) |Number of convolutional layers &lt;br&gt;Temporal processing of ConvNet output by max pooling, temporal convolution, LSTM or temporal convolution + LSTM | | |Inputs that maximally activate given filter &lt;br&gt;Activations of these inputs &lt;br&gt;&quot;Deconvolution&quot; for these inputs |Different filters were sensitive to different frequency bands &lt;br&gt;Later layers had more spatially localized activations &lt;br&gt;Learned features had noticeable links to well-known electrophysiological markers of cognitive load &lt;br&gt;</span>
<span class="s2">|Deep Feature Learning for EEG Recordings, </span><span class="si">{cite}</span><span class="s2">`stober_learning_2016` |Type of music rhythm, groupwise (ensembles of leave-one-subject-out trained models, evaluated on separate test set of same subjects) |Time, 0.5‚Äì30Hz | 2/1 | Kernel sizes | Pretraining first layer as convolutional autoencoder with different constraints |  | Weights (spatial+3 timesteps, pretrained as autoencoder) | Different constraints led to different weights, one type of constraints could enforce weights that are similar across subjects; other type of constraints led to weights that have similar spatial topographies under different architectural configurations and preprocessings</span>
<span class="s2">|Convolutional Neural Network for Multi-Category Rapid Serial Visual Presentation BCI, </span><span class="si">{cite}</span><span class="s2">`manor_convolutional_2015` |Oddball response (RSVP), within-subject |Time, 0.1‚Äì50 Hz | 3/3 (Spatio-temporal regularization) || ||Weights &lt;br&gt; Mean and single-trial activations |Spatiotemporal regularization led to softer peaks in weights &lt;br&gt;Spatial weights showed typical P300 distribution &lt;br&gt;Activations mostly had peaks at typical times (300-400ms)</span>
<span class="s2">|Parallel Convolutional-Linear Neural Network for Motor Imagery Classification, </span><span class="si">{cite}</span><span class="s2">`sakhavi_parallel_2015`  |Imagined movement classes, within-subject |Frequency, 4‚Äì40 Hz, using FBCSP | 2/2 (Final fully connected layer uses concatenated output by convolutionaland fully connected layers) |Combination ConvNet and MLP (trained on different features) vs. only ConvNet vs. only MLP | | | | </span>
<span class="s2">|Using Convolutional Neural networks to Recognize Rhythm Stimuli form Electroencephalography Recordings, </span><span class="si">{cite}</span><span class="s2">`stober_using_2014` |Type of music rhythm, within-subject | Time and frequency evaluated, 0-200 Hz | 1-2/1 |Best values from automatic hyperparameter optimization: frequency cutoff, one vs two layers, kernel sizes, number of channels, pooling width |Best values from automatic hyperparameter optimization: learning rate, learning rate decay, momentum, final momentum ||</span>
<span class="s2">|Convolutional deep belief networks for feature extraction of EEG signal, </span><span class="si">{cite}</span><span class="s2">`ren_convolutional_2014`  |Imagined movement classes, within-subject |Frequency, 8‚Äì30 Hz |2/0 (Convolutional deep belief network, separately trained RBF-SVM classifier) | | </span>
<span class="s2">|Deep feature learning using target priors with applications in ECoG signal decoding for BCI, </span><span class="si">{cite}</span><span class="s2">`wang_deep_2013`  |Finger flexion trajectory (regression), within-subject |Time, 0.15‚Äì200 Hz | 3/1 (Convolutional layers trained as convolutional stacked autoencoder with target prior) |Partially supervised CSA | </span>
<span class="s2">|Convolutional neural networks for P300 detection with application to brain-computer interfaces, </span><span class="si">{cite}</span><span class="s2">`cecotti_convolutional_2011`  |Oddball / attention response using P300 speller, within-subject | Time, 0.1-20 Hz | 2/2 |Electrode subset (fixed or automatically determined) &lt;br&gt;Using only one spatial filter &lt;br&gt;Different ensembling strategies | |Multiple: Linear SVM, gradient boosting, E-SVM, S-SVM, mLVQ, LDA, ... |Weights |Spatial filters were similar for different architectures &lt;br&gt;Spatial filters were different (more focal, more diffuse) for different subjects</span>
<span class="s2">&quot;&quot;&quot;</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">parts</span> <span class="o">=</span> <span class="p">[</span><span class="n">l</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;|&#39;</span><span class="p">)[</span><span class="mi">1</span><span class="p">:]</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">a</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)[</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parts</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[9, 9, 9, 9, 9, 9, 9, 8, 9, 9, 9, 9, 9, 9, 9, 9, 8, 6, 6, 9]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">headings</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Study&#39;</span><span class="p">,</span> <span class="s1">&#39;Decoding problem&#39;</span><span class="p">,</span> <span class="s1">&#39;Input domain&#39;</span><span class="p">,</span> <span class="s1">&#39;Conv/dense layers&#39;</span><span class="p">,</span> <span class="s1">&#39;Design choices&#39;</span><span class="p">,</span> 
<span class="s1">&#39;Training strategies&#39;</span><span class="p">,</span> <span class="s1">&#39;External baseline&#39;</span><span class="p">,</span> <span class="s1">&#39;Visualization type(s)&#39;</span><span class="p">,</span> <span class="s1">&#39;Visualization findings&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">parts</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span> <span class="o">+</span> <span class="p">[</span><span class="s1">&#39;&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="mi">9</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">p</span><span class="p">))</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parts</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">headings</span><span class="p">,</span> <span class="n">parts</span><span class="p">[</span><span class="mi">3</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(&#39;Study&#39;,
  &#39; Remembered or Forgotten? ‚Äì- An EEG-Based Computational Prediction Approach, {cite}`sun_remembered_2016` &#39;),
 (&#39;Decoding problem&#39;, &#39; Memory performance, within-subject &#39;),
 (&#39;Input domain&#39;, &#39;Time, 0.05‚Äì15 Hz &#39;),
 (&#39;Conv/dense layers&#39;, &#39; 2/2 &#39;),
 (&#39;Design choices&#39;, &#39; &#39;),
 (&#39;Training strategies&#39;, &#39; Different time windows &#39;),
 (&#39;External baseline&#39;, &#39;  &#39;),
 (&#39;Visualization type(s)&#39;, &#39;Weights (spatial) &#39;),
 (&#39;Visualization findings&#39;,
  &#39; Largest weights found over p\refrontal and temporal cortex&#39;)]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parts</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">part_arr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">parts</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">re</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span>  <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">part_arr</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">headings</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="p">[</span><span class="s1">&#39;Study&#39;</span><span class="p">,</span> <span class="s1">&#39;Design choices&#39;</span><span class="p">,</span> <span class="s1">&#39;Training strategies&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">to_markdown</span><span class="p">(</span><span class="n">showindex</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>| Study                                                                                                                              | Design choices                                                                                                                                                                                  | Training strategies                                                                                                                      |
|:-----------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------|
| This manuscript, Schirrmeister et. al (2017)                                                                                       | Different ConvNet architectures &lt;br&gt;Nonlinearities and pooling modes &lt;br&gt;Regularization and intermediate normalization layers &lt;br&gt;Factorized convolutions &lt;br&gt;Splitted vs one-step convolutions | Trial-wise vs. cropped training strategy                                                                                                 |
| Single-trial EEG classification of motor imagery using deep convolutional neural networks, citet{tang_single-trial_2017}           |                                                                                                                                                                                                 |                                                                                                                                          |
| EEGNet: A Compact Convolutional Network for EEG-based Brain-Computer Interfaces, {cite}`lawhern_eegnet:_2016`                      | Kernel sizes                                                                                                                                                                                    |                                                                                                                                          |
| Remembered or Forgotten? ‚Äì- An EEG-Based Computational Prediction Approach, {cite}`sun_remembered_2016`                            |                                                                                                                                                                                                 | Different time windows                                                                                                                   |
| Multimodal Neural Network for Rapid Serial Visual Presentation Brain Computer Interface, {cite}`manor_multimodal_2016`             |                                                                                                                                                                                                 |                                                                                                                                          |
| A novel deep learning approach for classification of EEG motor imagery signals, {cite}`tabar_novel_2017`                           | Addition of six-layer stacked autoencoder on ConvNet features &lt;br&gt; Kernel sizes}                                                                                                                |                                                                                                                                          |
| Predicting Seizures from Electroencephalography Recordings: A Knowledge Transfer Strategy, {cite}`liang_predicting_2016`           |                                                                                                                                                                                                 | Different subdivisions of frequency range &lt;br&gt;Different lengths of time crops &lt;br&gt;Transfer learning with auxiliary non-epilepsy datasets |
| EEG-based prediction of driver&#39;s cognitive performance by deep convolutional neural network, {cite}`hajinoroozi_eeg-based_2016`    | Replacement of convolutional layers by restricted Boltzmann machines with slightly varied network architecture}                                                                                 |                                                                                                                                          |
| Deep learning for epileptic intracranial EEG data, {cite}`antoniades_deep_2016`                                                    | 1 or 2 convolutional layers                                                                                                                                                                     |                                                                                                                                          |
| Learning Robust Features using Deep Learning for Automatic Seizure Detection, {cite}`thodoroff_learning_2016`                      |                                                                                                                                                                                                 |                                                                                                                                          |
| Single-trial EEG RSVP classification using convolutional neural networks, {cite}`george_single-trial_2016`                         |                                                                                                                                                                                                 |                                                                                                                                          |
| Wearable seizure detection using convolutional neural networks with transfer learning, {cite}`page_wearable_2016`                  |                                                                                                                                                                                                 | Cross-subject supervised training, within-subject finetuning of fully connected layers                                                   |
| Learning Representations from EEG with Deep Recurrent-Convolutional Neural Networks, {cite}`bashivan_learning_2016`                | Number of convolutional layers &lt;br&gt;Temporal processing of ConvNet output by max pooling, temporal convolution, LSTM or temporal convolution + LSTM                                              |                                                                                                                                          |
| Deep Feature Learning for EEG Recordings, {cite}`stober_learning_2016`                                                             | Kernel sizes                                                                                                                                                                                    | Pretraining first layer as convolutional autoencoder with different constraints                                                          |
| Convolutional Neural Network for Multi-Category Rapid Serial Visual Presentation BCI, {cite}`manor_convolutional_2015`             |                                                                                                                                                                                                 |                                                                                                                                          |
| Parallel Convolutional-Linear Neural Network for Motor Imagery Classification, {cite}`sakhavi_parallel_2015`                       | Combination ConvNet and MLP (trained on different features) vs. only ConvNet vs. only MLP                                                                                                       |                                                                                                                                          |
| Using Convolutional Neural networks to Recognize Rhythm Stimuli form Electroencephalography Recordings, {cite}`stober_using_2014`  | Best values from automatic hyperparameter optimization: frequency cutoff, one vs two layers, kernel sizes, number of channels, pooling width                                                    | Best values from automatic hyperparameter optimization: learning rate, learning rate decay, momentum, final momentum                     |
| Convolutional deep belief networks for feature extraction of EEG signal, {cite}`ren_convolutional_2014`                            |                                                                                                                                                                                                 |                                                                                                                                          |
| Deep feature learning using target priors with applications in ECoG signal decoding for BCI, {cite}`wang_deep_2013`                | Partially supervised CSA                                                                                                                                                                        |                                                                                                                                          |
| Convolutional neural networks for P300 detection with application to brain-computer interfaces, {cite}`cecotti_convolutional_2011` | Electrode subset (fixed or automatically determined) &lt;br&gt;Using only one spatial filter &lt;br&gt;Different ensembling strategies                                                                      |                                                                                                                                          |
</pre></div>
</div>
</div>
</div>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:left head"><p>Study</p></th>
<th class="text-align:left head"><p>Design choices</p></th>
<th class="text-align:left head"><p>Training strategies</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:left"><p>EEGNet: A Compact Convolutional Network for EEG-based Brain-Computer Interfaces, <span id="id37">[<a class="reference internal" href="References.html#id122">LSW+16</a>]</span></p></td>
<td class="text-align:left"><p>Kernel sizes</p></td>
<td class="text-align:left"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Remembered or Forgotten? ‚Äì- An EEG-Based Computational Prediction Approach, <span id="id38">[<a class="reference internal" href="References.html#id195">SQC+16</a>]</span></p></td>
<td class="text-align:left"><p></p></td>
<td class="text-align:left"><p>Different time windows</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>A novel deep learning approach for classification of EEG motor imagery signals, <span id="id39">[<a class="reference internal" href="References.html#id188">TH17</a>]</span></p></td>
<td class="text-align:left"><p>Addition of six-layer stacked autoencoder on ConvNet features <br> Kernel sizes}</p></td>
<td class="text-align:left"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Predicting Seizures from Electroencephalography Recordings: A Knowledge Transfer Strategy, <span id="id40">[<a class="reference internal" href="References.html#id144">LLZW16</a>]</span></p></td>
<td class="text-align:left"><p></p></td>
<td class="text-align:left"><p>Different subdivisions of frequency range <br>Different lengths of time crops <br>Transfer learning with auxiliary non-epilepsy datasets</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>EEG-based prediction of driver‚Äôs cognitive performance by deep convolutional neural network, <span id="id41">[<a class="reference internal" href="References.html#id152">HMJ+16</a>]</span></p></td>
<td class="text-align:left"><p>Replacement of convolutional layers by restricted Boltzmann machines with slightly varied network architecture}</p></td>
<td class="text-align:left"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Deep learning for epileptic intracranial EEG data, <span id="id42">[<a class="reference internal" href="References.html#id126">ASTS16</a>]</span></p></td>
<td class="text-align:left"><p>1 or 2 convolutional layers</p></td>
<td class="text-align:left"><p></p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Wearable seizure detection using convolutional neural networks with transfer learning, <span id="id43">[<a class="reference internal" href="References.html#id177">PSM16</a>]</span></p></td>
<td class="text-align:left"><p></p></td>
<td class="text-align:left"><p>Cross-subject supervised training, within-subject finetuning of fully connected layers</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Learning Representations from EEG with Deep Recurrent-Convolutional Neural Networks, <span id="id44">[<a class="reference internal" href="References.html#id170">BRYC16</a>]</span></p></td>
<td class="text-align:left"><p>Number of convolutional layers <br>Temporal processing of ConvNet output by max pooling, temporal convolution, LSTM or temporal convolution + LSTM</p></td>
<td class="text-align:left"><p></p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Deep Feature Learning for EEG Recordings, <span id="id45">[<a class="reference internal" href="References.html#id199">Sto16</a>]</span></p></td>
<td class="text-align:left"><p>Kernel sizes</p></td>
<td class="text-align:left"><p>Pretraining first layer as convolutional autoencoder with different constraints</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Parallel Convolutional-Linear Neural Network for Motor Imagery Classification, <span id="id46">[<a class="reference internal" href="References.html#id89">SGY15</a>]</span></p></td>
<td class="text-align:left"><p>Combination ConvNet and MLP (trained on different features) vs. only ConvNet vs. only MLP</p></td>
<td class="text-align:left"><p></p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Using Convolutional Neural networks to Recognize Rhythm Stimuli form Electroencephalography Recordings, <span id="id47">[<a class="reference internal" href="References.html#id156">SCG14</a>]</span></p></td>
<td class="text-align:left"><p>Best values from automatic hyperparameter optimization: frequency cutoff, one vs two layers, kernel sizes, number of channels, pooling width</p></td>
<td class="text-align:left"><p>Best values from automatic hyperparameter optimization: learning rate, learning rate decay, momentum, final momentum</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Deep feature learning using target priors with applications in ECoG signal decoding for BCI, <span id="id48">[<a class="reference internal" href="References.html#id182">WLSJ13</a>]</span></p></td>
<td class="text-align:left"><p>Partially supervised CSA</p></td>
<td class="text-align:left"><p></p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Convolutional neural networks for P300 detection with application to brain-computer interfaces, <span id="id49">[<a class="reference internal" href="References.html#id185">CG11</a>]</span></p></td>
<td class="text-align:left"><p>Electrode subset (fixed or automatically determined) <br>Using only one spatial filter <br>Different ensembling strategies</p></td>
<td class="text-align:left"><p></p></td>
</tr>
</tbody>
</table>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Study</th>
      <th>Decoding problem</th>
      <th>Input domain</th>
      <th>Conv/dense layers</th>
      <th>Design choices</th>
      <th>Training strategies</th>
      <th>External baseline</th>
      <th>Visualization type(s)</th>
      <th>Visualization findings</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>This manuscript, Schirrmeister et. al (2017)</td>
      <td>Imagined and executed movement classes, within...</td>
      <td>Time,  0‚Äì125 Hz</td>
      <td>5/1</td>
      <td>Different ConvNet architectures &lt;br&gt;Nonlineari...</td>
      <td>Trial-wise vs. cropped training strategy</td>
      <td>FBCSP + rLDA</td>
      <td>Feature activation correlation &lt;br&gt;Feature-pe...</td>
      <td></td>
    </tr>
    <tr>
      <th>1</th>
      <td>Single-trial EEG classification of motor imag...</td>
      <td>Imagined movement classes, within-subject</td>
      <td>Time,  8‚Äì30 Hz</td>
      <td>2/2</td>
      <td></td>
      <td></td>
      <td>FBCSP</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <th>2</th>
      <td>EEGNet: A Compact Convolutional Network for E...</td>
      <td>Oddball response (RSVP), error response (ERN)...</td>
      <td>Time, 0.1‚Äì40 Hz</td>
      <td>3/1</td>
      <td>Kernel sizes</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <th>3</th>
      <td>Remembered or Forgotten? ‚Äì- An EEG-Based Comp...</td>
      <td>Memory performance, within-subject</td>
      <td>Time, 0.05‚Äì15 Hz</td>
      <td>2/2</td>
      <td></td>
      <td>Different time windows</td>
      <td></td>
      <td>Weights (spatial)</td>
      <td></td>
    </tr>
    <tr>
      <th>4</th>
      <td>Multimodal Neural Network for Rapid Serial Vis...</td>
      <td>Oddball response using RSVP and image (combine...</td>
      <td>Time, 0.3‚Äì20 Hz</td>
      <td>3/2</td>
      <td></td>
      <td></td>
      <td></td>
      <td>Weights &lt;br&gt; Activations &lt;br&gt; Saliency maps b...</td>
      <td></td>
    </tr>
    <tr>
      <th>5</th>
      <td>A novel deep learning approach for classifica...</td>
      <td>Imagined and executed movement classes, within...</td>
      <td>Frequency, 6‚Äì30 Hz</td>
      <td>1/1</td>
      <td>Addition of six-layer stacked autoencoder on C...</td>
      <td></td>
      <td>FBCSP, Twin SVM, DDFBS, Bi-spectrum, RQNN</td>
      <td>Weights (spatial + frequential)</td>
      <td></td>
    </tr>
    <tr>
      <th>6</th>
      <td>Predicting Seizures from Electroencephalograp...</td>
      <td>Seizure prediction, within-subject</td>
      <td>Frequency, 0‚Äì200 Hz</td>
      <td>1/2</td>
      <td></td>
      <td>Different subdivisions of frequency range &lt;br...</td>
      <td></td>
      <td>Weights &lt;br&gt; Clustering of weights</td>
      <td></td>
    </tr>
    <tr>
      <th>7</th>
      <td>EEG-based prediction of driver's cognitive pe...</td>
      <td>Driver performance, within- and cross-subject</td>
      <td>Time,  1‚Äì50 Hz</td>
      <td>1/3</td>
      <td>Replacement of convolutional layers by restric...</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <th>8</th>
      <td>Deep learning for epileptic intracranial EEG ...</td>
      <td>Epileptic discharges, cross-subject</td>
      <td>Time,  0‚Äì100 HZ</td>
      <td>1‚Äì2/2</td>
      <td>1 or 2 convolutional layers</td>
      <td></td>
      <td></td>
      <td>Weights &lt;br&gt;Correlation weights and interictal...</td>
      <td></td>
    </tr>
    <tr>
      <th>9</th>
      <td>Learning Robust Features using Deep Learning ...</td>
      <td>Start of epileptic seizure, within- and cross-...</td>
      <td>Frequency, mean amplitude for 0‚Äì7 Hz, 7‚Äì14 Hz,...</td>
      <td>3/1 (+ LSTM as postprocessor)</td>
      <td></td>
      <td></td>
      <td>Hand crafted features + SVM</td>
      <td>Input occlusion and effect on prediction accu...</td>
      <td></td>
    </tr>
    <tr>
      <th>10</th>
      <td>Single-trial EEG RSVP classification using con...</td>
      <td>Oddball response (RSVP), groupwise (ConvNet tr...</td>
      <td>Time, 0.5‚Äì50 Hz</td>
      <td>4/3</td>
      <td></td>
      <td></td>
      <td></td>
      <td>Weights (spatial)</td>
      <td></td>
    </tr>
    <tr>
      <th>11</th>
      <td>Wearable seizure detection using convolutional...</td>
      <td>Seizure detection, cross-subject, within-subje...</td>
      <td>Time,  0‚Äì128 Hz</td>
      <td>1-3/1-3</td>
      <td></td>
      <td>Cross-subject supervised training, within-sub...</td>
      <td>Multiple: spectral features, higher order stat...</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <th>12</th>
      <td>Learning Representations from EEG with Deep Re...</td>
      <td>Cognitive load (number of characters to memori...</td>
      <td>Frequency, mean power for 4‚Äì7 Hz, 8‚Äì13 Hz, 13...</td>
      <td>3‚Äì7/2 (+ LSTM or other temporal post-processi...</td>
      <td>Number of convolutional layers &lt;br&gt;Temporal pr...</td>
      <td></td>
      <td></td>
      <td>Inputs that maximally activate given filter &lt;b...</td>
      <td></td>
    </tr>
    <tr>
      <th>13</th>
      <td>Deep Feature Learning for EEG Recordings, {cit...</td>
      <td>Type of music rhythm, groupwise (ensembles of ...</td>
      <td>Time, 0.5‚Äì30Hz</td>
      <td>2/1</td>
      <td>Kernel sizes</td>
      <td>Pretraining first layer as convolutional auto...</td>
      <td></td>
      <td>Weights (spatial+3 timesteps, pretrained as a...</td>
      <td></td>
    </tr>
    <tr>
      <th>14</th>
      <td>Convolutional Neural Network for Multi-Categor...</td>
      <td>Oddball response (RSVP), within-subject</td>
      <td>Time, 0.1‚Äì50 Hz</td>
      <td>3/3 (Spatio-temporal regularization)</td>
      <td></td>
      <td></td>
      <td></td>
      <td>Weights &lt;br&gt; Mean and single-trial activations</td>
      <td></td>
    </tr>
    <tr>
      <th>15</th>
      <td>Parallel Convolutional-Linear Neural Network f...</td>
      <td>Imagined movement classes, within-subject</td>
      <td>Frequency, 4‚Äì40 Hz, using FBCSP</td>
      <td>2/2 (Final fully connected layer uses concate...</td>
      <td>Combination ConvNet and MLP (trained on differ...</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <th>16</th>
      <td>Using Convolutional Neural networks to Recogni...</td>
      <td>Type of music rhythm, within-subject</td>
      <td>Time and frequency evaluated, 0-200 Hz</td>
      <td>1-2/1</td>
      <td>Best values from automatic hyperparameter opti...</td>
      <td>Best values from automatic hyperparameter opti...</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <th>17</th>
      <td>Convolutional deep belief networks for feature...</td>
      <td>Imagined movement classes, within-subject</td>
      <td>Frequency, 8‚Äì30 Hz</td>
      <td>2/0 (Convolutional deep belief network, separa...</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <th>18</th>
      <td>Deep feature learning using target priors with...</td>
      <td>Finger flexion trajectory (regression), within...</td>
      <td>Time, 0.15‚Äì200 Hz</td>
      <td>3/1 (Convolutional layers trained as convolut...</td>
      <td>Partially supervised CSA</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <th>19</th>
      <td>Convolutional neural networks for P300 detecti...</td>
      <td>Oddball / attention response using P300 spelle...</td>
      <td>Time, 0.1-20 Hz</td>
      <td>2/2</td>
      <td>Electrode subset (fixed or automatically deter...</td>
      <td></td>
      <td>Multiple: Linear SVM, gradient boosting, E-SVM...</td>
      <td>Weights</td>
      <td></td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="mi">1</span><span class="p">:,</span> <span class="p">[</span><span class="s1">&#39;Study&#39;</span><span class="p">,</span> <span class="s1">&#39;Visualization type(s)&#39;</span><span class="p">,</span> <span class="s1">&#39;Visualization findings&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">to_markdown</span><span class="p">(</span><span class="n">showindex</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>| Study                                                                                                                              | Visualization type(s)                                                                                            | Visualization findings                                                                                                                                                                                                                                                          |
|:-----------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Single-trial EEG classification of motor imagery using deep convolutional neural networks, citet{tang_single-trial_2017}           |                                                                                                                  |                                                                                                                                                                                                                                                                                 |
| EEGNet: A Compact Convolutional Network for EEG-based Brain-Computer Interfaces, {cite}`lawhern_eegnet:_2016`                      |                                                                                                                  |                                                                                                                                                                                                                                                                                 |
| Remembered or Forgotten? ‚Äì- An EEG-Based Computational Prediction Approach, {cite}`sun_remembered_2016`                            | Weights (spatial)                                                                                                | Largest weights found over p                                                                                                                                                                                                                                                    |
|                                                                                                                                    |                                                                                                                  | efrontal and temporal cortex                                                                                                                                                                                                                                                    |
| Multimodal Neural Network for Rapid Serial Visual Presentation Brain Computer Interface, {cite}`manor_multimodal_2016`             | Weights &lt;br&gt; Activations &lt;br&gt; Saliency maps by gradient                                                          | Weights showed typical P300 distribution &lt;br&gt;Activations were high at plausible times (300-500ms) &lt;br&gt;Saliency maps showed plausible spatio-temporal plots                                                                                                                      |
| A novel deep learning approach for classification of EEG motor imagery signals, {cite}`tabar_novel_2017`                           | Weights (spatial + frequential)                                                                                  | Some weights represented difference of values of two electrodes on different sides of head                                                                                                                                                                                      |
| Predicting Seizures from Electroencephalography Recordings: A Knowledge Transfer Strategy, {cite}`liang_predicting_2016`           | Weights &lt;br&gt; Clustering of weights                                                                               | Clusters of weights showed typical frequency band subdivision (delta, theta, alpha, beta, gamma)                                                                                                                                                                                |
| EEG-based prediction of driver&#39;s cognitive performance by deep convolutional neural network, {cite}`hajinoroozi_eeg-based_2016`    |                                                                                                                  |                                                                                                                                                                                                                                                                                 |
| Deep learning for epileptic intracranial EEG data, {cite}`antoniades_deep_2016`                                                    | Weights &lt;br&gt;Correlation weights and interictal epileptic discharges (IED) &lt;br&gt;Activations                        | Weights increasingly correlated with IED waveforms with increasing number of training iterations &lt;br&gt;Second layer captured more complex and well-defined epileptic shapes than first layer &lt;br&gt;IEDs led to highly synchronized activations for neighbouring electrodes          |
| Learning Robust Features using Deep Learning for Automatic Seizure Detection, {cite}`thodoroff_learning_2016`                      | Input occlusion and effect on prediction accuracy                                                                | Allowed to locate areas critical for seizure                                                                                                                                                                                                                                    |
| Single-trial EEG RSVP classification using convolutional neural networks, {cite}`george_single-trial_2016`                         | Weights (spatial)                                                                                                | Some filter weights had expected topographic distributions for P300 &lt;br&gt;Others filters had large weights on areas not traditionally associated with P300                                                                                                                        |
| Wearable seizure detection using convolutional neural networks with transfer learning, {cite}`page_wearable_2016`                  |                                                                                                                  |                                                                                                                                                                                                                                                                                 |
| Learning Representations from EEG with Deep Recurrent-Convolutional Neural Networks, {cite}`bashivan_learning_2016`                | Inputs that maximally activate given filter &lt;br&gt;Activations of these inputs &lt;br&gt;&quot;Deconvolution&quot; for these inputs | Different filters were sensitive to different frequency bands &lt;br&gt;Later layers had more spatially localized activations &lt;br&gt;Learned features had noticeable links to well-known electrophysiological markers of cognitive load &lt;br&gt;                                             |
| Deep Feature Learning for EEG Recordings, {cite}`stober_learning_2016`                                                             | Weights (spatial+3 timesteps, pretrained as autoencoder)                                                         | Different constraints led to different weights, one type of constraints could enforce weights that are similar across subjects; other type of constraints led to weights that have similar spatial topographies under different architectural configurations and preprocessings |
| Convolutional Neural Network for Multi-Category Rapid Serial Visual Presentation BCI, {cite}`manor_convolutional_2015`             | Weights &lt;br&gt; Mean and single-trial activations                                                                   | Spatiotemporal regularization led to softer peaks in weights &lt;br&gt;Spatial weights showed typical P300 distribution &lt;br&gt;Activations mostly had peaks at typical times (300-400ms)                                                                                                 |
| Parallel Convolutional-Linear Neural Network for Motor Imagery Classification, {cite}`sakhavi_parallel_2015`                       |                                                                                                                  |                                                                                                                                                                                                                                                                                 |
| Using Convolutional Neural networks to Recognize Rhythm Stimuli form Electroencephalography Recordings, {cite}`stober_using_2014`  |                                                                                                                  |                                                                                                                                                                                                                                                                                 |
| Convolutional deep belief networks for feature extraction of EEG signal, {cite}`ren_convolutional_2014`                            |                                                                                                                  |                                                                                                                                                                                                                                                                                 |
| Deep feature learning using target priors with applications in ECoG signal decoding for BCI, {cite}`wang_deep_2013`                |                                                                                                                  |                                                                                                                                                                                                                                                                                 |
| Convolutional neural networks for P300 detection with application to brain-computer interfaces, {cite}`cecotti_convolutional_2011` | Weights                                                                                                          | Spatial filters were similar for different architectures &lt;br&gt;Spatial filters were different (more focal, more diffuse) for different subjects                                                                                                                                   |
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">b</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;| Study                                                                                                                              | Visualization type(s)                                                                                            | Visualization findings                                                                                                                                                                                                                                                          |</span>
<span class="s2">|:-----------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|</span>
<span class="s2">| Single-trial EEG classification of motor imagery using deep convolutional neural networks, citet{tang_single-trial_2017}           |                                                                                                                  |                                                                                                                                                                                                                                                                                 |</span>
<span class="s2">| EEGNet: A Compact Convolutional Network for EEG-based Brain-Computer Interfaces, </span><span class="si">{cite}</span><span class="s2">`lawhern_eegnet:_2016`                      |                                                                                                                  |                                                                                                                                                                                                                                                                                 |</span>
<span class="s2">| Remembered or Forgotten? ‚Äì- An EEG-Based Computational Prediction Approach, </span><span class="si">{cite}</span><span class="s2">`sun_remembered_2016`                            | Weights (spatial)                                                                                                | Largest weights found over prefrontal and temporal cortex                                                                                                                                                                                                                                                                    |</span>
<span class="s2">| Multimodal Neural Network for Rapid Serial Visual Presentation Brain Computer Interface, </span><span class="si">{cite}</span><span class="s2">`manor_multimodal_2016`             | Weights &lt;br&gt; Activations &lt;br&gt; Saliency maps by gradient                                                          | Weights showed typical P300 distribution &lt;br&gt;Activations were high at plausible times (300-500ms) &lt;br&gt;Saliency maps showed plausible spatio-temporal plots                                                                                                                      |</span>
<span class="s2">| A novel deep learning approach for classification of EEG motor imagery signals, </span><span class="si">{cite}</span><span class="s2">`tabar_novel_2017`                           | Weights (spatial + frequential)                                                                                  | Some weights represented difference of values of two electrodes on different sides of head                                                                                                                                                                                      |</span>
<span class="s2">| Predicting Seizures from Electroencephalography Recordings: A Knowledge Transfer Strategy, </span><span class="si">{cite}</span><span class="s2">`liang_predicting_2016`           | Weights &lt;br&gt; Clustering of weights                                                                               | Clusters of weights showed typical frequency band subdivision (delta, theta, alpha, beta, gamma)                                                                                                                                                                                |</span>
<span class="s2">| EEG-based prediction of driver&#39;s cognitive performance by deep convolutional neural network, </span><span class="si">{cite}</span><span class="s2">`hajinoroozi_eeg-based_2016`    |                                                                                                                  |                                                                                                                                                                                                                                                                                 |</span>
<span class="s2">| Deep learning for epileptic intracranial EEG data, </span><span class="si">{cite}</span><span class="s2">`antoniades_deep_2016`                                                    | Weights &lt;br&gt;Correlation weights and interictal epileptic discharges (IED) &lt;br&gt;Activations                        | Weights increasingly correlated with IED waveforms with increasing number of training iterations &lt;br&gt;Second layer captured more complex and well-defined epileptic shapes than first layer &lt;br&gt;IEDs led to highly synchronized activations for neighbouring electrodes          |</span>
<span class="s2">| Learning Robust Features using Deep Learning for Automatic Seizure Detection, </span><span class="si">{cite}</span><span class="s2">`thodoroff_learning_2016`                      | Input occlusion and effect on prediction accuracy                                                                | Allowed to locate areas critical for seizure                                                                                                                                                                                                                                    |</span>
<span class="s2">| Single-trial EEG RSVP classification using convolutional neural networks, </span><span class="si">{cite}</span><span class="s2">`george_single-trial_2016`                         | Weights (spatial)                                                                                                | Some filter weights had expected topographic distributions for P300 &lt;br&gt;Others filters had large weights on areas not traditionally associated with P300                                                                                                                        |</span>
<span class="s2">| Wearable seizure detection using convolutional neural networks with transfer learning, </span><span class="si">{cite}</span><span class="s2">`page_wearable_2016`                  |                                                                                                                  |                                                                                                                                                                                                                                                                                 |</span>
<span class="s2">| Learning Representations from EEG with Deep Recurrent-Convolutional Neural Networks, </span><span class="si">{cite}</span><span class="s2">`bashivan_learning_2016`                | Inputs that maximally activate given filter &lt;br&gt;Activations of these inputs &lt;br&gt;&quot;Deconvolution&quot; for these inputs | Different filters were sensitive to different frequency bands &lt;br&gt;Later layers had more spatially localized activations &lt;br&gt;Learned features had noticeable links to well-known electrophysiological markers of cognitive load &lt;br&gt;                                             |</span>
<span class="s2">| Deep Feature Learning for EEG Recordings, </span><span class="si">{cite}</span><span class="s2">`stober_learning_2016`                                                             | Weights (spatial+3 timesteps, pretrained as autoencoder)                                                         | Different constraints led to different weights, one type of constraints could enforce weights that are similar across subjects; other type of constraints led to weights that have similar spatial topographies under different architectural configurations and preprocessings |</span>
<span class="s2">| Convolutional Neural Network for Multi-Category Rapid Serial Visual Presentation BCI, </span><span class="si">{cite}</span><span class="s2">`manor_convolutional_2015`             | Weights &lt;br&gt; Mean and single-trial activations                                                                   | Spatiotemporal regularization led to softer peaks in weights &lt;br&gt;Spatial weights showed typical P300 distribution &lt;br&gt;Activations mostly had peaks at typical times (300-400ms)                                                                                                 |</span>
<span class="s2">| Parallel Convolutional-Linear Neural Network for Motor Imagery Classification, </span><span class="si">{cite}</span><span class="s2">`sakhavi_parallel_2015`                       |                                                                                                                  |                                                                                                                                                                                                                                                                                 |</span>
<span class="s2">| Using Convolutional Neural networks to Recognize Rhythm Stimuli form Electroencephalography Recordings, </span><span class="si">{cite}</span><span class="s2">`stober_using_2014`  |                                                                                                                  |                                                                                                                                                                                                                                                                                 |</span>
<span class="s2">| Convolutional deep belief networks for feature extraction of EEG signal, </span><span class="si">{cite}</span><span class="s2">`ren_convolutional_2014`                            |                                                                                                                  |                                                                                                                                                                                                                                                                                 |</span>
<span class="s2">| Deep feature learning using target priors with applications in ECoG signal decoding for BCI, </span><span class="si">{cite}</span><span class="s2">`wang_deep_2013`                |                                                                                                                  |                                                                                                                                                                                                                                                                                 |</span>
<span class="s2">| Convolutional neural networks for P300 detection with application to brain-computer interfaces, </span><span class="si">{cite}</span><span class="s2">`cecotti_convolutional_2011` | Weights                                                                                                          | Spatial filters were similar for different architectures &lt;br&gt;Spatial filters were different (more focal, more diffuse) for different subjects                                                                                                                                   |&quot;&quot;&quot;</span>


<span class="n">c</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)[</span><span class="mi">2</span><span class="p">:]</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="s1">&#39;|&#39;</span> <span class="o">+</span> <span class="n">l</span><span class="p">[</span><span class="n">l</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="s1">&#39;cite&#39;</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:]</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">c</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>| citet{tang_single-trial_2017}           |                                                                                                                  |                                                                                                                                                                                                                                                                                 |
|{cite}`lawhern_eegnet:_2016`                      |                                                                                                                  |                                                                                                                                                                                                                                                                                 |
|{cite}`sun_remembered_2016`                            | Weights (spatial)                                                                                                | Largest weights found over prefrontal and temporal cortex                                                                                                                                                                                                                                                                    |
|{cite}`manor_multimodal_2016`             | Weights &lt;br&gt; Activations &lt;br&gt; Saliency maps by gradient                                                          | Weights showed typical P300 distribution &lt;br&gt;Activations were high at plausible times (300-500ms) &lt;br&gt;Saliency maps showed plausible spatio-temporal plots                                                                                                                      |
|{cite}`tabar_novel_2017`                           | Weights (spatial + frequential)                                                                                  | Some weights represented difference of values of two electrodes on different sides of head                                                                                                                                                                                      |
|{cite}`liang_predicting_2016`           | Weights &lt;br&gt; Clustering of weights                                                                               | Clusters of weights showed typical frequency band subdivision (delta, theta, alpha, beta, gamma)                                                                                                                                                                                |
|{cite}`hajinoroozi_eeg-based_2016`    |                                                                                                                  |                                                                                                                                                                                                                                                                                 |
|{cite}`antoniades_deep_2016`                                                    | Weights &lt;br&gt;Correlation weights and interictal epileptic discharges (IED) &lt;br&gt;Activations                        | Weights increasingly correlated with IED waveforms with increasing number of training iterations &lt;br&gt;Second layer captured more complex and well-defined epileptic shapes than first layer &lt;br&gt;IEDs led to highly synchronized activations for neighbouring electrodes          |
|{cite}`thodoroff_learning_2016`                      | Input occlusion and effect on prediction accuracy                                                                | Allowed to locate areas critical for seizure                                                                                                                                                                                                                                    |
|{cite}`george_single-trial_2016`                         | Weights (spatial)                                                                                                | Some filter weights had expected topographic distributions for P300 &lt;br&gt;Others filters had large weights on areas not traditionally associated with P300                                                                                                                        |
|{cite}`page_wearable_2016`                  |                                                                                                                  |                                                                                                                                                                                                                                                                                 |
|{cite}`bashivan_learning_2016`                | Inputs that maximally activate given filter &lt;br&gt;Activations of these inputs &lt;br&gt;&quot;Deconvolution&quot; for these inputs | Different filters were sensitive to different frequency bands &lt;br&gt;Later layers had more spatially localized activations &lt;br&gt;Learned features had noticeable links to well-known electrophysiological markers of cognitive load &lt;br&gt;                                             |
|{cite}`stober_learning_2016`                                                             | Weights (spatial+3 timesteps, pretrained as autoencoder)                                                         | Different constraints led to different weights, one type of constraints could enforce weights that are similar across subjects; other type of constraints led to weights that have similar spatial topographies under different architectural configurations and preprocessings |
|{cite}`manor_convolutional_2015`             | Weights &lt;br&gt; Mean and single-trial activations                                                                   | Spatiotemporal regularization led to softer peaks in weights &lt;br&gt;Spatial weights showed typical P300 distribution &lt;br&gt;Activations mostly had peaks at typical times (300-400ms)                                                                                                 |
|{cite}`sakhavi_parallel_2015`                       |                                                                                                                  |                                                                                                                                                                                                                                                                                 |
|{cite}`stober_using_2014`  |                                                                                                                  |                                                                                                                                                                                                                                                                                 |
|{cite}`ren_convolutional_2014`                            |                                                                                                                  |                                                                                                                                                                                                                                                                                 |
|{cite}`wang_deep_2013`                |                                                                                                                  |                                                                                                                                                                                                                                                                                 |
|{cite}`cecotti_convolutional_2011` | Weights                                                                                                          | Spatial filters were similar for different architectures &lt;br&gt;Spatial filters were different (more focal, more diffuse) for different subjects                                                                                                                                   |
</pre></div>
</div>
</div>
</div>
<div class="section" id="visulization-table-with-empty-papers">
<h2>visulization table with empty papers<a class="headerlink" href="#visulization-table-with-empty-papers" title="Permalink to this headline">¬∂</a></h2>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:left head"><p>Study</p></th>
<th class="text-align:left head"><p>Visualization type(s)</p></th>
<th class="text-align:left head"><p>Visualization findings</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:left"><p><span id="id50">[<a class="reference internal" href="References.html#id186">TLS17</a>]</span></p></td>
<td class="text-align:left"><p></p></td>
<td class="text-align:left"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p><span id="id51">[<a class="reference internal" href="References.html#id122">LSW+16</a>]</span></p></td>
<td class="text-align:left"><p></p></td>
<td class="text-align:left"><p></p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p><span id="id52">[<a class="reference internal" href="References.html#id195">SQC+16</a>]</span></p></td>
<td class="text-align:left"><p>Weights (spatial)</p></td>
<td class="text-align:left"><p>Largest weights found over prefrontal and temporal cortex</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p><span id="id53">[<a class="reference internal" href="References.html#id169">MMG16</a>]</span></p></td>
<td class="text-align:left"><p>Weights <br> Activations <br> Saliency maps by gradient</p></td>
<td class="text-align:left"><p>Weights showed typical P300 distribution <br>Activations were high at plausible times (300-500ms) <br>Saliency maps showed plausible spatio-temporal plots</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p><span id="id54">[<a class="reference internal" href="References.html#id188">TH17</a>]</span></p></td>
<td class="text-align:left"><p>Weights (spatial + frequential)</p></td>
<td class="text-align:left"><p>Some weights represented difference of values of two electrodes on different sides of head</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p><span id="id55">[<a class="reference internal" href="References.html#id144">LLZW16</a>]</span></p></td>
<td class="text-align:left"><p>Weights <br> Clustering of weights</p></td>
<td class="text-align:left"><p>Clusters of weights showed typical frequency band subdivision (delta, theta, alpha, beta, gamma)</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p><span id="id56">[<a class="reference internal" href="References.html#id152">HMJ+16</a>]</span></p></td>
<td class="text-align:left"><p></p></td>
<td class="text-align:left"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p><span id="id57">[<a class="reference internal" href="References.html#id126">ASTS16</a>]</span></p></td>
<td class="text-align:left"><p>Weights <br>Correlation weights and interictal epileptic discharges (IED) <br>Activations</p></td>
<td class="text-align:left"><p>Weights increasingly correlated with IED waveforms with increasing number of training iterations <br>Second layer captured more complex and well-defined epileptic shapes than first layer <br>IEDs led to highly synchronized activations for neighbouring electrodes</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p><span id="id58">[<a class="reference internal" href="References.html#id154">TPL16</a>]</span></p></td>
<td class="text-align:left"><p>Input occlusion and effect on prediction accuracy</p></td>
<td class="text-align:left"><p>Allowed to locate areas critical for seizure</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p><span id="id59">[<a class="reference internal" href="References.html#id173">SLK+16</a>]</span></p></td>
<td class="text-align:left"><p>Weights (spatial)</p></td>
<td class="text-align:left"><p>Some filter weights had expected topographic distributions for P300 <br>Others filters had large weights on areas not traditionally associated with P300</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p><span id="id60">[<a class="reference internal" href="References.html#id177">PSM16</a>]</span></p></td>
<td class="text-align:left"><p></p></td>
<td class="text-align:left"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p><span id="id61">[<a class="reference internal" href="References.html#id170">BRYC16</a>]</span></p></td>
<td class="text-align:left"><p>Inputs that maximally activate given filter <br>Activations of these inputs <br>‚ÄùDeconvolution‚Äù for these inputs</p></td>
<td class="text-align:left"><p>Different filters were sensitive to different frequency bands <br>Later layers had more spatially localized activations <br>Learned features had noticeable links to well-known electrophysiological markers of cognitive load <br></p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p><span id="id62">[<a class="reference internal" href="References.html#id199">Sto16</a>]</span></p></td>
<td class="text-align:left"><p>Weights (spatial+3 timesteps, pretrained as autoencoder)</p></td>
<td class="text-align:left"><p>Different constraints led to different weights, one type of constraints could enforce weights that are similar across subjects; other type of constraints led to weights that have similar spatial topographies under different architectural configurations and preprocessings</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p><span id="id63">[<a class="reference internal" href="References.html#id119">MG15</a>]</span></p></td>
<td class="text-align:left"><p>Weights <br> Mean and single-trial activations</p></td>
<td class="text-align:left"><p>Spatiotemporal regularization led to softer peaks in weights <br>Spatial weights showed typical P300 distribution <br>Activations mostly had peaks at typical times (300-400ms)</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p><span id="id64">[<a class="reference internal" href="References.html#id89">SGY15</a>]</span></p></td>
<td class="text-align:left"><p></p></td>
<td class="text-align:left"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p><span id="id65">[<a class="reference internal" href="References.html#id156">SCG14</a>]</span></p></td>
<td class="text-align:left"><p></p></td>
<td class="text-align:left"><p></p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p><span id="id66">[<a class="reference internal" href="References.html#id109">RW14</a>]</span></p></td>
<td class="text-align:left"><p></p></td>
<td class="text-align:left"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p><span id="id67">[<a class="reference internal" href="References.html#id182">WLSJ13</a>]</span></p></td>
<td class="text-align:left"><p></p></td>
<td class="text-align:left"><p></p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p><span id="id68">[<a class="reference internal" href="References.html#id185">CG11</a>]</span></p></td>
<td class="text-align:left"><p>Weights</p></td>
<td class="text-align:left"><p>Spatial filters were similar for different architectures <br>Spatial filters were different (more focal, more diffuse) for different subjects</p></td>
</tr>
</tbody>
</table>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span><span class="s1">&#39;Input domain&#39;</span><span class="p">]</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">a</span><span class="p">)[</span><span class="mi">1</span><span class="p">:]</span> <span class="c1"># exclude our own study</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="s1">&#39;Time,  8‚Äì30 Hz &#39;</span><span class="p">,</span> <span class="s1">&#39;Time, 0.1‚Äì40 Hz &#39;</span><span class="p">,</span> <span class="s1">&#39;Time, 0.05‚Äì15 Hz &#39;</span><span class="p">,</span>
       <span class="s1">&#39;Time, 0.3‚Äì20 Hz &#39;</span><span class="p">,</span> <span class="s1">&#39;Frequency, 6‚Äì30 Hz &#39;</span><span class="p">,</span> <span class="s1">&#39; Frequency, 0‚Äì200 Hz &#39;</span><span class="p">,</span>
       <span class="s1">&#39;Time,  1‚Äì50 Hz &#39;</span><span class="p">,</span> <span class="s1">&#39; Time,  0‚Äì100 HZ &#39;</span><span class="p">,</span>
       <span class="s1">&#39;Frequency, mean amplitude for 0‚Äì7 Hz, 7‚Äì14 Hz, 14‚Äì49 Hz &#39;</span><span class="p">,</span>
       <span class="s1">&#39;Time, 0.5‚Äì50 Hz &#39;</span><span class="p">,</span> <span class="s1">&#39;Time,  0‚Äì128 Hz &#39;</span><span class="p">,</span>
       <span class="s1">&#39; Frequency, mean power for 4‚Äì7 Hz, 8‚Äì13 Hz, 13‚Äì30 Hz &#39;</span><span class="p">,</span>
       <span class="s1">&#39;Time, 0.5‚Äì30Hz &#39;</span><span class="p">,</span> <span class="s1">&#39;Time, 0.1‚Äì50 Hz &#39;</span><span class="p">,</span>
       <span class="s1">&#39;Frequency, 4‚Äì40 Hz, using FBCSP &#39;</span><span class="p">,</span>
       <span class="s1">&#39; Time and frequency evaluated, 0-200 Hz &#39;</span><span class="p">,</span> <span class="s1">&#39;Frequency, 8‚Äì30 Hz &#39;</span><span class="p">,</span>
       <span class="s1">&#39;Time, 0.15‚Äì200 Hz &#39;</span><span class="p">,</span> <span class="s1">&#39; Time, 0.1-20 Hz &#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">re</span>
<span class="n">domain_strings</span> <span class="o">=</span> <span class="p">[</span><span class="n">s</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;,&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">a</span><span class="p">]</span>
<span class="n">start_fs</span> <span class="o">=</span> <span class="p">[</span><span class="nb">float</span><span class="p">(</span><span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;[a-z ]+&#39;</span><span class="p">,</span><span class="sa">r</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">re</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;[‚Äì-‚Äì-]&#39;</span><span class="p">,</span><span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;,&#39;</span><span class="p">)[</span><span class="mi">1</span><span class="p">:]))[</span><span class="mi">0</span><span class="p">]))</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">a</span><span class="p">]</span>
<span class="n">end_fs</span> <span class="o">=</span> <span class="p">[</span><span class="nb">float</span><span class="p">(</span><span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;[a-z HZFBCSP]+&#39;</span><span class="p">,</span><span class="sa">r</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">re</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;[‚Äì-‚Äì-]&#39;</span><span class="p">,</span><span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;,&#39;</span><span class="p">)[</span><span class="mi">1</span><span class="p">:]))[</span><span class="mi">1</span><span class="p">]))</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">a</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">cm</span>
<span class="kn">import</span> <span class="nn">seaborn</span>
<span class="n">seaborn</span><span class="o">.</span><span class="n">set_palette</span><span class="p">(</span><span class="s1">&#39;colorblind&#39;</span><span class="p">)</span>
<span class="n">seaborn</span><span class="o">.</span><span class="n">set_style</span><span class="p">(</span><span class="s1">&#39;darkgrid&#39;</span><span class="p">)</span>

<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="o">%</span><span class="k">config</span> InlineBackend.figure_format = &#39;png&#39;
<span class="c1">#matplotlib.rcParams[&#39;figure.figsize&#39;] = (12.0, 1.0)</span>
<span class="n">matplotlib</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;font.size&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">14</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">freq_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="s1">&#39;freq&#39;</span> <span class="ow">in</span> <span class="n">s</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">domain_strings</span><span class="p">])</span>
<span class="n">time_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="s1">&#39;time&#39;</span> <span class="ow">in</span> <span class="n">s</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">domain_strings</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">98349384</span><span class="p">)</span>
<span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;grey&#39;</span>
<span class="n">domain_strings</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">domain_strings</span><span class="p">)</span>
<span class="n">start_fs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">start_fs</span><span class="p">)</span>
<span class="n">end_fs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">end_fs</span><span class="p">)</span>
<span class="n">i_sort</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">flatnonzero</span><span class="p">(</span><span class="n">time_mask</span><span class="p">)[</span><span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">end_fs</span><span class="p">[</span><span class="n">time_mask</span><span class="p">])]</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">d</span><span class="p">,</span><span class="n">s</span><span class="p">,</span><span class="n">e</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span>
        <span class="n">domain_strings</span><span class="p">[</span><span class="n">i_sort</span><span class="p">],</span> <span class="n">start_fs</span><span class="p">[</span><span class="n">i_sort</span><span class="p">],</span> <span class="n">end_fs</span><span class="p">[</span><span class="n">i_sort</span><span class="p">])):</span>
    <span class="n">offset</span> <span class="o">=</span> <span class="mf">0.6</span><span class="o">*</span><span class="n">i</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">i_sort</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.3</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">offset</span><span class="p">,</span><span class="n">offset</span><span class="p">]</span> <span class="p">,</span> <span class="p">[</span><span class="n">s</span><span class="p">,</span> <span class="n">e</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">)</span>
<span class="n">i_sort</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">flatnonzero</span><span class="p">(</span><span class="n">freq_mask</span><span class="p">)[</span><span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">end_fs</span><span class="p">[</span><span class="n">freq_mask</span><span class="p">])]</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">d</span><span class="p">,</span><span class="n">s</span><span class="p">,</span><span class="n">e</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span>
        <span class="n">domain_strings</span><span class="p">[</span><span class="n">i_sort</span><span class="p">],</span> <span class="n">start_fs</span><span class="p">[</span><span class="n">i_sort</span><span class="p">],</span> <span class="n">end_fs</span><span class="p">[</span><span class="n">i_sort</span><span class="p">])):</span>
    <span class="n">offset</span> <span class="o">=</span> <span class="mf">0.6</span><span class="o">*</span><span class="n">i</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">i_sort</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.7</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">offset</span><span class="p">,</span><span class="n">offset</span><span class="p">]</span> <span class="p">,</span> <span class="p">[</span><span class="n">s</span><span class="p">,</span> <span class="n">e</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">1.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Input domain&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Frequency [Hz]&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;Time&quot;</span><span class="p">,</span> <span class="s2">&quot;Frequency&quot;</span><span class="p">],</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">45</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Input domains and frequency ranges in prior work&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.05</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0.5, 1.05, &#39;Input domains and frequency ranges in prior work&#39;)
</pre></div>
</div>
<img alt="_images/ConvertLatex_31_1.png" src="_images/ConvertLatex_31_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="mi">1</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">98349384</span><span class="p">)</span>
<span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;grey&#39;</span>
<span class="k">for</span> <span class="n">low_c</span><span class="p">,</span> <span class="n">high_c</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">low_conv_ls</span><span class="p">,</span> <span class="n">high_conv_ls</span><span class="p">):</span>
    <span class="n">offset</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.1</span>
    <span class="n">tried_cs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">low_c</span><span class="p">,</span> <span class="n">high_c</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">offset</span><span class="p">,]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">tried_cs</span><span class="p">),</span> <span class="n">tried_cs</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">)</span>
    
<span class="k">for</span> <span class="n">i_c</span><span class="p">,</span> <span class="n">n_c</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">bincount_conv</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">i_c</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="n">n_c</span><span class="o">*</span><span class="mi">40</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.535</span><span class="p">,</span> <span class="n">i_c</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">n_c</span><span class="p">)</span><span class="o">+</span> <span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;left&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">low_c</span><span class="p">,</span> <span class="n">high_c</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">low_dense_ls</span><span class="p">,</span> <span class="n">high_dense_ls</span><span class="p">):</span>
    <span class="n">offset</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">rng</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.1</span>
    <span class="n">tried_cs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">low_c</span><span class="p">,</span> <span class="n">high_c</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">offset</span><span class="p">,]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">tried_cs</span><span class="p">),</span> <span class="n">tried_cs</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">)</span>
    
<span class="k">for</span> <span class="n">i_c</span><span class="p">,</span> <span class="n">n_c</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">bincount_dense</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="mf">1.4</span><span class="p">,</span> <span class="n">i_c</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="n">n_c</span><span class="o">*</span><span class="mi">40</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">1.535</span><span class="p">,</span> <span class="n">i_c</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">n_c</span><span class="p">)</span><span class="o">+</span> <span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;left&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Type of layer&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Number of layers&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;Convolutional&quot;</span><span class="p">,</span> <span class="s2">&quot;Dense&quot;</span><span class="p">],</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">45</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">7</span><span class="p">]);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Number of layers in prior works&#39; architectures&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.05</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="layer-stuff">
<h1>layer stuff<a class="headerlink" href="#layer-stuff" title="Permalink to this headline">¬∂</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ls</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s1">&#39;Conv/dense layers&#39;</span><span class="p">])[</span><span class="mi">1</span><span class="p">:]</span> <span class="c1"># exclude ourstudy</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ls</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="s1">&#39; 2/2 &#39;</span><span class="p">,</span> <span class="s1">&#39; 3/1 &#39;</span><span class="p">,</span> <span class="s1">&#39; 2/2 &#39;</span><span class="p">,</span> <span class="s1">&#39; 3/2 &#39;</span><span class="p">,</span> <span class="s1">&#39; 1/1 &#39;</span><span class="p">,</span> <span class="s1">&#39; 1/2 &#39;</span><span class="p">,</span> <span class="s1">&#39; 1/3 &#39;</span><span class="p">,</span>
       <span class="s1">&#39; 1‚Äì2/2 &#39;</span><span class="p">,</span> <span class="s1">&#39; 3/1 (+ LSTM as postprocessor) &#39;</span><span class="p">,</span> <span class="s1">&#39; 4/3 &#39;</span><span class="p">,</span> <span class="s1">&#39; 1-3/1-3 &#39;</span><span class="p">,</span>
       <span class="s1">&#39; 3‚Äì7/2 (+ LSTM or other temporal post-processing (see design choices)) &#39;</span><span class="p">,</span>
       <span class="s1">&#39; 2/1 &#39;</span><span class="p">,</span> <span class="s1">&#39; 3/3 (Spatio-temporal regularization) &#39;</span><span class="p">,</span>
       <span class="s1">&#39; 2/2 (Final fully connected layer uses concatenated output by convolutionaland fully connected layers) &#39;</span><span class="p">,</span>
       <span class="s1">&#39; 1-2/1 &#39;</span><span class="p">,</span>
       <span class="s1">&#39;2/0 (Convolutional deep belief network, separately trained RBF-SVM classifier) &#39;</span><span class="p">,</span>
       <span class="s1">&#39; 3/1 (Convolutional layers trained as convolutional stacked autoencoder with target prior) &#39;</span><span class="p">,</span>
       <span class="s1">&#39; 2/2 &#39;</span><span class="p">])</span>

<span class="n">conv_ls</span> <span class="o">=</span> <span class="p">[</span><span class="n">l</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;/&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">ls</span><span class="p">]</span>
<span class="n">low_conv_ls</span> <span class="o">=</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">re</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;[‚Äì-]&#39;</span><span class="p">,</span> <span class="n">c</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span><span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">conv_ls</span><span class="p">]</span>
<span class="n">high_conv_ls</span> <span class="o">=</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">re</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;[‚Äì-]&#39;</span><span class="p">,</span> <span class="n">c</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span><span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">conv_ls</span><span class="p">]</span>
<span class="n">dense_ls</span> <span class="o">=</span> <span class="p">[</span><span class="n">l</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;/&#39;</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">ls</span><span class="p">]</span>
<span class="n">low_dense_ls</span> <span class="o">=</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">re</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;[‚Äì-]&#39;</span><span class="p">,</span> <span class="n">c</span><span class="p">[:</span><span class="mi">8</span><span class="p">])[</span><span class="mi">0</span><span class="p">][:</span><span class="mi">2</span><span class="p">])</span><span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">dense_ls</span><span class="p">]</span>
<span class="n">high_dense_ls</span> <span class="o">=</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">re</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;[‚Äì-]&#39;</span><span class="p">,</span> <span class="n">c</span><span class="p">[:</span><span class="mi">8</span><span class="p">])[</span><span class="o">-</span><span class="mi">1</span><span class="p">][:</span><span class="mi">2</span><span class="p">])</span><span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">dense_ls</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">cm</span>
<span class="kn">import</span> <span class="nn">seaborn</span>
<span class="n">seaborn</span><span class="o">.</span><span class="n">set_palette</span><span class="p">(</span><span class="s1">&#39;colorblind&#39;</span><span class="p">)</span>
<span class="n">seaborn</span><span class="o">.</span><span class="n">set_style</span><span class="p">(</span><span class="s1">&#39;darkgrid&#39;</span><span class="p">)</span>

<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="o">%</span><span class="k">config</span> InlineBackend.figure_format = &#39;png&#39;
<span class="c1">#matplotlib.rcParams[&#39;figure.figsize&#39;] = (12.0, 1.0)</span>
<span class="n">matplotlib</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;font.size&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">14</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">all_conv_ls</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">low_c</span><span class="p">,</span> <span class="n">high_c</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">low_c</span><span class="p">,</span> <span class="n">high_c</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">low_conv_ls</span><span class="p">,</span> <span class="n">high_conv_ls</span><span class="p">)])</span>
<span class="n">all_dense_ls</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">low_c</span><span class="p">,</span> <span class="n">high_c</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">low_c</span><span class="p">,</span> <span class="n">high_c</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">low_dense_ls</span><span class="p">,</span> <span class="n">high_dense_ls</span><span class="p">)])</span>
<span class="n">bincount_conv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">all_conv_ls</span><span class="p">)</span>
<span class="n">bincount_dense</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">all_dense_ls</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">98349384</span><span class="p">)</span>
<span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;grey&#39;</span>
<span class="k">for</span> <span class="n">low_c</span><span class="p">,</span> <span class="n">high_c</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">low_conv_ls</span><span class="p">,</span> <span class="n">high_conv_ls</span><span class="p">):</span>
    <span class="n">offset</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.1</span>
    <span class="n">tried_cs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">low_c</span><span class="p">,</span> <span class="n">high_c</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">offset</span><span class="p">,]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">tried_cs</span><span class="p">),</span> <span class="n">tried_cs</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">)</span>
    
<span class="k">for</span> <span class="n">i_c</span><span class="p">,</span> <span class="n">n_c</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">bincount_conv</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">i_c</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="n">n_c</span><span class="o">*</span><span class="mi">40</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.535</span><span class="p">,</span> <span class="n">i_c</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">n_c</span><span class="p">)</span><span class="o">+</span> <span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;left&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">low_c</span><span class="p">,</span> <span class="n">high_c</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">low_dense_ls</span><span class="p">,</span> <span class="n">high_dense_ls</span><span class="p">):</span>
    <span class="n">offset</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">rng</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.1</span>
    <span class="n">tried_cs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">low_c</span><span class="p">,</span> <span class="n">high_c</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">offset</span><span class="p">,]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">tried_cs</span><span class="p">),</span> <span class="n">tried_cs</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">)</span>
    
<span class="k">for</span> <span class="n">i_c</span><span class="p">,</span> <span class="n">n_c</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">bincount_dense</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="mf">1.4</span><span class="p">,</span> <span class="n">i_c</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="n">n_c</span><span class="o">*</span><span class="mi">40</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">1.535</span><span class="p">,</span> <span class="n">i_c</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">n_c</span><span class="p">)</span><span class="o">+</span> <span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;left&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Type of layer&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Number of layers&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;Convolutional&quot;</span><span class="p">,</span> <span class="s2">&quot;Dense&quot;</span><span class="p">],</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">45</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">7</span><span class="p">]);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Number of layers in prior works&#39; architectures&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.05</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="p">[</span><span class="s1">&#39;Study&#39;</span><span class="p">,</span> <span class="s1">&#39;Decoding problem&#39;</span><span class="p">,</span> <span class="s1">&#39;External baseline&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">to_markdown</span><span class="p">(</span><span class="n">showindex</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:left head"><p>Study</p></th>
<th class="text-align:left head"><p>Decoding problem</p></th>
<th class="text-align:left head"><p>External baseline</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:left"><p>This manuscript, Schirrmeister et. al (2017)</p></td>
<td class="text-align:left"><p>Imagined and executed movement classes, within subject</p></td>
<td class="text-align:left"><p>FBCSP + rLDA</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Single-trial EEG classification of motor imagery using deep convolutional neural networks, citet{tang_single-trial_2017}</p></td>
<td class="text-align:left"><p>Imagined movement classes, within-subject</p></td>
<td class="text-align:left"><p>FBCSP</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>EEGNet: A Compact Convolutional Network for EEG-based Brain-Computer Interfaces, <span id="id69">[<a class="reference internal" href="References.html#id122">LSW+16</a>]</span></p></td>
<td class="text-align:left"><p>Oddball response (RSVP), error response (ERN), movement classes (voluntarily started and imagined)</p></td>
<td class="text-align:left"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Remembered or Forgotten? ‚Äì- An EEG-Based Computational Prediction Approach, <span id="id70">[<a class="reference internal" href="References.html#id195">SQC+16</a>]</span></p></td>
<td class="text-align:left"><p>Memory performance, within-subject</p></td>
<td class="text-align:left"><p></p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Multimodal Neural Network for Rapid Serial Visual Presentation Brain Computer Interface, <span id="id71">[<a class="reference internal" href="References.html#id169">MMG16</a>]</span></p></td>
<td class="text-align:left"><p>Oddball response using RSVP and image (combined image-EEG decoding), within-subject</p></td>
<td class="text-align:left"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>A novel deep learning approach for classification of EEG motor imagery signals, <span id="id72">[<a class="reference internal" href="References.html#id188">TH17</a>]</span></p></td>
<td class="text-align:left"><p>Imagined and executed movement classes, within-subject</p></td>
<td class="text-align:left"><p>Weights (spatial + frequential)</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Predicting Seizures from Electroencephalography Recordings: A Knowledge Transfer Strategy, <span id="id73">[<a class="reference internal" href="References.html#id144">LLZW16</a>]</span></p></td>
<td class="text-align:left"><p>Seizure prediction, within-subject</p></td>
<td class="text-align:left"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>EEG-based prediction of driver‚Äôs cognitive performance by deep convolutional neural network, <span id="id74">[<a class="reference internal" href="References.html#id152">HMJ+16</a>]</span></p></td>
<td class="text-align:left"><p>Driver performance, within- and cross-subject</p></td>
<td class="text-align:left"><p></p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Deep learning for epileptic intracranial EEG data, <span id="id75">[<a class="reference internal" href="References.html#id126">ASTS16</a>]</span></p></td>
<td class="text-align:left"><p>Epileptic discharges, cross-subject</p></td>
<td class="text-align:left"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Learning Robust Features using Deep Learning for Automatic Seizure Detection, <span id="id76">[<a class="reference internal" href="References.html#id154">TPL16</a>]</span></p></td>
<td class="text-align:left"><p>Start of epileptic seizure, within- and cross-subject</p></td>
<td class="text-align:left"><p>Hand crafted features + SVM</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Single-trial EEG RSVP classification using convolutional neural networks, <span id="id77">[<a class="reference internal" href="References.html#id173">SLK+16</a>]</span></p></td>
<td class="text-align:left"><p>Oddball response (RSVP), groupwise (ConvNet trained on all subjects)</p></td>
<td class="text-align:left"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Wearable seizure detection using convolutional neural networks with transfer learning, <span id="id78">[<a class="reference internal" href="References.html#id177">PSM16</a>]</span></p></td>
<td class="text-align:left"><p>Seizure detection, cross-subject, within-subject, groupwise</p></td>
<td class="text-align:left"><p>Multiple: spectral features, higher order statistics + linear-SVM, RBF-SVM, ‚Ä¶</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Learning Representations from EEG with Deep Recurrent-Convolutional Neural Networks, <span id="id79">[<a class="reference internal" href="References.html#id170">BRYC16</a>]</span></p></td>
<td class="text-align:left"><p>Cognitive load (number of characters to memorize), cross-subject</p></td>
<td class="text-align:left"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Deep Feature Learning for EEG Recordings, <span id="id80">[<a class="reference internal" href="References.html#id199">Sto16</a>]</span></p></td>
<td class="text-align:left"><p>Type of music rhythm, groupwise (ensembles of leave-one-subject-out trained models, evaluated on separate test set of same subjects)</p></td>
<td class="text-align:left"><p></p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Convolutional Neural Network for Multi-Category Rapid Serial Visual Presentation BCI, <span id="id81">[<a class="reference internal" href="References.html#id119">MG15</a>]</span></p></td>
<td class="text-align:left"><p>Oddball response (RSVP), within-subject</p></td>
<td class="text-align:left"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Parallel Convolutional-Linear Neural Network for Motor Imagery Classification, <span id="id82">[<a class="reference internal" href="References.html#id89">SGY15</a>]</span></p></td>
<td class="text-align:left"><p>Imagined movement classes, within-subject</p></td>
<td class="text-align:left"><p></p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Using Convolutional Neural networks to Recognize Rhythm Stimuli form Electroencephalography Recordings, <span id="id83">[<a class="reference internal" href="References.html#id156">SCG14</a>]</span></p></td>
<td class="text-align:left"><p>Type of music rhythm, within-subject</p></td>
<td class="text-align:left"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Convolutional deep belief networks for feature extraction of EEG signal, <span id="id84">[<a class="reference internal" href="References.html#id109">RW14</a>]</span></p></td>
<td class="text-align:left"><p>Imagined movement classes, within-subject</p></td>
<td class="text-align:left"><p></p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Deep feature learning using target priors with applications in ECoG signal decoding for BCI, <span id="id85">[<a class="reference internal" href="References.html#id182">WLSJ13</a>]</span></p></td>
<td class="text-align:left"><p>Finger flexion trajectory (regression), within-subject</p></td>
<td class="text-align:left"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Convolutional neural networks for P300 detection with application to brain-computer interfaces, <span id="id86">[<a class="reference internal" href="References.html#id185">CG11</a>]</span></p></td>
<td class="text-align:left"><p>Oddball / attention response using P300 speller, within-subject</p></td>
<td class="text-align:left"><p>Multiple: Linear SVM, gradient boosting, E-SVM, S-SVM, mLVQ, LDA, ‚Ä¶</p></td>
</tr>
</tbody>
</table>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By The Jupyter Book community<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>