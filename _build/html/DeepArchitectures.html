
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Neural Network Architectures for EEG-Decoding &#8212; Deep Learning for Brain-Signal Decoding from Electroencephalography (EEG)</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script async="async" kind="hypothesis" src="https://hypothes.is/embed.js"></script>
    <script kind="utterances">

    var commentsRunWhenDOMLoaded = cb => {
    if (document.readyState != 'loading') {
        cb()
    } else if (document.addEventListener) {
        document.addEventListener('DOMContentLoaded', cb)
    } else {
        document.attachEvent('onreadystatechange', function() {
        if (document.readyState == 'complete') cb()
        })
    }
}

var addUtterances = () => {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src = "https://utteranc.es/client.js";
    script.async = "async";

    script.setAttribute("repo", "robintibor/eeg-deep-learning-phd-thesis");
    script.setAttribute("issue-term", "pathname");
    script.setAttribute("theme", "github-light");
    script.setAttribute("label", "üí¨ comment");
    script.setAttribute("crossorigin", "anonymous");

    sections = document.querySelectorAll("div.section");
    if (sections !== null) {
        section = sections[sections.length-1];
        section.appendChild(script);
    }
}
commentsRunWhenDOMLoaded(addUtterances);
</script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Cropped Training" href="CroppedTraining.html" />
    <link rel="prev" title="Prior Work" href="PriorWork.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/Rainbow_brain,_Aug_2014.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Deep Learning for Brain-Signal Decoding from Electroencephalography (EEG)</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Abstract.html">
   Deep Learning for Brain-Signal Decoding from Electroencephalography (EEG)
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="PriorWork.html">
   Prior Work
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Neural Network Architectures for EEG-Decoding
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="CroppedTraining.html">
   Cropped Training
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="PerturbationVisualization.html">
   Perturbation Visualization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="MovementDecoding.html">
   Decoding Movement-Related Brain Activity
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="TaskDecoding.html">
   Further Task-Related Decoding
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Pathology.html">
   Decoding Pathology
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="DeepLearningImprovements.html">
   Deep Learning Improvements
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="FutureWork.html">
   Future Work
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/DeepArchitectures.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/DeepArchitectures.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#filter-bank-common-spatial-patterns-as-a-starting-point">
   Filter Bank Common Spatial Patterns as a Starting Point
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#common-spatial-patterns">
     Common Spatial Patterns
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#filterbank">
   Filterbank
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#filterbank-network-architecture">
   Filterbank network architecture
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#shallow-network-architecture">
   Shallow Network Architecture
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#deep-network-architecture">
   Deep Network Architecture
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#residual-network">
   Residual Network
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#to-remove-temporal-convolutional-network">
   To remove? : Temporal Convolutional Network
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="neural-network-architectures-for-eeg-decoding">
<span id="network-architectures"></span><h1>Neural Network Architectures for EEG-Decoding<a class="headerlink" href="#neural-network-architectures-for-eeg-decoding" title="Permalink to this headline">¬∂</a></h1>
<p>We developed neural network architectures for EEG decoding with a EEG-specific development strategy. We started from smaller architectures that closely mimic a feature-based EEG-decoding algorithm and later progressed to a more generic architecture. This development strategy ensured that we could be fairly confident that the initial network architectures should perform as well as the feature-based algorithm. That also allowed us to use these smaller architectures to create a robust data preprocessing pipeline. After validating that the smaller architectures perform well with this pipeline, we could proceed to develop and evaluate more generic architectures.</p>
<div class="section" id="filter-bank-common-spatial-patterns-as-a-starting-point">
<h2>Filter Bank Common Spatial Patterns as a Starting Point<a class="headerlink" href="#filter-bank-common-spatial-patterns-as-a-starting-point" title="Permalink to this headline">¬∂</a></h2>
<p>We selected filter bank common spatial patterns (FBSCP) as the feature-based EEG-decoding algorithm to  mimic using neural network architectures. FBCSP is an EEG-decoding algorithm that has been successfully used in task-related EEG-decoding competitions [refs]. FBCSP aims to decode changes in the amplitude of different frequencies. These amplitude changes often happen in the EEG signal during certain tasks. The basic building block of FBCSP is the Common Spatial Patterns (CSP) algorithm. CSP aims to find a spatial filter over the EEG electrodes, such that the variance of the spatially filtered EEG signal allows distinguish two conditions. More specifically, the spatially filtered signal maximizes the ratio of the signal variance between the two conditions, e.g. of the signal during two different movements. For example, the signal of a spatial filter computed by CSP may have a very large variance during movements of the left hand and a very small variance during movements of the right hand.</p>
<div class="figure align-default" id="csp-figure">
<img alt="_images/Methods_Common_Spatial_Patterns_18_0.png" src="_images/Methods_Common_Spatial_Patterns_18_0.png" />
<p class="caption"><span class="caption-number">Fig. 3 </span><span class="caption-text">Common Spatial Patterns example taken from a master thesis [ref]. Top parts show EEG signals for three electrodes during a left hand and  a right hand movement. Bottom parts show spatially filtered signals of two CSP filters. Green parts have lower variance and red parts have higher variance. Note that this difference is strongly amplified after CSP filtering.</span><a class="headerlink" href="#csp-figure" title="Permalink to this image">¬∂</a></p>
</div>
<div class="section" id="common-spatial-patterns">
<h3>Common Spatial Patterns<a class="headerlink" href="#common-spatial-patterns" title="Permalink to this headline">¬∂</a></h3>
<p>In EEG Decoding, Common Spatial Patterns (CSP) [ref] is used to decode brain signals that lead to a change in the amplitudes of the EEG signal with a specific spatial topography. To do that, CSP aims to maximize the ratio of the signal variance between signals of two classes. Concretely, we are given signals <span class="math notranslate nohighlight">\(X_{1}, X_{2} \in \mathbb{R}^{n x k x t}\)</span> from <span class="math notranslate nohighlight">\(n\)</span> EEG trials (can be different for <span class="math notranslate nohighlight">\(X_1, X_2\)</span>), <span class="math notranslate nohighlight">\(k\)</span> EEG electrodes and <span class="math notranslate nohighlight">\(t\)</span> timepoints within each trial. CSP then finds a spatial filter <span class="math notranslate nohighlight">\(w\)</span> that maximize the ratio of the variances of the spatially filtered <span class="math notranslate nohighlight">\(X_1,X_2\)</span>:</p>
<p><span class="math notranslate nohighlight">\(w=\arg\!\max_w\frac{Var(w^T X_1)}{Var(w^T X_2)}= \arg\!\max_w\frac{||w^T X_1||^2}{||w^T X_2||^2}=\arg\!\max_w\frac{w^T X_1  X_1^T w}{w^T X_2  X_2^T w}\)</span></p>
<p>Rather than just finding a single spatial filter <span class="math notranslate nohighlight">\(w\)</span>, CSP is typically used to find a whole matrix of spatial filters <span class="math notranslate nohighlight">\(W^{kxk}\)</span>, with spatial filters ordered by the above variance ratio and orthogonal to each other. So the first filter <span class="math notranslate nohighlight">\(w_1\)</span> results in the largest variance ratio and the last filter <span class="math notranslate nohighlight">\(w_k\)</span> results in the smallest variance ratio. Different algorithms can then be used to subselect some set of filters to filter signals for a subsequent decoding algorithm.</p>
<p>The CSP-filtered signals can be used to construct features to train a classifier. Since the CSP-filtered signals should have very different variances for the different classes, the natural choice is to use the per-trial variances of the CSP-filtered signals as features. This results in as many features per trial as the number of CSP filters that were selected for decoding. Typically, one applies the logarithm to the variances to get more standard-normally distributed features.</p>
</div>
</div>
<div class="section" id="filterbank">
<h2>Filterbank<a class="headerlink" href="#filterbank" title="Permalink to this headline">¬∂</a></h2>
<p>CSP is typically applied to an EEG signal that has been bandpass filtered to a specific frequency range. The filtering to a frequency range is useful as brain signals cause EEG signal amplitude changes that are temporally and spatially different for different frequencies [refs]. For example, during movement the alpha rhythm may be suppressed for multiple electrodes covering a fairly large region on the scalp while the high gamma rhythm would be amplified for a few electrodes covering a smaller region.</p>
<p>Filterbank Common Spatial Patterns applies CSP separately on signals bandpass-filtered to different frequency ranges [ref]. This allows to capture multiple frequency-specific changes in the EEG signal and can also make the decoding more robust to subject-specific signal characteristics, i.e., which frequency range is most informative for a given subject. The trial-log-variance features of each frequencyband and each CSP filter are then concatenated to form the entire trial feature vector. Typically, a feature selection procedure will select a subset of these features to train the final classifier.</p>
<p>The overall FBCSP pipeline hence looks like this (from [ref]):</p>
<ol class="simple">
<li><p><strong>Bandpass filtering</strong>: Different bandpass filters are applied to separate the raw EEG signal into different frequency bands.</p></li>
<li><p><strong>Epoching</strong>: The continuous EEG signal is cut into trials as explained in the section ‚ÄúInput and labels.‚Äù</p></li>
<li><p><strong>CSP computation</strong>: Per frequency band, the common spatial patterns (CSP) algorithm is applied to extract spatial filters. CSP aims to extract spatial filters that make the trials discriminable by the power of the spatially filtered trial signal (see Koles et al. [1990], Ramoser et al. [2000], and Blankertz et al. [2008] for more details on the computation).</p></li>
<li><p><strong>Spatial filtering</strong>: The spatial filters computed in Step 2 are applied to the EEG signal.</p></li>
<li><p><strong>Feature construction</strong>: Feature vectors are constructed from the filtered signals: Specifically, feature vectors are the log-variance of the spatially filtered trial signal for each frequency band and for each spatial filter.</p></li>
<li><p><strong>Classification</strong>: A classifier is trained to predict per-trial labels based on the feature vectors.</p></li>
</ol>
</div>
<div class="section" id="filterbank-network-architecture">
<h2>Filterbank network architecture<a class="headerlink" href="#filterbank-network-architecture" title="Permalink to this headline">¬∂</a></h2>
<p>The first neural network architecture was developed by us in a prior master thesis [ref] to jointly learn the same steps that are learned separately by FBCSP. Concretely, the network simultaenously learn the spatial filters across many frequency bands and the classification weights for the trial variances of all resulting spatially filtered signals. To be able to do that, the network is fed with several signals that were bandpass-filtered to different frequency ranges. The network then performs the following steps:</p>
<ol class="simple">
<li><p>Apply learnable spatial filter weights, resulting in spatially filtered signals</p></li>
<li><p>Square the resulting signals</p></li>
<li><p>Sum the squared signals across the trial</p></li>
<li><p>Take the logarithm of the summed values</p></li>
<li><p>Apply learnable classification weights on these ‚Äúlog-variance‚Äù features</p></li>
<li><p>Take the softmax to produce per-class predictions.</p></li>
</ol>
<p>The spatial filter weights and the classification weights are trained jointly.</p>
<div class="figure align-default" id="filterbank-net-figure">
<img alt="_images/csp_as_a_net_explanation.png" src="_images/csp_as_a_net_explanation.png" />
<p class="caption"><span class="caption-number">Fig. 4 </span><span class="caption-text">Filterbank network architecture overview.  Input signals were bandpass filtered to different frequency ranges. Signals are first transformed by learned spatial filters, then squared, summed and the log-transformed. The resulting features are transformed into class probabilities by a classification weights followed by the softmax function.</span><a class="headerlink" href="#filterbank-net-figure" title="Permalink to this image">¬∂</a></p>
</div>
</div>
<div class="section" id="shallow-network-architecture">
<h2>Shallow Network Architecture<a class="headerlink" href="#shallow-network-architecture" title="Permalink to this headline">¬∂</a></h2>
<p>Next, we developed the shallow network architecture, a more flexible architecture that also learns temporal filters on the input signal and on the later representation. Instead of bandpass-filtered signals, it is fed the raw signals as input. The first step are learnable temporal filters that are indepedently convolved with the signals of each EEG electrode. Afterwards, the channel dimension of the network representation    contains <span class="math notranslate nohighlight">\(\mathrm{electrodes} \cdot \mathrm{temporal~ filters}\)</span> channels. In the next step that combines spatial filtering with mixing the outputs of the temporal filters, this network-channel dimension is linearly transformed by learned weights to a smaller dimensionality for further preprocessing. The resulting feature timeseries are then squared, average-pooled and log-transformed, which allows the network to more easily learn log-variance-based features. Unlike the filterbank network, the average pooling does not collapse the feature timeseries into one value per trial. So after these processing steps, still some temporal information about the timecourse of the variance throughout the trial can be preserved. Then, the final classification layer transforms these feature timecourses into class probabilities using a linear transformation and a softmax function.</p>
<div class="figure align-default" id="shallow-net-figure">
<a class="reference internal image-reference" href="_images/3D_Diagram_MatplotLib.ipynb.0.png"><img alt="_images/3D_Diagram_MatplotLib.ipynb.0.png" src="_images/3D_Diagram_MatplotLib.ipynb.0.png" style="width: 50%;" /></a>
<p class="caption"><span class="caption-number">Fig. 5 </span><span class="caption-text">Shallow network architecture, figure from [ref].  EEG input (at the top) is progressively transformed toward the bottom, until the final classifier output. Black cuboids: inputs/feature maps; brown cuboids: convolution/pooling kernels. The corresponding sizes are indicated in black and brown, respectively.</span><a class="headerlink" href="#shallow-net-figure" title="Permalink to this image">¬∂</a></p>
</div>
</div>
<div class="section" id="deep-network-architecture">
<h2>Deep Network Architecture<a class="headerlink" href="#deep-network-architecture" title="Permalink to this headline">¬∂</a></h2>
<p>The deep architecture is a more generic architecture, closer to network architectures used in computer vision. The first two temporal convolution and spatial filtering layers are the same in the shallow network, which is followed by a ELU nonlinearity [ref] and max pooling. The following three blocks simply consist of a convolution, a ELU nonlinearity and a max pooling. In the end, there is again a final linear classification layer with a softmax function. Due to its less specific and more generic computational steps, the deep architecture should be able to capture a large variety of features. Hence, the learned features may also be less biased towards the amplitude features commonly used in task-related EEG decoding.</p>
<div class="figure align-default" id="deep-net-figure">
<a class="reference internal image-reference" href="_images/3D_Diagram_MatplotLib.ipynb.1.png"><img alt="_images/3D_Diagram_MatplotLib.ipynb.1.png" src="_images/3D_Diagram_MatplotLib.ipynb.1.png" style="width: 75%;" /></a>
<p class="caption"><span class="caption-number">Fig. 6 </span><span class="caption-text">Deep network architecture, figure from [ref].</span><a class="headerlink" href="#deep-net-figure" title="Permalink to this image">¬∂</a></p>
</div>
</div>
<div class="section" id="residual-network">
<h2>Residual Network<a class="headerlink" href="#residual-network" title="Permalink to this headline">¬∂</a></h2>
<p>We also developed a residual network (ResNet) for EEG decoding. We use the same residual blocks as the original paper, described in Figure <a class="reference internal" href="#residual-net-figure"><span class="std std-numref">Fig. 7</span></a>. Our ResNet used exponential linear unit activation functions [Clevert et al., 2016] throughout the network (same as the deep ConvNet) and also starts with a splitted temporal and spatial convolution (same as the deep and shallow ConvNets), followed by 14 residual blocks, mean pooling and a final softmax dense classification layer (for further details, see Supporting Information, Section A.3 in [ref]).</p>
<div class="figure align-default" id="residual-net-figure">
<img alt="_images/residual_block.png" src="_images/residual_block.png" />
<p class="caption"><span class="caption-number">Fig. 7 </span><span class="caption-text">Residual block, Figure from [ref]. ‚ÄúResidual block used in the ResNet architecture and as described in original paper (He et al. [2015]; see Fig. 2) with identity shortcut option A, except using ELU instead of ReLU nonlinearities.‚Äù</span><a class="headerlink" href="#residual-net-figure" title="Permalink to this image">¬∂</a></p>
</div>
</div>
<div class="section" id="to-remove-temporal-convolutional-network">
<h2>To remove? : Temporal Convolutional Network<a class="headerlink" href="#to-remove-temporal-convolutional-network" title="Permalink to this headline">¬∂</a></h2>
<p>In another master thesis [ref], Patrick Ch‚Ä¶ developed the Temporal Convolutional Network for EEG decoding using automatic hyperparameter optimization. Temporal Convolutional Networks use residual blocks and dilated convolutions and had originally been introduced as an alternative to recurrent neural networks.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="PriorWork.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title">Prior Work</p>
            </div>
        </a>
    </div>
     <div id="next">
        <a class="right-next" href="CroppedTraining.html" title="next page">
            <div class="prevnext-info">
                <p class="prevnext-label">next</p>
                <p class="prevnext-title">Cropped Training</p>
            </div>
            <i class="prevnext-label fas fa-angle-right"></i>
        </a>
     </div>

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By The Jupyter Book community<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>