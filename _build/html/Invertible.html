

<!DOCTYPE html>


<html lang="en" data-theme="light">

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Invertible Networks &#8212; Deep Learning for Brain-Signal Decoding from Electroencephalography (EEG)</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "light";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script async="async" kind="hypothesis" src="https://hypothes.is/embed.js"></script>
    <script kind="utterances">

    var commentsRunWhenDOMLoaded = cb => {
    if (document.readyState != 'loading') {
        cb()
    } else if (document.addEventListener) {
        document.addEventListener('DOMContentLoaded', cb)
    } else {
        document.attachEvent('onreadystatechange', function() {
        if (document.readyState == 'complete') cb()
        })
    }
}

var addUtterances = () => {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src = "https://utteranc.es/client.js";
    script.async = "async";

    script.setAttribute("repo", "robintibor/eeg-deep-learning-phd-thesis");
    script.setAttribute("issue-term", "pathname");
    script.setAttribute("theme", "github-light");
    script.setAttribute("label", "ðŸ’¬ comment");
    script.setAttribute("crossorigin", "anonymous");

    sections = document.querySelectorAll("div.section");
    if (sections !== null) {
        section = sections[sections.length-1];
        section.appendChild(script);
    }
}
commentsRunWhenDOMLoaded(addUtterances);
</script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'Invertible';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Perturbation Visualization" href="PerturbationVisualization.html" />
    <link rel="prev" title="Cropped Training" href="CroppedTraining.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="light">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="Abstract.html">
  
  
  
  
    
    
    
    <img src="_static/braindecode-logo.svg" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="_static/braindecode-logo.svg" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="Introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="PriorWork.html">Prior Work</a></li>
<li class="toctree-l1"><a class="reference internal" href="FBCSPAndFBCSPNet.html">Filter Bank Common Spatial Patterns and Filterbank Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="DeepArchitectures.html">Neural Network Architectures for EEG-Decoding</a></li>
<li class="toctree-l1"><a class="reference internal" href="CroppedTraining.html">Cropped Training</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Invertible Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="PerturbationVisualization.html">Perturbation Visualization</a></li>
<li class="toctree-l1"><a class="reference internal" href="MovementDecoding.html">Decoding Movement-Related Brain Activity</a></li>
<li class="toctree-l1"><a class="reference internal" href="TaskDecoding.html">Generalization to Other Tasks</a></li>
<li class="toctree-l1"><a class="reference internal" href="Pathology.html">Decoding Pathology</a></li>
<li class="toctree-l1"><a class="reference internal" href="UnderstandingPathology.html">Understanding Pathology Decoding With Invertible Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="FutureWork.html">Future Work</a></li>
<li class="toctree-l1"><a class="reference internal" href="References.html">References</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/Invertible.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Invertible Networks</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#background">Background</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#invertible-layers">Invertible layers</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#generative-models-via-maximum-likelihood">Generative models via maximum likelihood</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#generative-classifiers">Generative classifiers</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#invertible-network-for-eeg-decoding">Invertible Network for EEG Decoding</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <div class="tex2jax_ignore mathjax_ignore section" id="invertible-networks">
<span id="id1"></span><h1>Invertible Networks<a class="headerlink" href="#invertible-networks" title="Permalink to this heading">#</a></h1>
<p>Invertible networks are networks that are invertible by design, i.e., any network output can be mapped back to a corresponding input [refs] bijectively. The ability to invert any output back to the input enables different interpretability methods and furthermore allows training invertible networks as generative models via maximum likelihood.</p>
<p>This chapter starts by explaining what invertible layers are used to design invertible networks, proceeds to detail their training methodologies as generative models or classifiers, and goes on to outline interpretability techniques that help reveal the learned features crucial for their classification tasks.</p>
<div class="section" id="background">
<h2>Background<a class="headerlink" href="#background" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>invertible layers</p>
<ul>
<li><p>explain invertible layers, maybe bold part in front</p></li>
<li><p>(mention wavelet permutation also)</p></li>
</ul>
</li>
<li><p>example for volume change, do make it (use seaborn colors, in <a class="reference external" href="http://draw.io">draw.io</a>)</p>
<ul>
<li><p>do consider making some bar plots with matplotlib why not</p></li>
</ul>
</li>
<li><p>architecture diagram</p>
<ul>
<li><p>what is one block? Haar Wavelet addition</p></li>
<li><p>Latent gaussian at end, no donâ€™t show</p></li>
</ul>
</li>
</ul>
<div class="section" id="invertible-layers">
<h3>Invertible layers<a class="headerlink" href="#invertible-layers" title="Permalink to this heading">#</a></h3>
<p>\begin{equation*}
\begin{aligned}</p>
<p>y_1 &amp;= x_1 + f(x_2); \quad \text{Forward comment 1} \
y_2 &amp;= x_2; \quad \text{Forward comment 2} \</p>
<p>x_1 &amp;= y_1 - f(y_2); \quad \text{Inverse comment 1} \
x_2 &amp;= y_2; \quad \text{Inverse comment 2} \
\end{aligned}
\end{equation*}</p>
<p><span class="math notranslate nohighlight">\(\require{color}\)</span></p>
<p><span class="math notranslate nohighlight">\(\definecolor{commentcolor}{RGB} {70,130,180}\)</span></p>
<p>[Figure coupling blocks]</p>
<ul class="simple">
<li><p>you could have a figure here with time series and half of time series computed sth, then additive coefficient added</p></li>
</ul>
<p>Invertible networks use layers constructed specifically to maintain invertibility, thereby rendering the entire network structure invertible. Often-used invertible layers are coupling layers, invertible linear layers and activation normalization layers.</p>
<p><strong>Coupling layers</strong> split a multidimensional input <span class="math notranslate nohighlight">\(x\)</span> into two parts  <span class="math notranslate nohighlight">\(x_1\)</span> and <span class="math notranslate nohighlight">\(x_2\)</span> with disjoint dimensions and use <span class="math notranslate nohighlight">\(x_2\)</span> to compute an invertible transformation to apply to <span class="math notranslate nohighlight">\(x_1\)</span>. Concretely, for an additive coupling block, the forward computation is like this:</p>
<p><span class="math notranslate nohighlight">\(
\begin{align*}
    y_1 &amp;= x_1 + f(x_2); &amp;&amp; \color{commentcolor}{\text{Compute \)</span>y_1<span class="math notranslate nohighlight">\( from \)</span>x_1<span class="math notranslate nohighlight">\( and arbitrary function of \)</span>x_2<span class="math notranslate nohighlight">\(}} \\
    y_2 &amp;= x_2 &amp;&amp; \color{commentcolor}{\text{Leave } x_2 \text{ unchanged}} \\
\end{align*}
\)</span></p>
<p>The inverse computation is:</p>
<p><span class="math notranslate nohighlight">\(
\begin{align*}
    x_1 &amp;= y_1 - f(y_2) &amp;&amp; \color{commentcolor}{\text{Invert to } x_1 \text{ using unchanged } y_2=x_2} \\
    x_2 &amp;= y_2 &amp;&amp;  \color{commentcolor}{x_2 \text{ was unchanged}}\\
\end{align*}
\)</span></p>
<p>of <span class="math notranslate nohighlight">\(d_1\)</span> and <span class="math notranslate nohighlight">\(d_2\)</span> dimensions with <span class="math notranslate nohighlight">\(d_1 + d_2 = d\)</span>.</p>
<p>For example in a timeseries one may take the input at all the even time indices as <span class="math notranslate nohighlight">\(x_1\)</span> and all the odd time indices as <span class="math notranslate nohighlight">\(x_2\)</span> odd time indices. Then, one transforms <span class="math notranslate nohighlight">\(x_1\)</span> in an invertible way (e.g., by adding something to it) based on computations only performed on <span class="math notranslate nohighlight">\(x_2\)</span>, while leaving <span class="math notranslate nohighlight">\(x_2\)</span> unchanged. For, example additive coupling works as follows:
<span class="math notranslate nohighlight">\(y_1 = x_1 + f(x_2); y_2=x_2\)</span>, with <span class="math notranslate nohighlight">\(f\)</span> being an arbitrary function, e.g., any (potentially non-invertible) neural network. It can be inverted given <span class="math notranslate nohighlight">\(y_1\)</span> and <span class="math notranslate nohighlight">\(y_2\)</span> as follows: <span class="math notranslate nohighlight">\(x_1 = y_1 - f(y_2); x_2=y_2\)</span>. Instead of addition one may use any other invertible function, a common one is an affine transformation where <span class="math notranslate nohighlight">\(f\)</span> produces translation and scaling coefficients <span class="math notranslate nohighlight">\(f_t\)</span> and <span class="math notranslate nohighlight">\(f_s\)</span>:
<span class="math notranslate nohighlight">\(y_1 = x_1 \cdot f_s(x_2) + f_t(x_2); y_2=x_2\)</span>, with inversion <span class="math notranslate nohighlight">\(x_1 = \frac{(y_1  - f_t(x_2))}{f_s(x_2)}; y_2=x_2\)</span>.
The splitting of dimensions can be done in multiply ways, like using odd or even indices or using difference and mean between two neighbouring samples (akin to one stage of a Haar Wavelet).</p>
<p>Invertible linear layers compute an invertible linear transformation (an automorphism) of their input. Concretely they multiply a <span class="math notranslate nohighlight">\(d\)</span>-dimensional vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> with a <span class="math notranslate nohighlight">\(dxd\)</span>-dimensional matrix <span class="math notranslate nohighlight">\(W\)</span>, where the <span class="math notranslate nohighlight">\(W\)</span> has to be invertible, i.e., have nonzero determinant.
<span class="math notranslate nohighlight">\(y=W \mathbf{x}\)</span> with inverse <span class="math notranslate nohighlight">\(x=W^{-1} \mathbf{y}\)</span>. For multidimensional arrays like feature maps in a convolutional network, these linear transformations are usually done per-position, as so-called invertible 1x1 convolutions in the 2d case.</p>
<p>Activation normalization layers perform an affine transformation with learned parameters, e.g., <span class="math notranslate nohighlight">\(y=x\cdot s+t\)</span>, with <span class="math notranslate nohighlight">\(s\)</span> and <span class="math notranslate nohighlight">\(t\)</span> learned scaling and translation parameters. These have also been used to replace batch normalization and initialized data-dependently to maintain unit variance at the beginning of training.</p>
</div>
</div>
<div class="section" id="generative-models-via-maximum-likelihood">
<h2>Generative models via maximum likelihood<a class="headerlink" href="#generative-models-via-maximum-likelihood" title="Permalink to this heading">#</a></h2>
<p>Invertible networks can also be trained as generative models via maximum likelihood. In maximum likelihood training, the network is optimized to maximize the probability densities of the training inputs. For that, the network maps the inputs into a latent space such that the probability densities are maximized under a predefined prior within this latent space, e.g., a Gaussian distribution. In addition, for these probability densities to form a valid probability distribution in the input space, one has to account for how much the networkâ€™s mapping function squeezes and expands volume. Weâ€™ll proceed to illustrate this in an example below.</p>
<p><span class="math notranslate nohighlight">\(p(x) = p_\textrm{prior}(f(x)) \cdot  | \det \left( \frac{\partial \mathbf{f}}{\partial \mathbf{x}} \right)|\)</span></p>
<ul class="simple">
<li><p>figure for that (map samples to latent space, shift achagne volume, latent dist just 3 bars, invert show how dist looks in input soace, what it integrates to</p></li>
<li><p>etc.</p></li>
</ul>
</div>
<div class="section" id="generative-classifiers">
<h2>Generative classifiers<a class="headerlink" href="#generative-classifiers" title="Permalink to this heading">#</a></h2>
<p>Invertible networks trained as class-conditional generative models can also be used as classifiers. Class-conditional generative networks may be implemented in different ways, for example with a separate prior in latent space for each class. Given the class-conditional probability densities <span class="math notranslate nohighlight">\(p(x|c_i)\)</span>, one can obtain class probabilities via Bayes formula as <span class="math notranslate nohighlight">\(p(c_i|x)=\frac{p(x|c_i)}{\sum_jp(x|c_j)}\)</span>.</p>
<p>Pure class-conditional generative training may not yield networks that perform well as classifiers. The reason is that at least in theory, the relative reductions in maximum likelihood loss one obtains from knowing the class label are very small for high-dimensional inputs, for example much smaller than typical differences between two runs of the same network [REF]. This is understandable from a compression perspective, so using that under Shannonâ€™s theorem more probable inputs need less bits to encode than less likely inputs, or more precisely <span class="math notranslate nohighlight">\(\textrm{Number of bits needed}(x) = \log_2 p(x)\)</span>. Even if you assume some one needs only 1 bit per dimension, a high-dimensional input like our 2688-dimensional EEG signal will need 2688 bits to encode. How many bits are needed for the class information? To distinguish between n classes, one needs only <span class="math notranslate nohighlight">\(\log_2(n)\)</span> bits, so in case of binary pathology classification, only 1 bit is needed, therefore the optimal class-conditional model will only be 1 bit better than the optimal class-independent model. In contrast, the loss difference between two training runs of the same network will typically be at least 1 to two orders of magnitude larger. In practice, the gains from using a class-conditional model, by e.g., using a separate prior per class in latent space, are usually larger, but it is not a priori clear if the reductions in loss from exploiting the class label are high enough to result in a good classification model.</p>
<p>Various methods have been proposed to improve the performance of using generative classifiers. For example, people have fixed the per-class latent gaussian priors so that they retain the same distance throughout training [Ref Pavel] or added a classification loss term <span class="math notranslate nohighlight">\(L_class(x)=\log_2 (...etc softmax) \)</span> to the training [Ref VIB heidelberg]. In our work, we experimented with adding a classification loss term to the training, and also found using a learned temperature before the softmax helps the training.</p>
</div>
<div class="section" id="invertible-network-for-eeg-decoding">
<h2>Invertible Network for EEG Decoding<a class="headerlink" href="#invertible-network-for-eeg-decoding" title="Permalink to this heading">#</a></h2>
<p>We designed an invertible network for EEG Decoding using invertible components used in the literature, primarily from the Glow architecture [REF]. Our architecture consists of three stages that operate on sequentially lower temporal resolutions. Similar to glow, the individual stages consists of several blocks of Activation Normalization, Invertible Linear Channel Transformations and Coupling Layers. Between each stage, we downsample by computing the mean and difference of two neighbouring timepoints and moving these into the channel dimension. Unlike Glow, we keep processing all dimensions throughout all stages, finding this architecture to reach competitive accuracy on pathology decoding.</p>
<p>[diagram]</p>
<p>Training and dataset details</p>
<p>Prototypes</p>
<p>Per-Chan Prototypes</p>
<p>EEG CosNet</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="CroppedTraining.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Cropped Training</p>
      </div>
    </a>
    <a class="right-next"
       href="PerturbationVisualization.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Perturbation Visualization</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#background">Background</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#invertible-layers">Invertible layers</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#generative-models-via-maximum-likelihood">Generative models via maximum likelihood</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#generative-classifiers">Generative classifiers</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#invertible-network-for-eeg-decoding">Invertible Network for EEG Decoding</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By The Jupyter Book community
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      Â© Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>