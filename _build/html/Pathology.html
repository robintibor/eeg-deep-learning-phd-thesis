

<!DOCTYPE html>


<html lang="en" data-theme="light">

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Decoding Pathology &#8212; Deep Learning for Brain-Signal Decoding from Electroencephalography (EEG)</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "light";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script async="async" kind="hypothesis" src="https://hypothes.is/embed.js"></script>
    <script kind="utterances">

    var commentsRunWhenDOMLoaded = cb => {
    if (document.readyState != 'loading') {
        cb()
    } else if (document.addEventListener) {
        document.addEventListener('DOMContentLoaded', cb)
    } else {
        document.attachEvent('onreadystatechange', function() {
        if (document.readyState == 'complete') cb()
        })
    }
}

var addUtterances = () => {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src = "https://utteranc.es/client.js";
    script.async = "async";

    script.setAttribute("repo", "robintibor/eeg-deep-learning-phd-thesis");
    script.setAttribute("issue-term", "pathname");
    script.setAttribute("theme", "github-light");
    script.setAttribute("label", "ðŸ’¬ comment");
    script.setAttribute("crossorigin", "anonymous");

    sections = document.querySelectorAll("div.section");
    if (sections !== null) {
        section = sections[sections.length-1];
        section.appendChild(script);
    }
}
commentsRunWhenDOMLoaded(addUtterances);
</script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'Pathology';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Understanding Pathology Decoding" href="UnderstandingPathology.html" />
    <link rel="prev" title="Generalization to Other Tasks" href="TaskDecoding.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="light">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="Abstract.html">
  
  
  
  
    
    
    
    <img src="_static/braindecode-logo.svg" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="_static/braindecode-logo.svg" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="Introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="PriorWork.html">Prior Work</a></li>
<li class="toctree-l1"><a class="reference internal" href="FBCSPAndFBCSPNet.html">Filter Bank Common Spatial Patterns and Filterbank Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="DeepArchitectures.html">Neural Network Architectures for EEG-Decoding</a></li>
<li class="toctree-l1"><a class="reference internal" href="CroppedTraining.html">Cropped Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="Invertible.html">Invertible Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="PerturbationVisualization.html">Perturbation Visualization</a></li>
<li class="toctree-l1"><a class="reference internal" href="MovementDecoding.html">Decoding Movement-Related Brain Activity</a></li>
<li class="toctree-l1"><a class="reference internal" href="TaskDecoding.html">Generalization to Other Tasks</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Decoding Pathology</a></li>
<li class="toctree-l1"><a class="reference internal" href="UnderstandingPathology.html">Understanding Pathology Decoding</a></li>
<li class="toctree-l1"><a class="reference internal" href="FutureWork.html">Future Work</a></li>
<li class="toctree-l1"><a class="reference internal" href="References.html">References</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/Pathology.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Decoding Pathology</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset-and-preprocessing">Dataset and Preprocessing</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#temple-university-hospital-eeg-abnormal-corpus">Temple University Hospital EEG Abnormal Corpus</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#preprocessing">Preprocessing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#decoding-from-reduced-eeg-time-segments">Decoding from reduced EEG time segments</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#network-architectures">Network architectures</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#network-training">Network training</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#automatic-architecture-optimization">Automatic architecture optimization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-and-shallow-convnets-reached-state-of-the-art-results">Deep and shallow ConvNets reached state-of-the-art results</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#architecture-optimization-yielded-unexpected-new-models">Architecture optimization yielded unexpected new models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#visualization">Visualization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#analysis-of-word-frequencies-in-the-medical-reports">Analysis of word frequencies in the medical reports</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#discussion">Discussion</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  
<style>
  .ss-layout-default-AB { grid-template-areas: 'A B'; }
</style>
<div class="tex2jax_ignore mathjax_ignore section" id="decoding-pathology">
<span id="pathology"></span><h1>Decoding Pathology<a class="headerlink" href="#decoding-pathology" title="Permalink to this heading">#</a></h1>
<div class="admonition-convnets-diagnose-pathology-with-good-accuracy-even-from-very-short-amounts-of-time admonition">
<p class="admonition-title">ConvNets diagnose pathology with good accuracy even from very short amounts of time</p>
<ul class="simple">
<li><p>ConvNets reach around 85% accuracy</p></li>
<li><p>Can reach high accuracies using a single minute per recording during inference</p></li>
<li><p>Struggle with recordings where contextual features like age and sleep affect the doctors diagnosis</p></li>
<li><p>Seem to learn temporal slowing indicates pathology, strong occipital alpha indicates healthy</p></li>
</ul>
</div>
<p>We also evaluated our deep ConvNets for automatic medical diagnosis from EEG. EEG is important in clinical practice both as a screening method as well as for hypothesis-based diagnostics, e.g., in epilepsy or stroke. One of the main limitations of using EEG for diagnostics is the required time and specialized knowledge of experts that need to be well-trained on EEG diagnostics to reach reliable results. Therefore, a deep-learning approach that aids in the diagnostic process could make EEG diagnosis more widely accessible, reduce time and effort for clinicians and potentially make diagnoses more accurate. Text and figures in this section are adapted from <span id="id1">[<a class="reference internal" href="References.html#id33" title="R. Schirrmeister, L. Gemein, K. Eggensperger, F. Hutter, and T. Ball. Deep learning with convolutional neural networks for decoding and visualization of eeg pathology. In 2017 IEEE Signal Processing in Medicine and Biology Symposium (SPMB), volume, 1-7. 2017. doi:10.1109/SPMB.2017.8257015.">Schirrmeister <em>et al.</em>, 2017</a>]</span>.</p>
<div class="section" id="dataset-and-preprocessing">
<h2>Dataset and Preprocessing<a class="headerlink" href="#dataset-and-preprocessing" title="Permalink to this heading">#</a></h2>
<div class="section" id="temple-university-hospital-eeg-abnormal-corpus">
<h3>Temple University Hospital EEG Abnormal Corpus<a class="headerlink" href="#temple-university-hospital-eeg-abnormal-corpus" title="Permalink to this heading">#</a></h3>
<table class="colwidths-auto table" id="table-tuh-dataset">
<caption><span class="caption-number">Table 17 </span><span class="caption-text">TUH EEG Abnormal Corpus 1.1.2 Statistics. Obtained from <a class="reference external" href="https://www.isip.piconepress.com/projects/tuh_eeg/">https://www.isip.piconepress.com/projects/tuh_eeg/</a>. Rater agreements refer to the agreement between the student annotator of the file and the medical report written by a certified neurologist.</span><a class="headerlink" href="#table-tuh-dataset" title="Permalink to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p></p></th>
<th class="head"><p>Files</p></th>
<th class="head"><p>Patients</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Train</p></td>
<td><p>Normal</p></td>
<td><p>1379 (50%)</p></td>
<td><p>1238 (58%)</p></td>
</tr>
<tr class="row-odd"><td><p></p></td>
<td><p>Pathological</p></td>
<td><p>1361(50%)</p></td>
<td><p>894 (42%)</p></td>
</tr>
<tr class="row-even"><td><p></p></td>
<td><p>Rater Agreement</p></td>
<td><p>2704 (99%)</p></td>
<td><p>2107 (97%)</p></td>
</tr>
<tr class="row-odd"><td><p></p></td>
<td><p>Rater Disagreement</p></td>
<td><p>36 (1%)</p></td>
<td><p>25 (0%)</p></td>
</tr>
<tr class="row-even"><td><p>Evaluation</p></td>
<td><p>Normal</p></td>
<td><p>150 (54%)</p></td>
<td><p>148 (58%)</p></td>
</tr>
<tr class="row-odd"><td><p></p></td>
<td><p>Pathological</p></td>
<td><p>127 (46%)</p></td>
<td><p>105 (42%)</p></td>
</tr>
<tr class="row-even"><td><p></p></td>
<td><p>Rater Agreement</p></td>
<td><p>277 (100%)</p></td>
<td><p>253 (100%)</p></td>
</tr>
<tr class="row-odd"><td><p></p></td>
<td><p>Rater Disagreement</p></td>
<td><p>0 (0%)</p></td>
<td><p>0 (0%)</p></td>
</tr>
</tbody>
</table>
<p>We used the Temple University Hospital (TUH) EEG Abnormal Corpus for evaluating our deep ConvNets on pathology detection from EEG. The Temple University Hospital (TUH) EEG Abnormal Corpus 1.1.2 is a dataset of manually labeled normal and pathological clinical EEG recordings. It is taken from the TUH EEG Data Corpus which contains over 16000 clinical recordings of more than 10000 subjects from over 12 years <span id="id2">[<a class="reference internal" href="References.html#id34" title="Iyad Obeid and Joseph Picone. The temple university hospital eeg data corpus. Frontiers in Neuroscience, 2016. URL: https://www.frontiersin.org/articles/10.3389/fnins.2016.00196, doi:10.3389/fnins.2016.00196.">Obeid and Picone, 2016</a>]</span>. The Abnormal Corpus contains 3017 recordings, 1529 of which were labeled normal and 1488 of which were labeled pathological. The Corpus was split into a training and evaluation set, see <a class="reference internal" href="#table-tuh-dataset"><span class="std std-numref">Table 17</span></a>. Recordings were acquired from  at least 21 standard electrode positions and with a sampling rate of in most cases 250 Hz. Per recording, there are around 20 minutes of EEG data. The inter-rater agreement on between the medical report of a certified neurologist and a medical student annotator was 99% for the training recordings and 100% for the evaluation recordings, also see <a class="reference internal" href="#table-tuh-dataset"><span class="std std-numref">Table 17</span></a>.</p>
</div>
<div class="section" id="preprocessing">
<h3>Preprocessing<a class="headerlink" href="#preprocessing" title="Permalink to this heading">#</a></h3>
<p>We minimally preprocessed the data with these steps:</p>
<ol class="arabic simple">
<li><p>Select a subset of 21 electrodes present in all recordings.</p></li>
<li><p>Remove the first minute of each recording as it contained stronger artifacts.</p></li>
<li><p>Use only up to 20 minutes of the remaining recording to speed up the computations.</p></li>
<li><p>Clip the amplitude values to the range of <span class="math notranslate nohighlight">\(\pm800\)</span> <span class="math notranslate nohighlight">\(\mu V\)</span> to reduce the effects of strong artifacts.</p></li>
<li><p>Resample the data to 100 Hz to further speed up the computation.</p></li>
</ol>
</div>
<div class="section" id="decoding-from-reduced-eeg-time-segments">
<h3>Decoding from reduced EEG time segments<a class="headerlink" href="#decoding-from-reduced-eeg-time-segments" title="Permalink to this heading">#</a></h3>
<p>We also evaluated the ConvNets on reduced versions of the datasets, using only the first 1, 2, 4, 8, or 16 minutes after the first minute of the recording (the first minute of the recordings was always excluded because it appeared to be more prone to artifact contamination than the later time windows).
We reduced either only the training data, only the test data, or both.
These analyses were carried out to study how long EEG recordings need to be for training and for predicting EEG pathologies with good accuracies.</p>
</div>
</div>
<div class="section" id="network-architectures">
<h2>Network architectures<a class="headerlink" href="#network-architectures" title="Permalink to this heading">#</a></h2>
<p>We used our deep and shallow ConvNets with only minor modifications to the architecture. To use larger time windows to make a single prediction, we adapted the architectures by changing the final layer kernel length so the ConvNets have an input length of about 600 input samples, which correspond to 6 seconds for the 100 Hz EEG input. Additionally, we moved the pooling strides of the deep ConvNet to the convolutional layers directly before each pooling. This modification, which we initially considered a mistake, allowed us to grow the ConvNet input length without strongly increased computation times and
provided good accuracies in preliminary experiments on the training data; therefore we decided to keep it.</p>
</div>
<div class="section" id="network-training">
<h2>Network training<a class="headerlink" href="#network-training" title="Permalink to this heading">#</a></h2>
<p>As in other studies, we optimized the ConvNet parameters using stochastic gradient descent with the optimizer Adam  <span id="id3">[<a class="reference internal" href="References.html#id169" title="Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In Proceedings of the 3rd International Conference on Learning Representations (ICLR). 2014. URL: http://arxiv.org/abs/1412.6980 (visited on 2017-01-09).">Kingma and Ba, 2014</a>]</span>. To make best use of the available
data, we trained the ConvNets on maximally overlapping time crops using cropped training as described in <a class="reference internal" href="CroppedTraining.html#cropped-training"><span class="std std-ref">Cropped Training</span></a>. Code to reproduce the results of this study is available under <a class="github reference external" href="https://github.com/robintibor/auto-eeg-diagnosis-example">robintibor/auto-eeg-diagnosis-example</a>.</p>
</div>
<div class="section" id="automatic-architecture-optimization">
<h2>Automatic architecture optimization<a class="headerlink" href="#automatic-architecture-optimization" title="Permalink to this heading">#</a></h2>
<p>We also carried out a preliminary study of automatic architecture optimization to further improve our ConvNet architectures. To that end, we used the automatic hyperparameter optimization algorithm SMAC \cite{hutter_sequential_2011} to optimize architecture hyperparameters of the deep and shallow ConvNets, such as filter lengths, strides and types of nonlinearities. As the objective function to optimize via SMAC, we used 10-fold cross-validation performance obtained on the first 1500 recordings of the training data (using each fold as an instance for SMAC to speed up the optimization).
We set a time limit of 3.5 hours for each configuration run on a single fold. Runs that timed out or crashed (e.g., networks configurations that did not fit in GPU memory) were scored with an accuracy of 0%.</p>
</div>
<div class="section" id="deep-and-shallow-convnets-reached-state-of-the-art-results">
<h2>Deep and shallow ConvNets reached state-of-the-art results<a class="headerlink" href="#deep-and-shallow-convnets-reached-state-of-the-art-results" title="Permalink to this heading">#</a></h2>
<table class="colwidths-auto table" id="pathology-convnet-results">
<caption><span class="caption-number">Table 18 </span><span class="caption-text">Decoding accuracies for discriminating normal and pathological EEG with deep and shallow ConvNets. For deep and shallow ConvNets, mean over five independent runs with different random seeds. Deep and shallow ConvNet outperformed the feature-based deep learning baseline.</span><a class="headerlink" href="#pathology-convnet-results" title="Permalink to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>Accuracy</p></th>
<th class="head"><p>Sensitivity</p></th>
<th class="head"><p>Specificity</p></th>
<th class="head"><p>Crop-accuracy</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Baseline <span id="id4">[<a class="reference internal" href="References.html#id32" title="S. Lopez de Diego. Automated interpretation of abnormal adult electroencephalography. Master's thesis, Temple University, 2017.">LopezÂ de Diego, 2017</a>]</span></p></td>
<td><p>78.8</p></td>
<td><p>75.4</p></td>
<td><p>81.9</p></td>
<td><p>n.a.</p></td>
</tr>
<tr class="row-odd"><td><p>Deep</p></td>
<td><p>85.4</p></td>
<td><p>75.1</p></td>
<td><p>94.1</p></td>
<td><p>82.5</p></td>
</tr>
<tr class="row-even"><td><p>Shallow</p></td>
<td><p>84.5</p></td>
<td><p>77.3</p></td>
<td><p>90.5</p></td>
<td><p>81.7</p></td>
</tr>
<tr class="row-odd"><td><p>Linear</p></td>
<td><p>51.4</p></td>
<td><p>20.9</p></td>
<td><p>77.3</p></td>
<td><p>50.2</p></td>
</tr>
<tr class="row-even"><td><p><span id="id5">[<a class="reference internal" href="References.html#id33" title="R. Schirrmeister, L. Gemein, K. Eggensperger, F. Hutter, and T. Ball. Deep learning with convolutional neural networks for decoding and visualization of eeg pathology. In 2017 IEEE Signal Processing in Medicine and Biology Symposium (SPMB), volume, 1-7. 2017. doi:10.1109/SPMB.2017.8257015.">Schirrmeister <em>et al.</em>, 2017</a>]</span></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p>Note that the baseline was evaluated on an older version of the corpus that has since been corrected to not contain the same patient in training and test recordings among other things.</p>
</div>
<p>Both the deep and the shallow ConvNet outperformed the only results that had been published on the TUH Abnormal EEG Corpus at the time (see <a class="reference internal" href="#pathology-convnet-results"><span class="std std-numref">Table 18</span></a>). Both ConvNets were more than 5% better than the baseline method of a convolutional network that included multiple fully connected layers at the end and took precomputed EEG features of an entire recording as one input <span id="id6">[<a class="reference internal" href="References.html#id32" title="S. Lopez de Diego. Automated interpretation of abnormal adult electroencephalography. Master's thesis, Temple University, 2017.">LopezÂ de Diego, 2017</a>]</span>.The ConvNets as applied here reduced the error rate from about 21% to about 15%. We also tested a linear classifier on the same 6-second inputs as our ConvNets. The linear classifier did not reach accuracies substantially different from chance (51.4%).</p>
<p>Interestingly, both of our ConvNet architectures already reached higher accuracies than the baseline when evaluating single predictions from 6-second crops. The average per-crop accuracy of individual predictions was only about 3% lower than average per-recording accuracy (averaged predictions of all crops in a recording). Furthermore, the individual prediction accuracies were already about 3% higher than the per-recording accuracies of the baseline.
This implies that predictions with high accuracies can be made from just 6 seconds of EEG data.</p>
<div class="sphinx-subfigure figure align-default" id="conf-mat-pathology-fig">
<div class="sphinx-subfigure-grid ss-layout-default-AB outline" style="display: grid; gap: 0px; grid-gap: 0px;">
<div class="sphinx-subfigure-area" style="display: flex; flex-direction: column; justify-content: center; align-items: center; grid-area: A;">
<a class="reference internal image-reference" href="_images/ConfMatDeep.pdf-1.png"><img alt="Confusion Matrix Deep ConvNet." src="_images/ConfMatDeep.pdf-1.png" style="width: 73%;" /></a>
<span class="caption">Confusion Matrix Deep ConvNet.</span>
</div>
<div class="sphinx-subfigure-area" style="display: flex; flex-direction: column; justify-content: center; align-items: center; grid-area: B;">
<img alt="Confusion Matrix Shallow ConvNet." src="_images/ConfMatShallow.pdf-1.png" />
<span class="caption">Confusion Matrix Shallow ConvNet.</span>
</div>
</div>
<p class="caption"><span class="caption-number">Fig. 34 </span><span class="caption-text">Confusion Matrices for deep and shallow ConvNets, summed over five independent runs <span id="id7">[<a class="reference internal" href="References.html#id33" title="R. Schirrmeister, L. Gemein, K. Eggensperger, F. Hutter, and T. Ball. Deep learning with convolutional neural networks for decoding and visualization of eeg pathology. In 2017 IEEE Signal Processing in Medicine and Biology Symposium (SPMB), volume, 1-7. 2017. doi:10.1109/SPMB.2017.8257015.">Schirrmeister <em>et al.</em>, 2017</a>]</span>.
Each entry of row r and column c for upper-left 2x2-square: Number of trials of target r predicted as class c (also written in percent of all trials).
Bold diagonal corresponds to correctly predicted trials for both classes. Percentages and colors indicate fraction of trials in each cell relative to all trials.
The lower-right value: overall accuracy. The first two values in the bottom row correspond to sensitivity and specificity.
Rightmost column corresponds to precision defined as the number of trials correctly predicted for class r/number of trials predicted as class r.</span><a class="headerlink" href="#conf-mat-pathology-fig" title="Permalink to this image">#</a></p>
</div>
<p>Both of our ConvNets made more errors on the pathological recordings, as can be seen from <a class="reference internal" href="#conf-mat-pathology-fig"><span class="std std-numref">Fig. 34</span></a>. Both ConvNets reached a specificity of above 90% and a sensitivity of about 75-78%. Confusion matrices between both approaches were very similar. Relative to the baseline, they reached a similar sensitivity (0.3% smaller for the deep ConvNet, 1.9% higher for the shallow ConvNet), and a higher specificity (12.2% higher for the deep ConvNet and 8.6% higher for the shallow ConvNet).</p>
<div class="figure align-default" id="pathology-time-fig">
<a class="reference internal image-reference" href="_images/Time_Plot.pdf-1.png"><img alt="_images/Time_Plot.pdf-1.png" src="_images/Time_Plot.pdf-1.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 35 </span><span class="caption-text">Results on reduced datasets for deep ConvNet. Train and/or test (evaluation) dataset was reduced from 20 minutes per recording to 1,2,4,8, or 16 minutes per recording, results are shown on the test set. Notably, when only reducing the duration of the test set recordings, maximal accuracies were observed  when using just 1 minute. We note that these results are each based on one run only; the slightly better performance than in Table \ref{tab:main-results} may thus be due to noise.</span><a class="headerlink" href="#pathology-time-fig" title="Permalink to this image">#</a></p>
</div>
<div class="figure align-default" id="time-crop-pred-fig">
<a class="reference internal image-reference" href="_images/Time_Crop_Pred_Plot.pdf-1.png"><img alt="_images/Time_Crop_Pred_Plot.pdf-1.png" src="_images/Time_Crop_Pred_Plot.pdf-1.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 36 </span><span class="caption-text">Moving average of cropwise accuracies for the deep ConvNet. 5-minute moving averages of the cropwise accuracies of the deep ConvNet, averaged over all test set recordings. Dashed lines represent 5 individual training runs with different random seeds, solid black line represents mean over results for these runs. x-axis shows center of 5-minute averaging window.</span><a class="headerlink" href="#time-crop-pred-fig" title="Permalink to this image">#</a></p>
</div>
<p>Deep ConvNets already reached their best trialwise accuracies with only one minute of data used for the prediction. While the reduction of the amount of length  of the training data led to crop- and trialwise accuracy decreases on the test data, reductions in the test data did not have such an effect (see <a class="reference internal" href="#pathology-time-fig"><span class="std std-numref">Fig. 35</span></a>). Remarkably, both crop- and trialwise accuracies slightly decreased when going from 1 minute to 2 or 4 minutes of test data. To investigate whether earlier parts of the recordings might be more informative, we also computed a 5-minute moving average of the cropwise accuracies on the test data for the Deep ConvNet trained on the full data. We show the average over all recordings for these moving averages in (see <a class="reference internal" href="#time-crop-pred-fig"><span class="std std-numref">Fig. 36</span></a>). Noticeably, as expected, accuracies slightly decreased with increasing recording time. However, the decrease is below 0.5% and thus should be interpreted cautiously.</p>
</div>
<div class="section" id="architecture-optimization-yielded-unexpected-new-models">
<h2>Architecture optimization yielded unexpected new models<a class="headerlink" href="#architecture-optimization-yielded-unexpected-new-models" title="Permalink to this heading">#</a></h2>
<div class="figure align-default" id="shallow-smac-net-fig">
<a class="reference internal image-reference" href="_images/ShallowSmacNet.pdf-1.png"><img alt="_images/ShallowSmacNet.pdf-1.png" src="_images/ShallowSmacNet.pdf-1.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 37 </span><span class="caption-text">Final shallow ConvNet architecture selected by SMAC. Note that max pooling is the only nonlinearity SMAC decided to use.</span><a class="headerlink" href="#shallow-smac-net-fig" title="Permalink to this image">#</a></p>
</div>
<table class="colwidths-auto table" id="pathology-smac-results">
<caption><span class="caption-number">Table 19 </span><span class="caption-text">Decoding accuracies with the default  of deep and shallow ConvNets as well as versions optimized by automatic architecture optimization. Train here refers to 10-fold cross-validation on the 1500 chronologically earliest recordings of the training data.</span><a class="headerlink" href="#pathology-smac-results" title="Permalink to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p></p></th>
<th class="head"><p>Train</p></th>
<th class="head"><p></p></th>
<th class="head"><p>Test</p></th>
<th class="head"><p></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p></p></td>
<td><p>Architecture <br>configuration</p></td>
<td><p>Trial</p></td>
<td><p>Crop</p></td>
<td><p>Trial</p></td>
<td><p>Crop</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Deep</strong></p></td>
<td><p>Default</p></td>
<td><p>84.2</p></td>
<td><p>81.6</p></td>
<td><p>85.4</p></td>
<td><p>82.5</p></td>
</tr>
<tr class="row-even"><td><p></p></td>
<td><p>Optimized</p></td>
<td><p>86.3</p></td>
<td><p>80.9</p></td>
<td><p>84.5</p></td>
<td><p>81.3</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Shallow</strong></p></td>
<td><p>Default</p></td>
<td><p>84.5</p></td>
<td><p>82.1</p></td>
<td><p>84.5</p></td>
<td><p>81.7</p></td>
</tr>
<tr class="row-even"><td><p></p></td>
<td><p>Optimized</p></td>
<td><p>85.9</p></td>
<td><p>80.3</p></td>
<td><p>83.0</p></td>
<td><p>79.8</p></td>
</tr>
<tr class="row-odd"><td><p><span id="id8">[<a class="reference internal" href="References.html#id33" title="R. Schirrmeister, L. Gemein, K. Eggensperger, F. Hutter, and T. Ball. Deep learning with convolutional neural networks for decoding and visualization of eeg pathology. In 2017 IEEE Signal Processing in Medicine and Biology Symposium (SPMB), volume, 1-7. 2017. doi:10.1109/SPMB.2017.8257015.">Schirrmeister <em>et al.</em>, 2017</a>]</span></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<p>The models discovered by automated architecture optimization were markedly different from our original deep and shallow ConvNets. For example, the optimized architectures used only 1.8 and 3.7 seconds of EEG data for the optimized deep and shallow ConvNet, respectively, in contrast to about 6 seconds in the original versions.
While the improved performance of these modified architectures for the 10-fold cross-validation on the training dataset (2.1% and 1.4% improvement for deep and shallow ConvNets, respectively) did not generalize to the evaluation set (0.9% and  1.5% deterioration for deep and shallow ConvNets, respectively, see <a class="reference internal" href="#pathology-smac-results"><span class="std std-numref">Table 19</span></a>, the modifications to the original network architectures already provided interesting insights for further exploration:
For example, in the case of the shallow ConvNet, the modified architecture did not use any of the original nonlinearities, but used max pooling as the only nonlinearity (see Fig. <a class="reference internal" href="#shallow-smac-net-fig"><span class="std std-numref">Fig. 37</span></a>), a configuration we had not considered in our  manual search so far.</p>
</div>
<div class="section" id="visualization">
<h2>Visualization<a class="headerlink" href="#visualization" title="Permalink to this heading">#</a></h2>
<p>We analyzed the spectral power changes in the data itself and the spectral characteristics of the function the deep networks learned on the data.</p>
<p>To understand class-specific spectral characteristics in the EEG recordings, we analyzed band powers in five frequency ranges: delta (0â€“4 Hz), theta (4â€“8 Hz), alpha (8â€“14 Hz), low beta (14â€“20 Hz), high beta (20â€“30 Hz) and low gamma (30â€“50 Hz).</p>
<p>For this, we performed the following steps:</p>
<ol class="arabic simple">
<li><p>Compute a short-term Fourier transformation with window size 12 seconds and overlap 6 seconds using a Blackman-Harris window.</p></li>
<li><p>Compute the median over all band powers of all windows and recordings in each frequency bin; independently for pathological and normal recordings.</p></li>
<li><p>Compute the log ratio of these median band powers of the pathological and normal recordings.</p></li>
<li><p>Compute the mean log ratio over all frequency bins in each desired frequency range for each electrode.</p></li>
<li><p>Visualize the resulting log ratios as a topographical map.</p></li>
</ol>
<p>To better understand the spectral characteristics of the function the ConvNets learned used in this study, we also used the perturbation-based visualization method described in <span id="id9">[<a class="reference internal" href="References.html#id36" title="Robin Tibor Schirrmeister, Jost Tobias Springenberg, Lukas Dominique Josef Fiederer, Martin Glasstetter, Katharina Eggensperger, Michael Tangermann, Frank Hutter, Wolfram Burgard, and Tonio Ball. Deep learning with convolutional neural networks for eeg decoding and visualization. Human Brain Mapping, aug 2017. URL: http://dx.doi.org/10.1002/hbm.23730, doi:10.1002/hbm.23730.">Schirrmeister <em>et al.</em>, 2017</a>]</span>.</p>
<div class="figure align-default" id="bandpower-pathology-fig">
<a class="reference internal image-reference" href="_images/Bandpower.pdf-1.png"><img alt="_images/Bandpower.pdf-1.png" src="_images/Bandpower.pdf-1.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 38 </span><span class="caption-text">Pathological vs. normal relative spectral bandpower differences for the training set. Shown is the logarithm of the ratio of the median bandpower of the pathological  vs. normal (according to the expertsâ€™ ratings) EEG recordings.</span><a class="headerlink" href="#bandpower-pathology-fig" title="Permalink to this image">#</a></p>
</div>
<div class="figure align-default" id="perturbation-deep-pathology-fig">
<a class="reference internal image-reference" href="_images/PerturbationDeep.pdf-1.png"><img alt="_images/PerturbationDeep.pdf-1.png" src="_images/PerturbationDeep.pdf-1.png" style="width: 100%;" /></a>
</div>
<div class="figure align-default" id="perturbation-shallow-pathology-fig">
<a class="reference internal image-reference" href="_images/PerturbationShallow.pdf-1.png"><img alt="_images/PerturbationShallow.pdf-1.png" src="_images/PerturbationShallow.pdf-1.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 39 </span><span class="caption-text">Input-perturbation network-prediction correlation maps for the deep (top) and shallow (bottom) ConvNet. Correlation of predictions for the pathological class with amplitude perturbations. Scalp maps revealed for example a bilateral positive correlation for the delta and theta frequency ranges and a spatially more broadly distributed negative correlation for the beta and low gamma frequency ranges, indicating that the ConvNets used these frequency components in their decisions</span><a class="headerlink" href="#perturbation-shallow-pathology-fig" title="Permalink to this image">#</a></p>
</div>
<p>Power was broadly increased for the the pathological class in the low frequency bands (delta and theta range) and decreased in the beta and low gamma ranges (<a class="reference internal" href="#bandpower-pathology-fig"><span class="std std-numref">Fig. 38</span></a>).
Alpha power was decreased for the occipital electrodes and increased for more frontal electrodes.</p>
<p>Scalp maps of the input-perturbation effects on predictions for the pathological class for the different frequency bands showed effects consistent with the power spectra in (<a class="reference internal" href="#perturbation-shallow-pathology-fig"><span class="std std-numref">Fig. 39</span></a>).
Both networks strongly relied on the lower frequencies in the delta and theta frequency range for their decoding decisions.</p>
</div>
<div class="section" id="analysis-of-word-frequencies-in-the-medical-reports">
<h2>Analysis of word frequencies in the medical reports<a class="headerlink" href="#analysis-of-word-frequencies-in-the-medical-reports" title="Permalink to this heading">#</a></h2>
<p>Furthermore, to better understand what kind of recordings are easier or harder for the ConvNets to correctly decode, we analyzed the textual clinical reports of each recording as included in the TUH Abnormal EEG Corpus. %todecide remove last part?
Specifically, we investigated which words were relatively more or less frequent in the incorrectly compared with the correctly predicted recordings.
We performed this analysis independently for both the normal and the pathological class of recordings.
Concretely, for each class, we first computed the relative frequencies <span class="math notranslate nohighlight">\(f_{i-}\)</span> for each word <span class="math notranslate nohighlight">\(w_{i-}\)</span> in the incorrectly predicted recordings, i.e.:
<span class="math notranslate nohighlight">\(f_{i-} = \frac{|w_{i-}|}{\sum_{i}|w_{i-}|}\)</span>, where <span class="math notranslate nohighlight">\(|w_{i-}|\)</span> denotes the number of occurrences for word <span class="math notranslate nohighlight">\(w_i\)</span> in the incorrectly predicted recordings.
We then computed the frequencies <span class="math notranslate nohighlight">\(f_{i+}\)</span> in the same way and computed the ratios <span class="math notranslate nohighlight">\(r_i=f_{i-}/f_{i+}\)</span>.
Finally, we analyzed words with very large ratios (<span class="math notranslate nohighlight">\(\gg1\)</span>) and very small ratios (<span class="math notranslate nohighlight">\(\ll1\)</span>) by inspecting the contexts of their occurrences in the  clinical reports.
This allowed us to gain insights into which clinical/contextual aspects of the recordings correlated with ConvNets failures.</p>
<p>Most notably, <code class="docutils literal notranslate"><span class="pre">small</span></code> and <code class="docutils literal notranslate"><span class="pre">amount</span></code> had a much larger word frequency (15.5 times larger) in the incorrectly predicted pathological recordings compared with the correctly predicted pathological recordings.
Closer inspection showed this is very sensible, as <code class="docutils literal notranslate"><span class="pre">small</span> <span class="pre">amount</span></code> was often used to describe more subtle EEG abnormalities (<code class="docutils literal notranslate"><span class="pre">small</span> <span class="pre">amount</span> <span class="pre">of</span> <span class="pre">temporal</span> <span class="pre">slowing</span></code>,  <code class="docutils literal notranslate"><span class="pre">Small</span> <span class="pre">amount</span> <span class="pre">of</span> <span class="pre">excess</span> <span class="pre">theta</span></code>,  <code class="docutils literal notranslate"><span class="pre">Small</span> <span class="pre">amount</span> <span class="pre">of</span> <span class="pre">background</span> <span class="pre">disorganization</span></code>,  <code class="docutils literal notranslate"><span class="pre">A</span> <span class="pre">small</span> <span class="pre">amount</span> <span class="pre">of</span> <span class="pre">rhythmic,</span> <span class="pre">frontal</span> <span class="pre">slowing</span></code>), as this subtlety of changes was likely the cause of the classification errors.</p>
<p>Secondly, other words with a notably different frequency were <code class="docutils literal notranslate"><span class="pre">age</span></code> (9.7 times larger) and <code class="docutils literal notranslate"><span class="pre">sleep</span></code> (3 occurrences in 630 words of texts of incorrectly predicted recordings, not present in texts of correctly predicted recordings).
Both typically indicate the clinician used the age of the subject or the fact that they were (partially) asleep during the recording to interpret the EEG (<code class="docutils literal notranslate"><span class="pre">Somewhat</span> <span class="pre">disorganized</span> <span class="pre">pattern</span> <span class="pre">for</span> <span class="pre">age</span></code>,  <code class="docutils literal notranslate"><span class="pre">Greater</span> <span class="pre">than</span> <span class="pre">anticipated</span> <span class="pre">disorganization</span> <span class="pre">for</span> <span class="pre">age.</span></code>,  <code class="docutils literal notranslate"><span class="pre">A</span> <span class="pre">single</span> <span class="pre">generalized</span> <span class="pre">discharge</span> <span class="pre">noted</span> <span class="pre">in</span> <span class="pre">stage</span> <span class="pre">II</span> <span class="pre">sleep.</span></code>).
Obviously, our ConvNets trained only on EEG do not have access to this context information, leaving them at a disadvantage compared to the clinicians and highlighting the potential of including contextual cues such as age or vigilance in the training/decoding approach.</p>
<p>Inspection of the textual records of misclassified normal recordings did not provide much insight, as they are typically very short (e.g., <code class="docutils literal notranslate"><span class="pre">Normal</span> <span class="pre">EEG.</span></code>,  <code class="docutils literal notranslate"><span class="pre">Normal</span> <span class="pre">EEG</span> <span class="pre">in</span> <span class="pre">wakefulness.</span></code>).</p>
<p>Finally, consistent with the strong usage of the delta and theta frequency range by the ConvNets as seen in the input-perturbation network-prediction correlation maps (<a class="reference internal" href="#perturbation-shallow-pathology-fig"><span class="std std-numref">Fig. 39</span></a>), <code class="docutils literal notranslate"><span class="pre">slowing</span></code> and <code class="docutils literal notranslate"><span class="pre">temporal</span></code> are the 6th and 10th most frequently occurring words in the textual reports of the pathological recordings, while never occurring in the textual reports of the normal recordings (irrespective of correct or incorrect predictions).</p>
</div>
<div class="section" id="discussion">
<h2>Discussion<a class="headerlink" href="#discussion" title="Permalink to this heading">#</a></h2>
<p>To the best of our knowledge, the ConvNet architectures used in this study achieved the best accuracies published so far on the TUH EEG Abnormal Corpus.
The architectures used were only very slightly modified versions of ConvNet architectures that we previously introduced to decode task-related information. This suggests that these architectures might be broadly applicable both for physiological and clinical EEG.
The identification of all-round architectures would greatly simplify the application of deep learning to EEG decoding problems and expand their potential use cases.</p>
<p>Remarkably, the ConvNets already reached good accuracies based on very limited time segments of the EEG recordings.
Further accuracy improvements could thus be possible with improved decoding models that can extract and integrate additional information from longer timescales.
The exact nature of such models, as well as the amount of EEG they would require, remains to be determined.
More accurate decoding models could either be ConvNets that are designed to intelligently use a larger input length or recurrent neural networks, since these are known to inherently work well for data with information both on shorter and longer term scales.
Furthermore, combinations between both approaches, for example using a recurrent neural network on top of a ConvNet, as they have been used in other domains like speech recognition <span id="id10">[<a class="reference internal" href="References.html#id203" title="X. Li and X. Wu. Constructing long short-term memory based deep recurrent neural networks for large vocabulary speech recognition. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 4520â€“4524. April 2015. doi:10.1109/ICASSP.2015.7178826.">Li and Wu, 2015</a>, <a class="reference internal" href="References.html#id204" title="T. N. Sainath, O. Vinyals, A. Senior, and H. Sak. Convolutional, Long Short-Term Memory, fully connected Deep Neural Networks. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 4580â€“4584. April 2015. doi:10.1109/ICASSP.2015.7178838.">Sainath <em>et al.</em>, 2015</a>, <a class="reference internal" href="References.html#id220" title="HaÅŸim Sak, Andrew Senior, Kanishka Rao, and FranÃ§oise Beaufays. Fast and Accurate Recurrent Neural Network Acoustic Models for Speech Recognition. In arXiv:1507.06947 [cs, stat]. July 2015. arXiv: 1507.06947. URL: http://arxiv.org/abs/1507.06947 (visited on 2016-12-21).">Sak <em>et al.</em>, 2015</a>]</span>, are promising.</p>
<p>Our automated architecture optimization provided interesting insights by yielding configurations that were markedly different from our hand-engineered architectures, yet reached similar accuracies. Since the marked improvements in training performance did not improve the evaluation accuracies in this study, in future work, we plan to use more training recordings in the optimization and study different cross-validation methods to also improve evaluation accuracies.
A full-blown architecture search <span id="id11">[<a class="reference internal" href="References.html#id28" title="H. Mendoza, A. Klein, M. Feurer, J. Springenberg, and F. Hutter. Towards Automatically-Tuned Neural Networks. In ICML 2016 AutoML Workshop. June 2016.">Mendoza <em>et al.</em>, 2016</a>, <a class="reference internal" href="References.html#id26" title="Risto Miikkulainen, Jason Liang, Elliot Meyerson, Aditya Rawal, Dan Fink, Olivier Francon, Bala Raju, Hormoz Shahrzad, Arshak Navruzyan, Nigel Duffy, and Babak Hodjat. Evolving Deep Neural Networks. arXiv:1703.00548 [cs], March 2017. arXiv: 1703.00548. URL: http://arxiv.org/abs/1703.00548 (visited on 2017-08-26).">Miikkulainen <em>et al.</em>, 2017</a>, <a class="reference internal" href="References.html#id27" title="Esteban Real, Sherry Moore, Andrew Selle, Saurabh Saxena, Yutaka Leon Suematsu, Jie Tan, Quoc Le, and Alex Kurakin. Large-Scale Evolution of Image Classifiers. arXiv:1703.01041 [cs], March 2017. arXiv: 1703.01041. URL: http://arxiv.org/abs/1703.01041 (visited on 2017-08-26).">Real <em>et al.</em>, 2017</a>, <a class="reference internal" href="References.html#id25" title="Barret Zoph and Quoc V. Le. Neural Architecture Search with Reinforcement Learning. arXiv:1611.01578 [cs], November 2016. arXiv: 1611.01578. URL: http://arxiv.org/abs/1611.01578 (visited on 2017-08-26).">Zoph and Le, 2016</a>, <a class="reference internal" href="References.html#id24" title="Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V. Le. Learning Transferable Architectures for Scalable Image Recognition. arXiv:1707.07012 [cs], July 2017. arXiv: 1707.07012. URL: http://arxiv.org/abs/1707.07012 (visited on 2017-08-26).">Zoph <em>et al.</em>, 2017</a>]</span> could also further improve accuracy. With such improved methods it would also be important not only to decode pathological vs. normal EEG in a binary fashion, but to also evaluate the possibility to derive more fine-grained clinical information, such as the type of pathological change (slowing, asymmetry, etc) or the likely underlying disorder (such as epilepsy).</p>
<p>Any of these or other improvements might eventually bring the machine-learning decoding performance of pathological EEG closer to human-level performance.
Since clinicians make their judgments from patterns they see in the EEG and other available context information, there is no clear reason why machine learning models with access to the same information could not reach human-level accuracy.
This human-level performance is a benchmark for decoding accuracies that does not exist for other brain-signal decoding tasks, e.g. in decoding task-related information for brain-computer interfaces, where there is inherent uncertainty what information is even present in the EEG and no human-level benchmark exists.</p>
<p>Our perturbation visualizations of the ConvNetsâ€™ decoding behavior showed that they used spectral power changes in the delta (0-4 Hz) and theta (4-8 Hz) frequency range, particularly from temporal EEG channels, possibly alongside other features (<a class="reference internal" href="#perturbation-shallow-pathology-fig"><span class="std std-numref">Fig. 39</span></a>). This observation is consistent both with the expectations implied by the spectral analysis of the EEG data (<a class="reference internal" href="#bandpower-pathology-fig"><span class="std std-numref">Fig. 38</span></a>) and by the textual reports that frequently mentioned <code class="docutils literal notranslate"><span class="pre">temporal</span></code>  and <code class="docutils literal notranslate"><span class="pre">slowing</span></code> with respect to the pathological samples, but never in the normal ones.
Our  perturbation visualization showed results that were consistent with expectations that the ConvNets would use the bandpower differences between the classes that were already visible in the spectra to perform their decoding.
Similarly, the textual reports also yielded plausible insights, e.g., that <code class="docutils literal notranslate"><span class="pre">small</span> <span class="pre">amounts</span></code> of abnormalities as indicated in the written clinical reports were more difficult for the networks to decode correctly.
Additionally, inspection of the textual reports also emphasized the importance of  integrating contextual information such as the age of the subject.</p>
<p>Still, to yield more clinically useful insights and diagnosis explanations, further improvements in ConvNet visualizations are needed.
Deep learning models that use an attention mechanism might be more interpretable, since these models can highlight which parts of the recording were most important for the decoding decision.
Other deep learning visualization methods like recent saliency map methods <span id="id12">[<a class="reference internal" href="References.html#id23" title="Pieter-Jan Kindermans, Kristof T. SchÃ¼tt, Maximilian Alber, Klaus-Robert MÃ¼ller, and Sven DÃ¤hne. PatternNet and PatternLRP - Improving the interpretability of neural networks. CoRR, 2017. URL: http://arxiv.org/abs/1705.05598.">Kindermans <em>et al.</em>, 2017</a>, <a class="reference internal" href="References.html#id22" title="GrÃ©goire Montavon, Wojciech Samek, and Klaus-Robert MÃ¼ller. Methods for Interpreting and Understanding Deep Neural Networks. CoRR, 2017. URL: http://arxiv.org/abs/1706.07979.">Montavon <em>et al.</em>, 2017</a>]</span> to explain individual decisions or conditional generative adversarial networks  <span id="id13">[<a class="reference internal" href="References.html#id29" title="Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784, 2014.">Mirza and Osindero, 2014</a>, <a class="reference internal" href="References.html#id31" title="Jost Tobias Springenberg. Unsupervised and semi-supervised learning with categorical generative adversarial networks. arXiv preprint arXiv:1511.06390, 2015.">Springenberg, 2015</a>]</span> to understand what makes a recording pathological or normal might further improve the clinical benefit of deep learning methods that decode pathological EEG.</p>
<div class="tip admonition">
<p class="admonition-title">Open Questions</p>
<ul class="simple">
<li><p>How to further visualize the features networks learn to diagnose pathology?</p></li>
</ul>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="TaskDecoding.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Generalization to Other Tasks</p>
      </div>
    </a>
    <a class="right-next"
       href="UnderstandingPathology.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Understanding Pathology Decoding</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset-and-preprocessing">Dataset and Preprocessing</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#temple-university-hospital-eeg-abnormal-corpus">Temple University Hospital EEG Abnormal Corpus</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#preprocessing">Preprocessing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#decoding-from-reduced-eeg-time-segments">Decoding from reduced EEG time segments</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#network-architectures">Network architectures</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#network-training">Network training</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#automatic-architecture-optimization">Automatic architecture optimization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-and-shallow-convnets-reached-state-of-the-art-results">Deep and shallow ConvNets reached state-of-the-art results</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#architecture-optimization-yielded-unexpected-new-models">Architecture optimization yielded unexpected new models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#visualization">Visualization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#analysis-of-word-frequencies-in-the-medical-reports">Analysis of word frequencies in the medical reports</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#discussion">Discussion</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By The Jupyter Book community
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      Â© Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>