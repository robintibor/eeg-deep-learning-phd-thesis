
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Generalization to Other Tasks &#8212; Deep Learning for Brain-Signal Decoding from Electroencephalography (EEG)</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script async="async" kind="hypothesis" src="https://hypothes.is/embed.js"></script>
    <script kind="utterances">

    var commentsRunWhenDOMLoaded = cb => {
    if (document.readyState != 'loading') {
        cb()
    } else if (document.addEventListener) {
        document.addEventListener('DOMContentLoaded', cb)
    } else {
        document.attachEvent('onreadystatechange', function() {
        if (document.readyState == 'complete') cb()
        })
    }
}

var addUtterances = () => {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src = "https://utteranc.es/client.js";
    script.async = "async";

    script.setAttribute("repo", "robintibor/eeg-deep-learning-phd-thesis");
    script.setAttribute("issue-term", "pathname");
    script.setAttribute("theme", "github-light");
    script.setAttribute("label", "ðŸ’¬ comment");
    script.setAttribute("crossorigin", "anonymous");

    sections = document.querySelectorAll("div.section");
    if (sections !== null) {
        section = sections[sections.length-1];
        section.appendChild(script);
    }
}
commentsRunWhenDOMLoaded(addUtterances);
</script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Decoding Pathology" href="Pathology.html" />
    <link rel="prev" title="Decoding Movement-Related Brain Activity" href="MovementDecoding.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/braindecode-logo.svg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Deep Learning for Brain-Signal Decoding from Electroencephalography (EEG)</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="Abstract.html">
                    Deep Learning for Brain-Signal Decoding from Electroencephalography
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="PriorWork.html">
   Prior Work
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="FBCSPAndFBCSPNet.html">
   Filterbank Common Spatial Patterns and Filterbank Network
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="DeepArchitectures.html">
   Neural Network Architectures for EEG-Decoding
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="CroppedTraining.html">
   Cropped Training
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="PerturbationVisualization.html">
   Perturbation Visualization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="MovementDecoding.html">
   Decoding Movement-Related Brain Activity
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Generalization to Other Tasks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Pathology.html">
   Decoding Pathology
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Invertible.html">
   Invertible Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="FutureWork.html">
   Future Work
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="References.html">
   References
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/TaskDecoding.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/TaskDecoding.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#decoding-different-mental-imageries">
   Decoding different mental imageries
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#decoding-error-related-signals">
   Decoding error-related signals
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#decoding-observation-of-robots-making-errors">
     Decoding Observation of Robots Making Errors
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#decoding-of-eriksen-flanker-task-errors-and-errors-during-online-gui-control">
     Decoding of Eriksen Flanker Task Errors and Errors during Online GUI Control
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#proof-of-concept-assistive-system">
   Proof-of-concept assistive system
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#intracranial-eeg-decoding">
   Intracranial EEG decoding
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#intracranial-eeg-decoding-of-eriksen-flanker-task">
     Intracranial EEG Decoding of Eriksen Flanker Task
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#transfer-learning-for-intracranial-error-decoding">
     Transfer Learning for Intracranial Error Decoding
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#microelectrocorticography-decoding-of-auditory-evoked-responses-in-sheep">
     Microelectrocorticography decoding of auditory evoked responses in sheep
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#evaluation-on-large-scale-task-diverse-dataset">
   Evaluation on large-scale task-diverse dataset
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Generalization to Other Tasks</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#decoding-different-mental-imageries">
   Decoding different mental imageries
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#decoding-error-related-signals">
   Decoding error-related signals
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#decoding-observation-of-robots-making-errors">
     Decoding Observation of Robots Making Errors
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#decoding-of-eriksen-flanker-task-errors-and-errors-during-online-gui-control">
     Decoding of Eriksen Flanker Task Errors and Errors during Online GUI Control
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#proof-of-concept-assistive-system">
   Proof-of-concept assistive system
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#intracranial-eeg-decoding">
   Intracranial EEG decoding
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#intracranial-eeg-decoding-of-eriksen-flanker-task">
     Intracranial EEG Decoding of Eriksen Flanker Task
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#transfer-learning-for-intracranial-error-decoding">
     Transfer Learning for Intracranial Error Decoding
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#microelectrocorticography-decoding-of-auditory-evoked-responses-in-sheep">
     Microelectrocorticography decoding of auditory evoked responses in sheep
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#evaluation-on-large-scale-task-diverse-dataset">
   Evaluation on large-scale task-diverse dataset
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="generalization-to-other-tasks">
<span id="task-related"></span><h1>Generalization to Other Tasks<a class="headerlink" href="#generalization-to-other-tasks" title="Permalink to this headline">#</a></h1>
<p>After our initial work designing and evaluating convolutional neural networks for movement decoding from EEG, we evaluated the resulting networks on a wide variety of other EEG decoding tasks, finding that they generalize well to a large number of settings.</p>
<div class="section" id="decoding-different-mental-imageries">
<h2>Decoding different mental imageries<a class="headerlink" href="#decoding-different-mental-imageries" title="Permalink to this headline">#</a></h2>
<table class="colwidths-auto table" id="mixed-imagery-dataset-results">
<caption><span class="caption-number">Table 9 </span><span class="caption-text">Accuracies on the Mixed-Imagery dataset. ConvNet accuracies show the difference to the FBCSP accuracy.</span><a class="headerlink" href="#mixed-imagery-dataset-results" title="Permalink to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>FBCSP</p></th>
<th class="head"><p>Deep ConvNet</p></th>
<th class="head"><p>Shallow ConvNet</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>71.2</p></td>
<td><p>+1.0</p></td>
<td><p>-3.5</p></td>
</tr>
<tr class="row-odd"><td><p><span id="id1">[<a class="reference internal" href="References.html#id34" title="Robin Tibor Schirrmeister, Jost Tobias Springenberg, Lukas Dominique Josef Fiederer, Martin Glasstetter, Katharina Eggensperger, Michael Tangermann, Frank Hutter, Wolfram Burgard, and Tonio Ball. Deep learning with convolutional neural networks for eeg decoding and visualization. Human Brain Mapping, aug 2017. URL: http://dx.doi.org/10.1002/hbm.23730, doi:10.1002/hbm.23730.">Schirrmeister <em>et al.</em>, 2017</a>]</span></p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<p>The Mixed Imagery Dataset (MID) was obtained from 4 healthy subjects (3 female, all right-handed, age
26.75Â±5.9 (meanÂ±std)) with a varying number of trials (S1: 675, S2: 2172, S3: 698, S4: 464) of imagined
movements (right hand and feet), mental rotation and mental word generation. All details were the same as
for the High Gamma Dataset, except: a 64-electrode subset of electrodes was used for recording, recordings
were not performed in the electromagnetically shielded cabin, thus possibly better approximating conditions
of real-world BCI usage, and trials varied in duration between 1 to 7 seconds. The dataset was analyzed
by cutting out time windows of 2 seconds with 1.5 second overlap from all trials longer than 2 seconds (S1:
6074 windows, S2: 21339, S3: 6197, S4: 4220), and both methods were evaluated using the accuracy of the
predictions for all the 2-second windows for the last two runs of roughly 130 trials (S1: 129, S2: 160, S3:
124, S4: 123).</p>
<p>For the mixed imagery dataset, we find the deep ConvNet to perform slightly better and the shallow ConvNet to perform slightly worse than the FBCSP algorithm, as can be seen in <a class="reference internal" href="#mixed-imagery-dataset-results"><span class="std std-numref">Table 9</span></a>.</p>
</div>
<div class="section" id="decoding-error-related-signals">
<h2>Decoding error-related signals<a class="headerlink" href="#decoding-error-related-signals" title="Permalink to this headline">#</a></h2>
<div class="section" id="decoding-observation-of-robots-making-errors">
<h3>Decoding Observation of Robots Making Errors<a class="headerlink" href="#decoding-observation-of-robots-making-errors" title="Permalink to this headline">#</a></h3>
<table class="colwidths-auto table" id="robot-ball-results">
<caption><span class="caption-number">Table 10 </span><span class="caption-text">Accuracies for decoding watching of successful or unsuccessful robot-liquid pouring or ball-lifting.</span><a class="headerlink" href="#robot-ball-results" title="Permalink to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>robot task</p></th>
<th class="head"><p>time interval</p></th>
<th class="head"><p>Deep ConvNet</p></th>
<th class="head"><p>rLDA</p></th>
<th class="head"><p>FBCSP</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Pouring Liquid</p></td>
<td><p>2-5s</p></td>
<td><p>78.2 Â± 8.4</p></td>
<td><p>67.5 Â± 8.5</p></td>
<td><p>60.1 Â± 3.7</p></td>
</tr>
<tr class="row-odd"><td><p>Pouring Liquid</p></td>
<td><p>3.3-7.5s</p></td>
<td><p>71.9 Â± 7.6</p></td>
<td><p>63.0 Â± 9.3</p></td>
<td><p>66.5 Â± 5.7</p></td>
</tr>
<tr class="row-even"><td><p>Lifting Ball</p></td>
<td><p>4.8-6.3s</p></td>
<td><p>59.6 Â± 6.4</p></td>
<td><p>58.1 Â± 6.6</p></td>
<td><p>52.4 Â± 2.8</p></td>
</tr>
<tr class="row-odd"><td><p>Lifting Ball</p></td>
<td><p>4-7s</p></td>
<td><p>64.6 Â± 6.1</p></td>
<td><p>58.5 Â± 8.2</p></td>
<td><p>53.1 Â± 2.5</p></td>
</tr>
<tr class="row-even"><td><p><span id="id2">[<a class="reference internal" href="References.html#id39" title="Joos Behncke, Robin T Schirrmeister, Wolfram Burgard, and Tonio Ball. The signature of robot action success in eeg signals of a human observer: decoding and visualization using deep convolutional neural networks. In 2018 6th international conference on brain-computer interface (BCI), 1â€“6. IEEE, 2018.">Behncke <em>et al.</em>, 2018</a>]</span></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<p>In this study, we aimed to classify whether a person had watched a video of a successful or an unsuccessful attempt of a robot performing one of two tasks (lifting a ball or pouring liquid) based on EEG recorded during the video observation. We compared the performance of our deep ConvNet to that of regularized linear discriminant analysis (rLDA) and FBCSP in this task. Our results, presented in <a class="reference internal" href="#robot-ball-results"><span class="std std-numref">Table 10</span></a>, demonstrate that the deep ConvNet outperformed the other methods for both tasks and both decoding intervals.</p>
</div>
<div class="section" id="decoding-of-eriksen-flanker-task-errors-and-errors-during-online-gui-control">
<span id="flanker-and-gui-section"></span><h3>Decoding of Eriksen Flanker Task Errors and Errors during Online GUI Control<a class="headerlink" href="#decoding-of-eriksen-flanker-task-errors-and-errors-during-online-gui-control" title="Permalink to this headline">#</a></h3>
<div class="sd-container-fluid sd-sphinx-override sd-mb-4 docutils">
<div class="sd-row sd-g-2 sd-g-xs-2 sd-g-sm-2 sd-g-md-2 sd-g-lg-2 docutils">
<div class="sd-col sd-d-flex-column docutils">
<div class="figure align-default" id="within-subject-flanker-gui-fig">
<a class="reference internal image-reference" href="_images/within-subject-flanker-gui.png"><img alt="_images/within-subject-flanker-gui.png" src="_images/within-subject-flanker-gui.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 21 </span><span class="caption-text">Comparison of within-subject decoding by rLDA and deep ConvNets. Error bars show the SEM. A) Eriksen flanker task (mean of 31 subjects), last 20% of subject data as test set. Deep ConvNets were 7.12% better than rLDA, pval = 6.24 *10-20 (paired t-test). B) Online GUI control (mean of 4 subjects), last session of each subject as test data <span id="id3">[<a class="reference internal" href="References.html#id40" title="Martin VÃ¶lker, Robin T Schirrmeister, Lukas DJ Fiederer, Wolfram Burgard, and Tonio Ball. Deep transfer learning for error decoding from non-invasive eeg. In 2018 6th International Conference on Brain-Computer Interface (BCI), 1â€“6. IEEE, 2018.">VÃ¶lker <em>et al.</em>, 2018</a>]</span></span><a class="headerlink" href="#within-subject-flanker-gui-fig" title="Permalink to this image">#</a></p>
</div>
</div>
<div class="sd-col sd-d-flex-column docutils">
<div class="figure align-default" id="cross-subject-flanker-gui-fig">
<a class="reference internal image-reference" href="_images/cross-subject-flanker-gui.png"><img alt="_images/cross-subject-flanker-gui.png" src="_images/cross-subject-flanker-gui.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 22 </span><span class="caption-text">Mean normalized decoding accuracy on unknown subjects. Error bars show the SEM. A) Eriksen flanker task, trained on 30 subjects, tested on 1 subject.  Deep ConvNets were 5.05% better than rLDA, p = 3.16 *10-4 (paired t-test). B) Online GUI control. Trained on 3 subjects, tested on the  respective remaining subject. <span id="id4">[<a class="reference internal" href="References.html#id40" title="Martin VÃ¶lker, Robin T Schirrmeister, Lukas DJ Fiederer, Wolfram Burgard, and Tonio Ball. Deep transfer learning for error decoding from non-invasive eeg. In 2018 6th International Conference on Brain-Computer Interface (BCI), 1â€“6. IEEE, 2018.">VÃ¶lker <em>et al.</em>, 2018</a>]</span></span><a class="headerlink" href="#cross-subject-flanker-gui-fig" title="Permalink to this image">#</a></p>
</div>
</div>
</div>
</div>
<p>In two addtional error-related decoding experiments, we evaluated an Eriksen flanker task and errors during an the online control of a graphical user interface through a brain-computer-interface. In the Eriksen flanker task, the subjects were asked to press the left or right button on a gamepad depending on whether an â€˜Lâ€™ or an â€˜Râ€™ was the middle character of a 5-letter string displayed on the screen. For the online graphical user interface (GUI) control, the subjects were given an aim to reach using the GUI. They had to think of one of the classes of the aforementioned Mixed Imagery Dataset to choose one of four possible GUI actions. The correct GUI action was always determined by the specificed aim for the subject, hence an erroneous action could be detected. The decoding task in this paper was to distinguish whether the BCI-selected action was correct or erroneous. Results in <a class="reference internal" href="#within-subject-flanker-gui-fig"><span class="std std-numref">Fig. 21</span></a> and <a class="reference internal" href="#cross-subject-flanker-gui-fig"><span class="std std-numref">Fig. 22</span></a> show that deep ConvNets outperform rLDA in all settings except cross-subject error-decoding for online GUI control, where the low number of subjects (4) may prevent the ConvNets to learn enough to outperform rLDA.</p>
</div>
</div>
<div class="section" id="proof-of-concept-assistive-system">
<h2>Proof-of-concept assistive system<a class="headerlink" href="#proof-of-concept-assistive-system" title="Permalink to this headline">#</a></h2>
<div class="figure align-default" id="robot-bci-overview-fig">
<img alt="_images/robot-bci-overview.png" src="_images/robot-bci-overview.png" />
<p class="caption"><span class="caption-number">Fig. 23 </span><span class="caption-text">Overview of the proof-of-concept assistive system from <span id="id5">[<a class="reference internal" href="References.html#id41" title="Felix Burget, Lukas Dominique Josef Fiederer, Daniel Kuhner, Martin VÃ¶lker, Johannes Aldinger, Robin Tibor Schirrmeister, Chau Do, Joschka Boedecker, Bernhard Nebel, Tonio Ball, and others. Acting thoughts: towards a mobile robotic service assistant for users with limited communication skills. In 2017 European Conference on Mobile Robots (ECMR), 1â€“6. IEEE, 2017.">Burget <em>et al.</em>, 2017</a>]</span> using the deep ConvNet in the BCI component. Robotic arm could be given high-level commands via the BCI, high-level commands were extracted from a knowledge base. The commands were then autonomously planned and executed by the robotic arm.</span><a class="headerlink" href="#robot-bci-overview-fig" title="Permalink to this image">#</a></p>
</div>
<table class="colwidths-auto table" id="bci-robot-results">
<caption><span class="caption-number">Table 11 </span><span class="caption-text">Results for BCI control of the GUI. Accuracy is fraction of correct commands, time is time per command, steps is steps needed to reach the aim, path optimality is ratio of miniminally needed  nubmer of steps to actually used number of steps when every step is optimal, and time/step is time per step.</span><a class="headerlink" href="#bci-robot-results" title="Permalink to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Subject</p></th>
<th class="head"><p>Runs</p></th>
<th class="head"><p>Accuracy* [%]</p></th>
<th class="head"><p>Time [s]</p></th>
<th class="head"><p>Steps</p></th>
<th class="head"><p>Path Optimality [%]</p></th>
<th class="head"><p>Time/Step [s]</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>S1</p></td>
<td><p>18</p></td>
<td><p>84.1 Â± 6.1</p></td>
<td><p>125 Â± 84</p></td>
<td><p>13.0 Â± 7.8</p></td>
<td><p>70.1 Â± 22.3</p></td>
<td><p>9 Â± 2</p></td>
</tr>
<tr class="row-odd"><td><p>S2</p></td>
<td><p>14</p></td>
<td><p>76.8 Â± 14.1</p></td>
<td><p>150 Â± 32</p></td>
<td><p>10.1 Â± 2.8</p></td>
<td><p>91.3 Â± 12.0</p></td>
<td><p>9 Â± 3</p></td>
</tr>
<tr class="row-even"><td><p>S3</p></td>
<td><p>17</p></td>
<td><p>82.0 Â± 7.4</p></td>
<td><p>200 Â± 159</p></td>
<td><p>17.6 Â± 11.4</p></td>
<td><p>65.7 Â± 28.9</p></td>
<td><p>11 Â± 4</p></td>
</tr>
<tr class="row-odd"><td><p>S4</p></td>
<td><p>3</p></td>
<td><p>63.8 Â± 15.6</p></td>
<td><p>176 Â± 102</p></td>
<td><p>26.3 Â± 11.2</p></td>
<td><p>34.5 Â± 1.2</p></td>
<td><p>6 Â± 2</p></td>
</tr>
<tr class="row-even"><td><p>Average</p></td>
<td><p>13</p></td>
<td><p>76.7 Â± 9.1</p></td>
<td><p>148 Â± 50</p></td>
<td><p>16.7 Â± 7.1</p></td>
<td><p>65.4 Â± 23.4</p></td>
<td><p>9 Â± 2</p></td>
</tr>
<tr class="row-odd"><td><p><span id="id6">[<a class="reference internal" href="References.html#id41" title="Felix Burget, Lukas Dominique Josef Fiederer, Daniel Kuhner, Martin VÃ¶lker, Johannes Aldinger, Robin Tibor Schirrmeister, Chau Do, Joschka Boedecker, Bernhard Nebel, Tonio Ball, and others. Acting thoughts: towards a mobile robotic service assistant for users with limited communication skills. In 2017 European Conference on Mobile Robots (ECMR), 1â€“6. IEEE, 2017.">Burget <em>et al.</em>, 2017</a>]</span></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<p>We also evaluated the use of our deep ConvNet as part of an assistive robot system where the brain-computer interface was sending high-level commands to a robotic arm. In this proof-of-concept system, the robotic arm could be instructed by the user via the BCI to fetch a cup and directly move the cup to the persons mouth to drink from it. An overview can be seen in <a class="reference internal" href="#robot-bci-overview-fig"><span class="std std-numref">Fig. 23</span></a>. Results from <a class="reference internal" href="#bci-robot-results"><span class="std std-numref">Table 11</span></a> show that 3 out of 4 subjects had a command accuracy of more than 75% and were able to reach the target using less than twice the steps of the minimal path through the GUI (path optimality &gt; %50%).</p>
</div>
<div class="section" id="intracranial-eeg-decoding">
<h2>Intracranial EEG decoding<a class="headerlink" href="#intracranial-eeg-decoding" title="Permalink to this headline">#</a></h2>
<div class="section" id="intracranial-eeg-decoding-of-eriksen-flanker-task">
<h3>Intracranial EEG Decoding of Eriksen Flanker Task<a class="headerlink" href="#intracranial-eeg-decoding-of-eriksen-flanker-task" title="Permalink to this headline">#</a></h3>
<table class="colwidths-auto table" id="intracranial-error-results-table">
<caption><span class="caption-number">Table 12 </span><span class="caption-text">Results for single-channel intracranial decoding of errors during an Eriksen flanker task. Balanced Accuracy is the mean of the accuracies for correct class ground truth labels and error class ground truth labels.</span><a class="headerlink" href="#intracranial-error-results-table" title="Permalink to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Classifier</p></th>
<th class="head"><p>Balanced Accuracy  [%]</p></th>
<th class="head"><p>Accuracy Correct Class [%]</p></th>
<th class="head"><p>Accuracy Error Class  [%]</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Deep4Net</p></td>
<td><p>59.28 Â± 0.50</p></td>
<td><p>69.37 Â± 0.44</p></td>
<td><p>49.19 Â± 0.56</p></td>
</tr>
<tr class="row-odd"><td><p>ShallowNet</p></td>
<td><p>58.42 Â± 0.32</p></td>
<td><p>74.83 Â± 0.25</p></td>
<td><p>42.01 Â± 0.40</p></td>
</tr>
<tr class="row-even"><td><p>EEGNet</p></td>
<td><p>57.73 Â± 0.52</p></td>
<td><p>57.78 Â± 0.48</p></td>
<td><p>57.68 Â± 0.56</p></td>
</tr>
<tr class="row-odd"><td><p>rLDA</p></td>
<td><p>53.76 Â± 0.32</p></td>
<td><p>76.12 Â± 0.26</p></td>
<td><p>31.40 Â± 0.38</p></td>
</tr>
<tr class="row-even"><td><p>ResNet</p></td>
<td><p>52.45 Â± 0.21</p></td>
<td><p>95.47 Â± 0.14</p></td>
<td><p>09.43 Â± 0.28</p></td>
</tr>
<tr class="row-odd"><td><p><span id="id7">[<a class="reference internal" href="References.html#id38" title="Martin Volker, Jiri Hammer, Robin T Schirrmeister, Joos Behncke, Lukas DJ Fiederer, Andreas Schulze-Bonhage, Petr Marusic, Wolfram Burgard, and Tonio Ball. Intracranial error detection via deep learning. In 2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC), 568â€“575. IEEE, 2018.">Volker <em>et al.</em>, 2018</a>]</span></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<div class="figure align-default" id="intracranial-error-results-fig">
<img alt="_images/IntracranialError.png" src="_images/IntracranialError.png" />
<p class="caption"><span class="caption-number">Fig. 24 </span><span class="caption-text">Results for all-channel intracranial decoding of errors during an Eriksen flanker task <span id="id8">[<a class="reference internal" href="References.html#id38" title="Martin Volker, Jiri Hammer, Robin T Schirrmeister, Joos Behncke, Lukas DJ Fiederer, Andreas Schulze-Bonhage, Petr Marusic, Wolfram Burgard, and Tonio Ball. Intracranial error detection via deep learning. In 2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC), 568â€“575. IEEE, 2018.">Volker <em>et al.</em>, 2018</a>]</span>.</span><a class="headerlink" href="#intracranial-error-results-fig" title="Permalink to this image">#</a></p>
</div>
<p>We further evaluated whether the same networks developed for noninvasive EEG decoding can successfully learn to decode intracranial EEG. Therefore, in one work we used the same Eriksen flanker task as described in <a class="reference internal" href="#flanker-and-gui-section"><span class="std std-ref">Decoding of Eriksen Flanker Task Errors and Errors during Online GUI Control</span></a>, but recorded intracranial EEG from 23 patients who had pharmacoresistant epilepsy <span id="id9">[<a class="reference internal" href="References.html#id38" title="Martin Volker, Jiri Hammer, Robin T Schirrmeister, Joos Behncke, Lukas DJ Fiederer, Andreas Schulze-Bonhage, Petr Marusic, Wolfram Burgard, and Tonio Ball. Intracranial error detection via deep learning. In 2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC), 568â€“575. IEEE, 2018.">Volker <em>et al.</em>, 2018</a>]</span>.</p>
</div>
<div class="section" id="transfer-learning-for-intracranial-error-decoding">
<h3>Transfer Learning for Intracranial Error Decoding<a class="headerlink" href="#transfer-learning-for-intracranial-error-decoding" title="Permalink to this headline">#</a></h3>
<div class="figure align-default" id="eriksen-flanker-car-driving-tasks-fig">
<img alt="_images/eriksen-flanker-car-driving-tasks.png" src="_images/eriksen-flanker-car-driving-tasks.png" />
<p class="caption"><span class="caption-number">Fig. 25 </span><span class="caption-text">Sketch of the Eriksen flanker task (A) and screenshot of the car driving task (B). <span id="id10">[<a class="reference internal" href="References.html#id37" title="Joos Behncke, Robin Tibor Schirrmeister, Martin Volker, Jiri Hammer, Petr Marusic, Andreas Schulze-Bonhage, Wolfram Burgard, and Tonio Ball. Cross-paradigm pretraining of convolutional networks improves intracranial eeg decoding. In 2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC), 1046â€“1053. IEEE, 2018.">Behncke <em>et al.</em>, 2018</a>]</span>.</span><a class="headerlink" href="#eriksen-flanker-car-driving-tasks-fig" title="Permalink to this image">#</a></p>
</div>
<div class="figure align-default" id="cross-training-eft-cdt-results-fig">
<img alt="_images/cross-training-eft-cdt-results.png" src="_images/cross-training-eft-cdt-results.png" />
<p class="caption"><span class="caption-number">Fig. 26 </span><span class="caption-text">Results for transfer learning on the Eriksen flanker task (EFT) and the car driving task (CDT) <span id="id11">[<a class="reference internal" href="References.html#id37" title="Joos Behncke, Robin Tibor Schirrmeister, Martin Volker, Jiri Hammer, Petr Marusic, Andreas Schulze-Bonhage, Wolfram Burgard, and Tonio Ball. Cross-paradigm pretraining of convolutional networks improves intracranial eeg decoding. In 2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC), 1046â€“1053. IEEE, 2018.">Behncke <em>et al.</em>, 2018</a>]</span>. All results are computed for a varying fraction of available data for the target decoding task (bottom row). <strong>A</strong> compares CDT accuracies after training only on CDT or pretraining on EFT and  finetuning on CDT. <strong>B</strong> compares EFT accuracies after only training on EFT or after  pretraining on CDT and finetuning on EFT. As a sanity check for the results in <strong>B</strong>, <strong>C</strong> compares EFT accuracies when pretraining on original CDT data and finetuning on EFT to pretraining on CDT data with shuffled labels (CDT*) and finetuning on EFT. Results show that pretraining on CDT helps EFT decoding when little EFT data is available.</span><a class="headerlink" href="#cross-training-eft-cdt-results-fig" title="Permalink to this image">#</a></p>
</div>
<p>We further tested the potential of ConvNets to transfer knowledge learned from decoding intracranial signals in error-decoding paradigm to decoding signals in another a different error-decoding paradigm <span id="id12">[<a class="reference internal" href="References.html#id37" title="Joos Behncke, Robin Tibor Schirrmeister, Martin Volker, Jiri Hammer, Petr Marusic, Andreas Schulze-Bonhage, Wolfram Burgard, and Tonio Ball. Cross-paradigm pretraining of convolutional networks improves intracranial eeg decoding. In 2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC), 1046â€“1053. IEEE, 2018.">Behncke <em>et al.</em>, 2018</a>]</span>. The two error-decoding paradigms were the aforementioned Eriksen flanker task (EFT) and a car driving task (CDT), where subjects had to use a steering wheel to steer a car in a computer game and avoid hitting obstacles, where hitting an obstacle was considered an error event (see <a class="reference internal" href="#eriksen-flanker-car-driving-tasks-fig"><span class="std std-numref">Fig. 25</span></a>). Results in <a class="reference internal" href="#cross-training-eft-cdt-results-fig"><span class="std std-numref">Fig. 26</span></a> show that pretraining on CDT helps EFT decoding when few EDT data is available.</p>
</div>
<div class="section" id="microelectrocorticography-decoding-of-auditory-evoked-responses-in-sheep">
<h3>Microelectrocorticography decoding of auditory evoked responses in sheep<a class="headerlink" href="#microelectrocorticography-decoding-of-auditory-evoked-responses-in-sheep" title="Permalink to this headline">#</a></h3>
<div class="figure align-default" id="sheep-sounds-fig">
<img alt="_images/sheep-sounds.jpg" src="_images/sheep-sounds.jpg" />
<p class="caption"><span class="caption-number">Fig. 27 </span><span class="caption-text">Overview over decoding tasks for auditory evoked responses in a sheep <span id="id13">[<a class="reference internal" href="References.html#id36" title="X. Wang, C. A. Gkogkidis, R. T. Schirrmeister, F. A. Heilmeyer, M. Gierthmuehlen, F. Kohler, M. Schuettler, T. Stieglitz, and T. Ball. Deep learning for micro-electrocorticographic (Âµecog) data. In 2018 IEEE-EMBS Conference on Biomedical Engineering and Sciences (IECBES), volume, 63-68. 2018. doi:10.1109/IECBES.2018.8626607.">Wang <em>et al.</em>, 2018</a>]</span>. First task (top) was to distingish 3 seconds when the sound was playing from the second before and the second after. Second task (bottom) was to distinguish the first, second and third second during theplaying of the sound. Signals are averaged responses from one electrode during different days, with black and grey being signals while the sheep was awake and red ones while the sheep was under general anesthesia.</span><a class="headerlink" href="#sheep-sounds-fig" title="Permalink to this image">#</a></p>
</div>
<div class="figure align-default" id="sheep-accuracies-fig">
<img alt="_images/sheep-accuracies.png" src="_images/sheep-accuracies.png" />
<p class="caption"><span class="caption-number">Fig. 28 </span><span class="caption-text">Results of decoding auditory evoked responses from sheep with rlDA and FBSCP or the deep ConvNet. Open circles represent accuracies for individual experiment days and closed circles represent the average over these accuracies.</span><a class="headerlink" href="#sheep-accuracies-fig" title="Permalink to this image">#</a></p>
</div>
<p>In this study, we evaluated the ConvNets for decoding auditory evoked responses played to a sheep that was chronically implanted with  a Î¼ECoG-based neural interfacing device <span id="id14">[<a class="reference internal" href="References.html#id36" title="X. Wang, C. A. Gkogkidis, R. T. Schirrmeister, F. A. Heilmeyer, M. Gierthmuehlen, F. Kohler, M. Schuettler, T. Stieglitz, and T. Ball. Deep learning for micro-electrocorticographic (Âµecog) data. In 2018 IEEE-EMBS Conference on Biomedical Engineering and Sciences (IECBES), volume, 63-68. 2018. doi:10.1109/IECBES.2018.8626607.">Wang <em>et al.</em>, 2018</a>]</span>. 3-seconds-long sounds were presented to the sheep and two decoding tasks were defined from those 3 seconds as well as the second immediately before and after the playing of the sound. The first decoding task was to distinguish the 3 seconds when the sound was playing from the second  immediately before and the second immediately after the sound. The second task was distinguishing the first, second and third second of the playing of the sound to discriminate early, intermediate and late auditory evoked response (see <a class="reference internal" href="#sheep-sounds-fig"><span class="std std-numref">Fig. 27</span></a>). Results in <a class="reference internal" href="#sheep-accuracies-fig"><span class="std std-numref">Fig. 28</span></a> show that the  deep ConvNet can perform as good as FBSCP and rLDA, and perform well on both tasks, whereas rLDA performs competitively only on the first and FBSCP only on the second task.</p>
</div>
</div>
<div class="section" id="evaluation-on-large-scale-task-diverse-dataset">
<h2>Evaluation on large-scale task-diverse dataset<a class="headerlink" href="#evaluation-on-large-scale-task-diverse-dataset" title="Permalink to this headline">#</a></h2>
<table class="colwidths-auto table" id="large-framework-overview-table">
<caption><span class="caption-number">Table 13 </span><span class="caption-text">Datasets for the large-scale evaluation framework.</span><a class="headerlink" href="#large-framework-overview-table" title="Permalink to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Name (Acronym)</p></th>
<th class="head"><p>#Classes</p></th>
<th class="head"><p>Task Type</p></th>
<th class="head"><p>#Subjects</p></th>
<th class="head"><p>Trials per Subject</p></th>
<th class="head"><p>Class balance</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>High-Gamma Dataset (Motor)</p></td>
<td><p>4</p></td>
<td><p>Motor task</p></td>
<td><p>20</p></td>
<td><p>1000</p></td>
<td><p>balanced</p></td>
</tr>
<tr class="row-odd"><td><p>KUKA Pouring Observation (KPO)</p></td>
<td><p>2</p></td>
<td><p>Error observation</p></td>
<td><p>5</p></td>
<td><p>720-800</p></td>
<td><p>balanced</p></td>
</tr>
<tr class="row-even"><td><p>Robot-Grasping Observation (RGO)</p></td>
<td><p>2</p></td>
<td><p>Error observation</p></td>
<td><p>12</p></td>
<td><p>720-800</p></td>
<td><p>balanced</p></td>
</tr>
<tr class="row-odd"><td><p>Error-Related Negativity (ERN)</p></td>
<td><p>2</p></td>
<td><p>Eriksen flanker task</p></td>
<td><p>31</p></td>
<td><p>1000</p></td>
<td><p>1/2 up to 1/15</p></td>
</tr>
<tr class="row-even"><td><p>Semantic Categories</p></td>
<td><p>3</p></td>
<td><p>Speech imagery</p></td>
<td><p>16</p></td>
<td><p>750</p></td>
<td><p>balanced</p></td>
</tr>
<tr class="row-odd"><td><p>Real vs. Pseudo Words</p></td>
<td><p>2</p></td>
<td><p>Speech imagery</p></td>
<td><p>16</p></td>
<td><p>1000</p></td>
<td><p>3/1</p></td>
</tr>
<tr class="row-even"><td><p><span id="id15">[<a class="reference internal" href="References.html#id35" title="Felix A Heilmeyer, Robin T Schirrmeister, Lukas DJ Fiederer, Martin Volker, Joos Behncke, and Tonio Ball. A large-scale evaluation framework for eeg deep learning architectures. In 2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC), 1039â€“1045. IEEE, 2018.">Heilmeyer <em>et al.</em>, 2018</a>]</span></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<div class="figure align-default" id="large-framework-per-dataset-results-fig">
<img alt="_images/large-framework-per-dataset-results.png" src="_images/large-framework-per-dataset-results.png" />
<p class="caption"><span class="caption-number">Fig. 29 </span><span class="caption-text">Per-dataset results for the large-scale evaluation of deep ConvNet, shallow ConvNet and two versions of EEGNet. Boxplots show the distribution over per-subject accuracies for the individual decoding tasks. ern, kpo and rgo are the error-related datasets, ern: Error-related negativity Eriksen flanker task, KPO: KUKA Pouring Observation paradigm, rgo: robot-grasping observation paradigm. motor is the high-gamma dataset with 6 additional subjects that were excluded for data quality reasons from <span id="id16">[<a class="reference internal" href="References.html#id34" title="Robin Tibor Schirrmeister, Jost Tobias Springenberg, Lukas Dominique Josef Fiederer, Martin Glasstetter, Katharina Eggensperger, Michael Tangermann, Frank Hutter, Wolfram Burgard, and Tonio Ball. Deep learning with convolutional neural networks for eeg decoding and visualization. Human Brain Mapping, aug 2017. URL: http://dx.doi.org/10.1002/hbm.23730, doi:10.1002/hbm.23730.">Schirrmeister <em>et al.</em>, 2017</a>]</span>. pseudovsreal and semantic are two semantic processing datasets to classify silent repetitions of  pseudowords vs. realwords (pseudovsreal) or different semantic categories (semantic) .</span><a class="headerlink" href="#large-framework-per-dataset-results-fig" title="Permalink to this image">#</a></p>
</div>
<div class="figure align-default" id="large-framework-averaged-results-fig">
<img alt="_images/large-framework-averaged-results.png" src="_images/large-framework-averaged-results.png" />
<p class="caption"><span class="caption-number">Fig. 30 </span><span class="caption-text">Dataset-averaged results for the large-scale evaluation of deep ConvNet, shallow ConvNet and two versions of EEGNet. Accuracies are normalized to the average of the accuracies of all models.</span><a class="headerlink" href="#large-framework-averaged-results-fig" title="Permalink to this image">#</a></p>
</div>
<table class="colwidths-auto table" id="large-framework-results-table">
<caption><span class="caption-number">Table 14 </span><span class="caption-text">Dataset-averaged results for the large-scale evaluation of deep ConvNet, shallow ConvNet and two versions of EEGNet. Accuracies are normalized to the average of the accuracies of all models.</span><a class="headerlink" href="#large-framework-results-table" title="Permalink to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>Mean accuracy</p></th>
<th class="head"><p>Mean normalized accuracy</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Deep ConvNet</p></td>
<td><p>70.08% Â± 20.92%</p></td>
<td><p>1.00 Â± 0.05</p></td>
</tr>
<tr class="row-odd"><td><p>EEGNetv2</p></td>
<td><p>70.00% Â±18.86%</p></td>
<td><p>1.02 Â± 0.08</p></td>
</tr>
<tr class="row-even"><td><p>EEGNet</p></td>
<td><p>67.71% Â± 19.04%</p></td>
<td><p>0.98 Â± 0.06</p></td>
</tr>
<tr class="row-odd"><td><p>Shallow ConvNet</p></td>
<td><p>67.71% Â±19.04%</p></td>
<td><p>0.99 Â± 0.06</p></td>
</tr>
<tr class="row-even"><td><p><span id="id17">[<a class="reference internal" href="References.html#id35" title="Felix A Heilmeyer, Robin T Schirrmeister, Lukas DJ Fiederer, Martin Volker, Joos Behncke, and Tonio Ball. A large-scale evaluation framework for eeg deep learning architectures. In 2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC), 1039â€“1045. IEEE, 2018.">Heilmeyer <em>et al.</em>, 2018</a>]</span></p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<p>We also compared the deep and shallow ConvNet architectures as well as EEGNet on six classification tasks with more than 90000 trials in total (see <a class="reference internal" href="#large-framework-overview-table"><span class="std std-numref">Table 13</span></a>) <span id="id18">[<a class="reference internal" href="References.html#id35" title="Felix A Heilmeyer, Robin T Schirrmeister, Lukas DJ Fiederer, Martin Volker, Joos Behncke, and Tonio Ball. A large-scale evaluation framework for eeg deep learning architectures. In 2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC), 1039â€“1045. IEEE, 2018.">Heilmeyer <em>et al.</em>, 2018</a>]</span>. The datasets tasks were all recorded in our lab and included the high-gamma dataset, three error-related tasks described before (Eriksen flanker task, robot grasping and robot pouring observations) as well as two tasks on semantic processing. In the semantic processing dataset, the classification tasks were to distinguish different types of words that a subject silently repeated <span id="id19">[<a class="reference internal" href="References.html#id33" title="V. Rau. Eeg correlates of inner speech. Bachelor's Thesis, University of Freiburg, DOI, 2015.">Rau, 2015</a>]</span>. The first task was to distinguish existing real words from nonexisting pseudowords. The second classification task was to distingiush three semantic categories (food, animals, tools) the word may belong to. The evaluation code for all models always used the original code and hyperparameters from the original studies in order to ensure a fair comparison. Results show that the deep ConvNet and the more recent version of EEGNet (EEGNetv2) perform similarly well, with shallow and an older version of EEGNet performing slightly worse, see  <a class="reference internal" href="#large-framework-per-dataset-results-fig"><span class="std std-numref">Fig. 29</span></a>, <a class="reference internal" href="#large-framework-averaged-results-fig"><span class="std std-numref">Fig. 30</span></a>  and <a class="reference internal" href="#large-framework-results-table"><span class="std std-numref">Table 14</span></a>.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="MovementDecoding.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Decoding Movement-Related Brain Activity</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="Pathology.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Decoding Pathology</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By The Jupyter Book community<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>