
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Further Task-Related Decoding &#8212; Deep Learning for Brain-Signal Decoding from Electroencephalography (EEG)</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script async="async" kind="hypothesis" src="https://hypothes.is/embed.js"></script>
    <script kind="utterances">

    var commentsRunWhenDOMLoaded = cb => {
    if (document.readyState != 'loading') {
        cb()
    } else if (document.addEventListener) {
        document.addEventListener('DOMContentLoaded', cb)
    } else {
        document.attachEvent('onreadystatechange', function() {
        if (document.readyState == 'complete') cb()
        })
    }
}

var addUtterances = () => {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src = "https://utteranc.es/client.js";
    script.async = "async";

    script.setAttribute("repo", "robintibor/eeg-deep-learning-phd-thesis");
    script.setAttribute("issue-term", "pathname");
    script.setAttribute("theme", "github-light");
    script.setAttribute("label", "ðŸ’¬ comment");
    script.setAttribute("crossorigin", "anonymous");

    sections = document.querySelectorAll("div.section");
    if (sections !== null) {
        section = sections[sections.length-1];
        section.appendChild(script);
    }
}
commentsRunWhenDOMLoaded(addUtterances);
</script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Decoding Pathology" href="Pathology.html" />
    <link rel="prev" title="Decoding Movement-Related Brain Activity" href="MovementDecoding.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/braindecode-logo.svg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Deep Learning for Brain-Signal Decoding from Electroencephalography (EEG)</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Abstract.html">
   Deep Learning for Brain-Signal Decoding from Electroencephalography (EEG)
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="PriorWork.html">
   Prior Work
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="DeepArchitectures.html">
   Neural Network Architectures for EEG-Decoding
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="CroppedTraining.html">
   Cropped Training
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="PerturbationVisualization.html">
   Perturbation Visualization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="MovementDecoding.html">
   Decoding Movement-Related Brain Activity
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Further Task-Related Decoding
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Pathology.html">
   Decoding Pathology
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Invertible.html">
   Invertible Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="FutureWork.html">
   Future Work
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="References.html">
   References
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/TaskDecoding.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/TaskDecoding.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#decoding-different-mental-imageries">
   Decoding different mental imageries
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#decoding-error-related-signals">
   Decoding error-related signals
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#decoding-observation-of-robots-making-errors">
     Decoding Observation of Robots Making Errors
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#decoding-of-eriksen-flanker-task-errors-and-errors-during-online-gui-control">
     Decoding of Eriksen Flanker Task Errors and Errors during Online GUI Control
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#proof-of-concept-assistive-system">
   Proof-of-concept assistive system
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#intracranial-eeg-decoding">
   Intracranial EEG decoding
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#intracranial-eeg-decoding-of-eriksen-flanker-task">
     Intracranial EEG Decoding of Eriksen Flanker Task
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#transfer-learning-for-intracranial-error-decoding">
     Transfer Learning for Intracranial Error Decoding
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#microelectrocorticography-decoding-of-auditory-evoked-responses-in-sheep">
     Microelectrocorticography decoding of auditory evoked responses in sheep
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#evaluation-on-large-scale-task-diverse-dataset">
   Evaluation on large-scale task-diverse dataset
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="further-task-related-decoding">
<span id="task-related"></span><h1>Further Task-Related Decoding<a class="headerlink" href="#further-task-related-decoding" title="Permalink to this headline">Â¶</a></h1>
<p>After our initial work designing and evaluating convolutional neural networks for movement decoding from EEG, we evaluated the resulting networks on a wide variety of other EEG decoding tasks.</p>
<ul class="simple">
<li><p>mention you helped with writing and setting up code for papers past xx</p></li>
</ul>
<div class="section" id="decoding-different-mental-imageries">
<h2>Decoding different mental imageries<a class="headerlink" href="#decoding-different-mental-imageries" title="Permalink to this headline">Â¶</a></h2>
<blockquote>
<div><p>The Mixed Imagery Dataset (MID) was obtained from 4 healthy subjects (3 female, all right-handed, age
26.75Â±5.9 (meanÂ±std)) with a varying number of trials (S1: 675, S2: 2172, S3: 698, S4: 464) of imagined
movements (right hand and feet), mental rotation and mental word generation. All details were the same as
for the High Gamma Dataset, except: a 64-electrode subset of electrodes was used for recording, recordings
were not performed in the electromagnetically shielded cabin, thus possibly better approximating conditions
of real-world BCI usage, and trials varied in duration between 1 to 7 seconds. The dataset was analyzed
by cutting out time windows of 2 seconds with 1.5 second overlap from all trials longer than 2 seconds (S1:
6074 windows, S2: 21339, S3: 6197, S4: 4220), and both methods were evaluated using the accuracy of the
predictions for all the 2-second windows for the last two runs of roughly 130 trials (S1: 129, S2: 160, S3:
124, S4: 123).</p>
</div></blockquote>
<p>For the mixed imagery dataset, we find the deep ConvNet to perform slightly better and the shallow ConvNet to perform slightly worse than the FBCSP algorithm, as can be seen in <a class="reference internal" href="#mixed-imagery-dataset-results"><span class="std std-numref">Table 6</span></a>.</p>
<table class="colwidths-auto table" id="mixed-imagery-dataset-results">
<caption><span class="caption-number">Table 6 </span><span class="caption-text">Accuracies on the Mixed-Imagery dataset. ConvNet accuracies show the difference to the FBCSP accuracy.</span><a class="headerlink" href="#mixed-imagery-dataset-results" title="Permalink to this table">Â¶</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>FBCSP</p></th>
<th class="head"><p>Deep ConvNet</p></th>
<th class="head"><p>Shallow ConvNet</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>71.2</p></td>
<td><p>+1.0</p></td>
<td><p>-3.5</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="decoding-error-related-signals">
<h2>Decoding error-related signals<a class="headerlink" href="#decoding-error-related-signals" title="Permalink to this headline">Â¶</a></h2>
<div class="section" id="decoding-observation-of-robots-making-errors">
<h3>Decoding Observation of Robots Making Errors<a class="headerlink" href="#decoding-observation-of-robots-making-errors" title="Permalink to this headline">Â¶</a></h3>
<p>In two datasets about observing robots making errors, subjects watched videos of a robot either successfully or unsuccessfully attempting one of two tasks: lifting ball from the ground (failure: letting it fall to the ground) or pouring liquid into a glass (failure: pouring the liquid outside of the glass).The decoding task was to classify whether the person watched a successful or an unsuccessful video from the EEG recorded during the observation of the corresponding video.
Results for both tasks and two decoding intervals in <a class="reference internal" href="#robot-ball-results"><span class="std std-numref">Table 7</span></a> show that the deep ConvNet outperforms regularized linear discriminant analysis (rLDA) as well as FBCSP.</p>
<table class="colwidths-auto table" id="robot-ball-results">
<caption><span class="caption-number">Table 7 </span><span class="caption-text">Accuracies for decoding watching of successful or unsuccessful robot-liquid pouring or ball-lifting.</span><a class="headerlink" href="#robot-ball-results" title="Permalink to this table">Â¶</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>robot task</p></th>
<th class="head"><p>time interval</p></th>
<th class="head"><p>Deep ConvNet</p></th>
<th class="head"><p>rLDA</p></th>
<th class="head"><p>FBCSP</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Pouring Liquid</p></td>
<td><p>2-5s</p></td>
<td><p>78.2 Â± 8.4</p></td>
<td><p>67.5 Â± 8.5</p></td>
<td><p>60.1 Â± 3.7</p></td>
</tr>
<tr class="row-odd"><td><p>Pouring Liquid</p></td>
<td><p>3.3-7.5s</p></td>
<td><p>71.9 Â± 7.6</p></td>
<td><p>63.0 Â± 9.3</p></td>
<td><p>66.5 Â± 5.7</p></td>
</tr>
<tr class="row-even"><td><p>Lifting Ball</p></td>
<td><p>4.8-6.3s</p></td>
<td><p>59.6 Â± 6.4</p></td>
<td><p>58.1 Â± 6.6</p></td>
<td><p>52.4 Â± 2.8</p></td>
</tr>
<tr class="row-odd"><td><p>Lifting Ball</p></td>
<td><p>4-7s</p></td>
<td><p>64.6 Â± 6.1</p></td>
<td><p>58.5 Â± 8.2</p></td>
<td><p>53.1 Â± 2.5</p></td>
</tr>
<tr class="row-even"><td><p><span id="id1">[<a class="reference internal" href="References.html#id33">BSBB18</a>]</span></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="decoding-of-eriksen-flanker-task-errors-and-errors-during-online-gui-control">
<span id="flanker-and-gui-section"></span><h3>Decoding of Eriksen Flanker Task Errors and Errors during Online GUI Control<a class="headerlink" href="#decoding-of-eriksen-flanker-task-errors-and-errors-during-online-gui-control" title="Permalink to this headline">Â¶</a></h3>
<p>In two further error-related decoding experiments, we evaluated an Eriksen flanker task and errors during an the online control of a graphical user interface through a brain-computer-interface. The Eriksen flanker task required the students to press a left or a right button on a gamepad depending on whether a â€˜Lâ€™ or an â€˜Râ€™ was the middle character of a 5-letter string displayed on the screen. For the online GUI control the subjects were given an aim to reach using the GUI. They had to thinki of one of the classes of the aforementioned Mixed Imagery Dataset to choose one of four possible GUI actions. The correct GUI action was always determined by the specificed aim for the subject, hence an erroneous action could be detected. The decoding task in this paper was to distinguish whether the BCI-selected action was correct or erroneous. Results in <a class="reference internal" href="#within-subject-flanker-gui-fig"><span class="std std-numref">Fig. 19</span></a> and <a class="reference internal" href="#cross-subject-flanker-gui-fig"><span class="std std-numref">Fig. 20</span></a> show that deep ConvNets outperform rLDA in all settings except cross-subject error-decoding for online GUI control, where the low number of subjects (4) may prevent the ConvNets to learn enough to outperform rLDA.</p>
<div class="sphinx-bs container-fluid docutils">
<div class="row docutils">
<div class="d-flex col-lg-6 col-md-6 col-sm-6 col-xs-12 docutils">
<div class="card w-100 shadow-none border-0 docutils">
<div class="card-body docutils">
<div class="figure align-default" id="within-subject-flanker-gui-fig">
<a class="reference internal image-reference" href="_images/within-subject-flanker-gui.png"><img alt="_images/within-subject-flanker-gui.png" src="_images/within-subject-flanker-gui.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 19 </span><span class="caption-text">Comparison of within-subject decoding by rLDA and deep ConvNets. Error bars show the SEM. A) Eriksen flanker task (mean of 31 subjects), last 20% of subject data as test set. Deep ConvNets were 7.12% better than rLDA, pval = 6.24 *10-20 (paired t-test). B) Online GUI control (mean of 4 subjects), last session of each subject as test data <span id="id2">[<a class="reference internal" href="References.html#id34">VolkerSF+18</a>]</span></span><a class="headerlink" href="#within-subject-flanker-gui-fig" title="Permalink to this image">Â¶</a></p>
</div>
</div>
</div>
</div>
<div class="d-flex col-lg-6 col-md-6 col-sm-6 col-xs-12 docutils">
<div class="card w-100 shadow-none border-0 docutils">
<div class="card-body docutils">
<div class="figure align-default" id="cross-subject-flanker-gui-fig">
<a class="reference internal image-reference" href="_images/cross-subject-flanker-gui.png"><img alt="_images/cross-subject-flanker-gui.png" src="_images/cross-subject-flanker-gui.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 20 </span><span class="caption-text">Mean normalized decoding accuracy on unknown subjects. Error bars show the SEM. A) Eriksen flanker task, trained on 30 subjects, tested on 1 subject.  Deep ConvNets were 5.05% better than rLDA, p = 3.16 *10-4 (paired t-test). B) Online GUI control. Trained on 3 subjects, tested on the  respective remaining subject. <span id="id3">[<a class="reference internal" href="References.html#id34">VolkerSF+18</a>]</span></span><a class="headerlink" href="#cross-subject-flanker-gui-fig" title="Permalink to this image">Â¶</a></p>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="proof-of-concept-assistive-system">
<h2>Proof-of-concept assistive system<a class="headerlink" href="#proof-of-concept-assistive-system" title="Permalink to this headline">Â¶</a></h2>
<p>We also evaluated the use of our deep ConvNet as part of a assistive robot system where the brain-computer interface was sending high-level commands to a robotic arm. In this proof of concept system, the robotic arm could be instructed by the user via the BCI to fetch a cup and directly move the cup to the persons mouth to drink from it. An overview can be seen in <a class="reference internal" href="#robot-bci-overview-fig"><span class="std std-numref">Fig. 21</span></a>. Results from <a class="reference internal" href="#bci-robot-results"><span class="std std-numref">Table 8</span></a> show that 3 out of 4 subjects had a command accuracy of more than 75% and were able to reach the target using less than twice the steps of the minimal path through the GUI (path optimality &gt; %50%).</p>
<div class="figure align-default" id="robot-bci-overview-fig">
<img alt="_images/robot-bci-overview.png" src="_images/robot-bci-overview.png" />
<p class="caption"><span class="caption-number">Fig. 21 </span><span class="caption-text">Overview of the proof-of-concept assistive system from <span id="id4">[<a class="reference internal" href="References.html#id35">BFK+17</a>]</span> using the deep ConvNet in the BCI component. Robotic arm could be given high-level commands via the BCI, high-level commands were extracted from a knowledge base. The commands were then autonomously planned and executed by the robotic arm.</span><a class="headerlink" href="#robot-bci-overview-fig" title="Permalink to this image">Â¶</a></p>
</div>
<table class="colwidths-auto table" id="bci-robot-results">
<caption><span class="caption-number">Table 8 </span><span class="caption-text">Results for BCI control of the GUI. Accuracy is fraction of correct commands, time is time per command, steps is steps needed to reach the aim, path optimality is ratio of miniminally needed  nubmer of steps to actually used number of steps when every step is optimal, and time/step is time per step.</span><a class="headerlink" href="#bci-robot-results" title="Permalink to this table">Â¶</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Subject</p></th>
<th class="head"><p>Runs</p></th>
<th class="head"><p>Accuracy* [%]</p></th>
<th class="head"><p>Time [s]</p></th>
<th class="head"><p>Steps</p></th>
<th class="head"><p>Path Optimality [%]</p></th>
<th class="head"><p>Time/Step [s]</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>S1</p></td>
<td><p>18</p></td>
<td><p>84.1 Â± 6.1</p></td>
<td><p>125 Â± 84</p></td>
<td><p>13.0 Â± 7.8</p></td>
<td><p>70.1 Â± 22.3</p></td>
<td><p>9 Â± 2</p></td>
</tr>
<tr class="row-odd"><td><p>S2</p></td>
<td><p>14</p></td>
<td><p>76.8 Â± 14.1</p></td>
<td><p>150 Â± 32</p></td>
<td><p>10.1 Â± 2.8</p></td>
<td><p>91.3 Â± 12.0</p></td>
<td><p>9 Â± 3</p></td>
</tr>
<tr class="row-even"><td><p>S3</p></td>
<td><p>17</p></td>
<td><p>82.0 Â± 7.4</p></td>
<td><p>200 Â± 159</p></td>
<td><p>17.6 Â± 11.4</p></td>
<td><p>65.7 Â± 28.9</p></td>
<td><p>11 Â± 4</p></td>
</tr>
<tr class="row-odd"><td><p>S4</p></td>
<td><p>3</p></td>
<td><p>63.8 Â± 15.6</p></td>
<td><p>176 Â± 102</p></td>
<td><p>26.3 Â± 11.2</p></td>
<td><p>34.5 Â± 1.2</p></td>
<td><p>6 Â± 2</p></td>
</tr>
<tr class="row-even"><td><p>Average</p></td>
<td><p>13</p></td>
<td><p>76.7 Â± 9.1</p></td>
<td><p>148 Â± 50</p></td>
<td><p>16.7 Â± 7.1</p></td>
<td><p>65.4 Â± 23.4</p></td>
<td><p>9 Â± 2</p></td>
</tr>
<tr class="row-odd"><td><p><span id="id5">[<a class="reference internal" href="References.html#id35">BFK+17</a>]</span></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="intracranial-eeg-decoding">
<h2>Intracranial EEG decoding<a class="headerlink" href="#intracranial-eeg-decoding" title="Permalink to this headline">Â¶</a></h2>
<div class="section" id="intracranial-eeg-decoding-of-eriksen-flanker-task">
<h3>Intracranial EEG Decoding of Eriksen Flanker Task<a class="headerlink" href="#intracranial-eeg-decoding-of-eriksen-flanker-task" title="Permalink to this headline">Â¶</a></h3>
<p>We further evaluated whether the same networks developed for noninvasive EEG decoding can successfully learn to decode intracranial EEG. Therefore, in one work we used the same Eriksen flanker task as described in <a class="reference internal" href="#flanker-and-gui-section"><span class="std std-ref">Decoding of Eriksen Flanker Task Errors and Errors during Online GUI Control</span></a>, but recorded intracranial EEG from 23 patients who had pharmacoresistant epilepsy <span id="id6">[<a class="reference internal" href="References.html#id32">VHS+18</a>]</span>.</p>
<table class="colwidths-auto table" id="intracranial-error-results-table">
<caption><span class="caption-number">Table 9 </span><span class="caption-text">Results for single-channel intracranial decoding of errors during an Eriksen flanker task. Balanced Accuracy is the mean of the accuracies for correct class ground truth labels and error class ground truth labels.</span><a class="headerlink" href="#intracranial-error-results-table" title="Permalink to this table">Â¶</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Classifier</p></th>
<th class="head"><p>Balanced Accuracy  [%]</p></th>
<th class="head"><p>Accuracy Correct Class [%]</p></th>
<th class="head"><p>Accuracy Error Class  [%]</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Deep4Net</p></td>
<td><p>59.28 Â± 0.50</p></td>
<td><p>69.37 Â± 0.44</p></td>
<td><p>49.19 Â± 0.56</p></td>
</tr>
<tr class="row-odd"><td><p>ShallowNet</p></td>
<td><p>58.42 Â± 0.32</p></td>
<td><p>74.83 Â± 0.25</p></td>
<td><p>42.01 Â± 0.40</p></td>
</tr>
<tr class="row-even"><td><p>EEGNet</p></td>
<td><p>57.73 Â± 0.52</p></td>
<td><p>57.78 Â± 0.48</p></td>
<td><p>57.68 Â± 0.56</p></td>
</tr>
<tr class="row-odd"><td><p>rLDA</p></td>
<td><p>53.76 Â± 0.32</p></td>
<td><p>76.12 Â± 0.26</p></td>
<td><p>31.40 Â± 0.38</p></td>
</tr>
<tr class="row-even"><td><p>ResNet</p></td>
<td><p>52.45 Â± 0.21</p></td>
<td><p>95.47 Â± 0.14</p></td>
<td><p>09.43 Â± 0.28</p></td>
</tr>
<tr class="row-odd"><td><p><span id="id7">[<a class="reference internal" href="References.html#id32">VHS+18</a>]</span></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<div class="figure align-default" id="intracranial-error-results-fig">
<img alt="_images/IntracranialError.png" src="_images/IntracranialError.png" />
<p class="caption"><span class="caption-number">Fig. 22 </span><span class="caption-text">Results for all-channel intracranial decoding of errors during an Eriksen flanker task <span id="id8">[<a class="reference internal" href="References.html#id32">VHS+18</a>]</span>.</span><a class="headerlink" href="#intracranial-error-results-fig" title="Permalink to this image">Â¶</a></p>
</div>
</div>
<div class="section" id="transfer-learning-for-intracranial-error-decoding">
<h3>Transfer Learning for Intracranial Error Decoding<a class="headerlink" href="#transfer-learning-for-intracranial-error-decoding" title="Permalink to this headline">Â¶</a></h3>
<div class="figure align-default" id="eriksen-flanker-car-driving-tasks-fig">
<img alt="_images/eriksen-flanker-car-driving-tasks.png" src="_images/eriksen-flanker-car-driving-tasks.png" />
<p class="caption"><span class="caption-number">Fig. 23 </span><span class="caption-text">Sketch of the Eriksen flanker task (A) and screenshot of the car driving task (B). <span id="id9">[<a class="reference internal" href="References.html#id31">BSV+18</a>]</span>.</span><a class="headerlink" href="#eriksen-flanker-car-driving-tasks-fig" title="Permalink to this image">Â¶</a></p>
</div>
<p>We further tested the potential of ConvNets to transfer knowledge learned from decoding intracranial signals in error-decoding paradigm to decoding signals in another a different error-decoding paradigm <span id="id10">[<a class="reference internal" href="References.html#id31">BSV+18</a>]</span>. The two error-decoding paradigms were the aforementioned Eriksen flanker task (EFT) and a car driving task (CDT), where subjects had to use a steering wheel to steer a car in a computer game and avoid hitting obstacles, where hitting an obstacle was considered an error event (see <a class="reference internal" href="#eriksen-flanker-car-driving-tasks-fig"><span class="std std-numref">Fig. 23</span></a>). Results in <a class="reference internal" href="#cross-training-eft-cdt-results-fig"><span class="std std-numref">Fig. 24</span></a> show that pretraining on CDT helps EFT decoding when few EDT data is available.</p>
<div class="figure align-default" id="cross-training-eft-cdt-results-fig">
<img alt="_images/cross-training-eft-cdt-results.png" src="_images/cross-training-eft-cdt-results.png" />
<p class="caption"><span class="caption-number">Fig. 24 </span><span class="caption-text">Results for transfer learning on the Eriksen flanker task (EFT) and the car driving task (CDT) <span id="id11">[<a class="reference internal" href="References.html#id31">BSV+18</a>]</span>. All results are computed for a varying fraction of available data for the target decoding task (bottom row). <strong>A</strong> compares CDT accuracies after training only on CDT or pretraining on EFT and  finetuning on CDT. <strong>B</strong> compares EFT accuracies after only training on EFT or after  pretraining on CDT and finetuning on EFT. As a sanity check for the results in <strong>B</strong>, <strong>C</strong> compares EFT accuracies when pretraining on original CDT data and finetuning on EFT to pretraining on CDT data with shuffled labels (CDT*) and finetuning on EFT. Results show that pretraining on CDT helps EFT decoding when little EFT data is available.</span><a class="headerlink" href="#cross-training-eft-cdt-results-fig" title="Permalink to this image">Â¶</a></p>
</div>
</div>
<div class="section" id="microelectrocorticography-decoding-of-auditory-evoked-responses-in-sheep">
<h3>Microelectrocorticography decoding of auditory evoked responses in sheep<a class="headerlink" href="#microelectrocorticography-decoding-of-auditory-evoked-responses-in-sheep" title="Permalink to this headline">Â¶</a></h3>
<div class="figure align-default" id="sheep-sounds-fig">
<img alt="_images/sheep-sounds.jpg" src="_images/sheep-sounds.jpg" />
<p class="caption"><span class="caption-number">Fig. 25 </span><span class="caption-text">Overview over decoding tasks for auditory evoked responses in a sheep <span id="id12">[<a class="reference internal" href="References.html#id30">WGS+18</a>]</span>. First task (top) was to distingish 3 seconds when the sound was playing from the second before and the second after. Second task (bottom) was to distinguish the first, second and third second during theplaying of the sound. Signals are averaged responses from one electrode during different days, with black and grey being signals while the sheep was awake and red ones while the sheep was under general anesthesia.</span><a class="headerlink" href="#sheep-sounds-fig" title="Permalink to this image">Â¶</a></p>
</div>
<p>In this study, we evaluated the ConvNets for decoding auditory evoked responses played to a sheep that was chronically implanted with  a Î¼ECoG-based neural interfacing device <span id="id13">[<a class="reference internal" href="References.html#id30">WGS+18</a>]</span>. 3-seconds-long sounds were presented to the sheep and two decoding tasks were defined from those 3 seconds as well as the second immediately before and after the playing of the sound. The first decoding task was to distinguish the 3 seconds when the sound was playing from the second  immediately before and the second immediately after the sound. The second task was distinguishing the first, second and third second of the playing of the sound to discriminate early, intermediate and late auditory evoked response (see <a class="reference internal" href="#sheep-sounds-fig"><span class="std std-numref">Fig. 25</span></a>). Results in <a class="reference internal" href="#sheep-accuracies-fig"><span class="std std-numref">Fig. 26</span></a> show that the  deep ConvNet can perform as good as FBSCP and rLDA, and perform well on both tasks, whereas rLDA performs competitively only on the first and FBSCP only on the second task.</p>
<div class="figure align-default" id="sheep-accuracies-fig">
<img alt="_images/sheep-accuracies.png" src="_images/sheep-accuracies.png" />
<p class="caption"><span class="caption-number">Fig. 26 </span><span class="caption-text">Results of decoding auditory evoked responses from sheep with rlDA and FBSCP or the deep ConvNet. Open circles represent accuracies for individual experiment days and closed circles represent the average over these accuracies.</span><a class="headerlink" href="#sheep-accuracies-fig" title="Permalink to this image">Â¶</a></p>
</div>
</div>
</div>
<div class="section" id="evaluation-on-large-scale-task-diverse-dataset">
<h2>Evaluation on large-scale task-diverse dataset<a class="headerlink" href="#evaluation-on-large-scale-task-diverse-dataset" title="Permalink to this headline">Â¶</a></h2>
<table class="colwidths-auto table" id="large-framework-overview-table">
<caption><span class="caption-number">Table 10 </span><span class="caption-text">Datasets for the large-scale evaluation framework.</span><a class="headerlink" href="#large-framework-overview-table" title="Permalink to this table">Â¶</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Name (Acronym)</p></th>
<th class="head"><p>#Classes</p></th>
<th class="head"><p>Task Type</p></th>
<th class="head"><p>#Subjects</p></th>
<th class="head"><p>Trials per Subject</p></th>
<th class="head"><p>Class balance</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>High-Gamma Dataset (Motor)</p></td>
<td><p>4</p></td>
<td><p>Motor task</p></td>
<td><p>20</p></td>
<td><p>1000</p></td>
<td><p>balanced</p></td>
</tr>
<tr class="row-odd"><td><p>KUKA Pouring Observation (KPO)</p></td>
<td><p>2</p></td>
<td><p>Error observation</p></td>
<td><p>5</p></td>
<td><p>720-800</p></td>
<td><p>balanced</p></td>
</tr>
<tr class="row-even"><td><p>Robot-Grasping Observation (RGO)</p></td>
<td><p>2</p></td>
<td><p>Error observation</p></td>
<td><p>12</p></td>
<td><p>720-800</p></td>
<td><p>balanced</p></td>
</tr>
<tr class="row-odd"><td><p>Error-Related Negativity (ERN)</p></td>
<td><p>2</p></td>
<td><p>Eriksen flanker task</p></td>
<td><p>31</p></td>
<td><p>1000</p></td>
<td><p>1/2 up to 1/15</p></td>
</tr>
<tr class="row-even"><td><p>Semantic Categories</p></td>
<td><p>3</p></td>
<td><p>Speech imagery</p></td>
<td><p>16</p></td>
<td><p>750</p></td>
<td><p>balanced</p></td>
</tr>
<tr class="row-odd"><td><p>Real vs. Pseudo Words</p></td>
<td><p>2</p></td>
<td><p>Speech imagery</p></td>
<td><p>16</p></td>
<td><p>1000</p></td>
<td><p>3/1</p></td>
</tr>
<tr class="row-even"><td><p><span id="id14">[<a class="reference internal" href="References.html#id29">HSF+18</a>]</span></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<p>We also compared the deep and shallow ConvNet architectures as well as EEGNet on six classification tasks with more than
90000 trials in total (see <a class="reference internal" href="#large-framework-overview-table"><span class="std std-numref">Table 10</span></a>) <span id="id15">[<a class="reference internal" href="References.html#id29">HSF+18</a>]</span>. The datasets tasks were all recorded in our lab and included the high-gamma dataset, three error-related tasks described before (Eriksen flanker task, robot grasping and robot pouring observations) as well as two tasks on semantic processing. In the semantic processing dataset, the classification tasks were to distinguish different types of words that a subject silently repeated <span id="id16">[<a class="reference internal" href="References.html#id27">Rau15</a>]</span>. The first task was to distinguish existing real words from nonexisting pseudowords. The second classification task was to distingiush three semantic categories (food, animals, tools) the word may belong to. The evaluation code for all models always used the original code and hyperparameters from the original studies in order to ensure a fair comparison. Results show that the deep ConvNet and the more recent version of EEGNet (EEGNetv2) perform similarly well, with shallow and an older version of EEGNet performing slightly worse, see  <a class="reference internal" href="#large-framework-per-dataset-results-fig"><span class="std std-numref">Fig. 27</span></a>, <a class="reference internal" href="#large-framework-averaged-results-fig"><span class="std std-numref">Fig. 28</span></a>  and <a class="reference internal" href="#large-framework-results-table"><span class="std std-numref">Table 11</span></a>.</p>
<div class="figure align-default" id="large-framework-per-dataset-results-fig">
<img alt="_images/large-framework-per-dataset-results.png" src="_images/large-framework-per-dataset-results.png" />
<p class="caption"><span class="caption-number">Fig. 27 </span><span class="caption-text">Per-dataset results for the large-scale evaluation of deep ConvNet, shallow ConvNet and two versions of EEGNet. Boxplots show the distribution over per-subject accuracies for the individual decoding tasks. ern, kpo and rgo are the error-related datasets, ern: Error-related negativity Eriksen flanker task, KPO: KUKA Pouring Observation paradigm, rgo: robot-grasping observation paradigm. motor is the high-gamma dataset with 6 additional subjects that were excluded for data quality reasons from <span id="id17">[<a class="reference internal" href="References.html#id28">SSF+17</a>]</span>. pseudovsreal and semantic are two semantic processing datasets to classify silent repetitions of  pseudowords vs. realwords (pseudovsreal) or different semantic categories (semantic) .</span><a class="headerlink" href="#large-framework-per-dataset-results-fig" title="Permalink to this image">Â¶</a></p>
</div>
<div class="figure align-default" id="large-framework-averaged-results-fig">
<img alt="_images/large-framework-averaged-results.png" src="_images/large-framework-averaged-results.png" />
<p class="caption"><span class="caption-number">Fig. 28 </span><span class="caption-text">Dataset-averaged results for the large-scale evaluation of deep ConvNet, shallow ConvNet and two versions of EEGNet. Accuracies are normalized to the average of the accuracies of all models.</span><a class="headerlink" href="#large-framework-averaged-results-fig" title="Permalink to this image">Â¶</a></p>
</div>
<table class="colwidths-auto table" id="large-framework-results-table">
<caption><span class="caption-number">Table 11 </span><span class="caption-text">Dataset-averaged results for the large-scale evaluation of deep ConvNet, shallow ConvNet and two versions of EEGNet. Accuracies are normalized to the average of the accuracies of all models.</span><a class="headerlink" href="#large-framework-results-table" title="Permalink to this table">Â¶</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>Mean accuracy</p></th>
<th class="head"><p>Mean normalized accuracy</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Deep ConvNet</p></td>
<td><p>70.08% Â± 20.92%</p></td>
<td><p>1.00 Â± 0.05</p></td>
</tr>
<tr class="row-odd"><td><p>EEGNetv2</p></td>
<td><p>70.00% Â±18.86%</p></td>
<td><p>1.02 Â± 0.08</p></td>
</tr>
<tr class="row-even"><td><p>EEGNet</p></td>
<td><p>67.71% Â± 19.04%</p></td>
<td><p>0.98 Â± 0.06</p></td>
</tr>
<tr class="row-odd"><td><p>Shallow ConvNet</p></td>
<td><p>67.71% Â±19.04%</p></td>
<td><p>0.99 Â± 0.06</p></td>
</tr>
<tr class="row-even"><td><p><span id="id18">[<a class="reference internal" href="References.html#id29">HSF+18</a>]</span></p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="MovementDecoding.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title">Decoding Movement-Related Brain Activity</p>
            </div>
        </a>
    </div>
     <div id="next">
        <a class="right-next" href="Pathology.html" title="next page">
            <div class="prevnext-info">
                <p class="prevnext-label">next</p>
                <p class="prevnext-title">Decoding Pathology</p>
            </div>
            <i class="prevnext-label fas fa-angle-right"></i>
        </a>
     </div>

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By The Jupyter Book community<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>