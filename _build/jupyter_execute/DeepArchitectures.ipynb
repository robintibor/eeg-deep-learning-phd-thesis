{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40e140aa",
   "metadata": {},
   "source": [
    "(network-architectures)=\n",
    "# Neural Network Architectures for EEG-Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728ebf37",
   "metadata": {},
   "source": [
    "We continued developing our neural network architectures with our EEG-specific development strategy of starting with networks that resemble feature-based algorithms. After the filterbank network from the master thesis, we adapted the so-called shallow network, initally also developed in the same master thesis. The shallow network still resembles filter bank common spatial patterns, but less closely than the filterbank network. After validating that these initial network architectures perform as well as filter bank common spatial patterns, we progressed to developing and evaluating more generic architectures.\n",
    "\n",
    "In this section, I describe the architectures presented in our first publication on EEG deep learning decoding {cite}`schirrmeisterdeephbm2017`. This part uses text and figures from {cite}`schirrmeisterdeephbm2017` adapted for readibility in this thesis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69d020e",
   "metadata": {},
   "source": [
    "## Shallow Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd383c5c",
   "metadata": {},
   "source": [
    "We developed the shallow network architecture, a more flexible architecture than the filterbank network that also learns temporal filters on the input signal and on the later representation. Instead of bandpass-filtered signals, it is fed the raw signals as input. The first step are learnable temporal filters that are indepedently convolved with the signals of each EEG electrode. Afterwards, the channel dimension of the network representation    contains $\\mathrm{electrodes} \\cdot \\mathrm{temporal~ filters}$ channels. In the next step that combines spatial filtering with mixing the outputs of the temporal filters, this network-channel dimension is linearly transformed by learned weights to a smaller dimensionality for further preprocessing. The resulting feature timeseries are then squared, average-pooled and log-transformed, which allows the network to more easily learn log-variance-based features. Unlike the filterbank network, the average pooling does not collapse the feature timeseries into one value per trial. So after these processing steps, still some temporal information about the timecourse of the variance throughout the trial can be preserved. Then, the final classification layer transforms these feature timecourses into class probabilities using a linear transformation and a softmax function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3f4e7f",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "![title](images/3D_Diagram_MatplotLib.ipynb.0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42076922",
   "metadata": {},
   "source": [
    "```{figure} images/3D_Diagram_MatplotLib.ipynb.0.png\n",
    "---\n",
    "name: shallow-net-figure\n",
    "width: 50%\n",
    "---\n",
    "Shallow network architecture, figure from [ref].  EEG input (at the top) is progressively transformed toward the bottom, until the final classifier output. Black cuboids: inputs/feature maps; brown cuboids: convolution/pooling kernels. The corresponding sizes are indicated in black and brown, respectively. Note that the final dense layer operates only on a small remaining temporal dimension, making it similar to a regular convolutional layer.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bae5008",
   "metadata": {},
   "source": [
    "## Deep Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b131e7",
   "metadata": {},
   "source": [
    "The deep architecture is a more generic architecture, closer to network architectures used in computer vision. The first two temporal convolution and spatial filtering layers are the same in the shallow network, which is followed by a ELU nonlinearity (ELUs, $f(x)=x$ for $x > 0$ and $f(x) = e^x-1$ for $x <= 0$ {cite}`clevert_fast_2016`) and max pooling. The following three blocks simply consist of a convolution, a ELU nonlinearity and a max pooling. In the end, there is again a final linear classification layer with a softmax function. Due to its less specific and more generic computational steps, the deep architecture should be able to capture a large variety of features. Hence, the learned features may also be less biased towards the amplitude features commonly used in task-related EEG decoding. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c431a4f4",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "![title](images/3D_Diagram_MatplotLib.ipynb.1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12791f5",
   "metadata": {},
   "source": [
    "```{figure} images/3D_Diagram_MatplotLib.ipynb.1.png\n",
    "---\n",
    "name: deep-net-figure\n",
    "width: 75%\n",
    "---\n",
    "Deep network architecture, figure from {cite}`schirrmeisterdeephbm2017`. Conventions as in {numref}`shallow-net-figure`.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851124ff",
   "metadata": {},
   "source": [
    "## Residual Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9262185",
   "metadata": {},
   "source": [
    "We also developed a residual network (ResNet {cite}`he_deep_2015`) for EEG decoding. We use the same residual blocks as the original paper, described in Figure {numref}`residual-net-figure`. Our ResNet used ELU activation functions throughout the network (same as the deep ConvNet) and also starts with a splitted temporal and spatial convolution (same as the deep and shallow ConvNets), followed by 14 residual blocks, mean pooling and a final softmax dense classification layer (for further details, see Supporting Information, Section A.3 in {cite}`schirrmeisterdeephbm2017`). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725281e1",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "![title](images/residual_block.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94120cfd",
   "metadata": {},
   "source": [
    "```{figure} images/residual_block.png\n",
    "---\n",
    "name: residual-net-figure\n",
    "---\n",
    "\n",
    "Residual block, Figure from {cite}`schirrmeisterdeephbm2017`. \"Residual block used in the ResNet architecture and as described in original paper ({cite}`he_deep_2015`; see Fig. 2) with identity shortcut option A, except using ELU instead of ReLU nonlinearities.\"\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}