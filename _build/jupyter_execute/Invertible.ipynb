{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "847ddbb5",
   "metadata": {},
   "source": [
    "(invertible-networks)=\n",
    "# Invertible Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dff48f-5b2f-4c24-b5eb-7498f80b4a23",
   "metadata": {},
   "source": [
    "Invertible networks are networks that are invertible by design, i.e., any network output can be mapped back to a corresponding input [refs] bijectively. The ability to invert any output back to the input enables different interpretability methods and furthermore allows training invertible networks as generative models via maximum likelihood. \n",
    "\n",
    "This chapter starts by explaining what invertible layers are used to design invertible networks, proceeds to detail their training methodologies as generative models or classifiers, and goes on to outline interpretability techniques that help reveal the learned features crucial for their classification tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165beff0-eb29-4251-ab37-5a3a4a99fdab",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c3548a-9bf1-44cb-95b6-09614c9c8494",
   "metadata": {},
   "source": [
    "* invertible layers\n",
    "    * explain invertible layers, maybe bold part in front\n",
    "    * (mention wavelet permutation also)\n",
    "* example for volume change, do make it (use seaborn colors, in draw.io)\n",
    "    * do consider making some bar plots with matplotlib why not\n",
    "* architecture diagram\n",
    "    * what is one block? Haar Wavelet addition\n",
    "    * Latent gaussian at end, no don't show\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9615b982-18cb-4f90-b0be-6c083ddb80d4",
   "metadata": {},
   "source": [
    "### Invertible layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1676dde5-1d2b-49b5-9ee0-128816a0cd62",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "% Forward\n",
    "y_1 &= x_1 + f(x_2); \\quad \\text{Forward comment 1} \\\\\n",
    "y_2 &= x_2; \\quad \\text{Forward comment 2} \\\\\n",
    "% Inverse\n",
    "x_1 &= y_1 - f(y_2); \\quad \\text{Inverse comment 1} \\\\\n",
    "x_2 &= y_2; \\quad \\text{Inverse comment 2} \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d7b50a-668f-4031-8bd4-4489e47e3000",
   "metadata": {},
   "source": [
    "$\\require{color}$\n",
    "\n",
    "$\\definecolor{commentcolor}{RGB} {70,130,180}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6438331-1b53-4fd8-9de4-0f4f5f9feef2",
   "metadata": {},
   "source": [
    "[Figure coupling blocks]\n",
    "* you could have a figure here with time series and half of time series computed sth, then additive coefficient added \n",
    "\n",
    "Invertible networks use layers constructed specifically to maintain invertibility, thereby rendering the entire network structure invertible. Often-used invertible layers are coupling layers, invertible linear layers and activation normalization layers.\n",
    "\n",
    "**Coupling layers** split a multidimensional input $x$ into two parts  $x_1$ and $x_2$ with disjoint dimensions and use $x_2$ to compute an invertible transformation to apply to $x_1$. Concretely, for an additive coupling block, the forward computation is like this:\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "    y_1 &= x_1 + f(x_2); && \\color{commentcolor}{\\text{Compute $y_1$ from $x_1$ and arbitrary function of $x_2$}} \\\\\n",
    "    y_2 &= x_2 && \\color{commentcolor}{\\text{Leave } x_2 \\text{ unchanged}} \\\\\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "The inverse computation is:\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "    x_1 &= y_1 - f(y_2) && \\color{commentcolor}{\\text{Invert to } x_1 \\text{ using unchanged } y_2=x_2} \\\\\n",
    "    x_2 &= y_2 &&  \\color{commentcolor}{x_2 \\text{ was unchanged}}\\\\\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "of $d_1$ and $d_2$ dimensions with $d_1 + d_2 = d$.\n",
    "\n",
    "For example in a timeseries one may take the input at all the even time indices as $x_1$ and all the odd time indices as $x_2$ odd time indices. Then, one transforms $x_1$ in an invertible way (e.g., by adding something to it) based on computations only performed on $x_2$, while leaving $x_2$ unchanged. For, example additive coupling works as follows:\n",
    "$y_1 = x_1 + f(x_2); y_2=x_2$, with $f$ being an arbitrary function, e.g., any (potentially non-invertible) neural network. It can be inverted given $y_1$ and $y_2$ as follows: $x_1 = y_1 - f(y_2); x_2=y_2$. Instead of addition one may use any other invertible function, a common one is an affine transformation where $f$ produces translation and scaling coefficients $f_t$ and $f_s$:\n",
    "$y_1 = x_1 \\cdot f_s(x_2) + f_t(x_2); y_2=x_2$, with inversion $x_1 = \\frac{(y_1  - f_t(x_2))}{f_s(x_2)}; y_2=x_2$. \n",
    "The splitting of dimensions can be done in multiply ways, like using odd or even indices or using difference and mean between two neighbouring samples (akin to one stage of a Haar Wavelet).\n",
    "\n",
    "Invertible linear layers compute an invertible linear transformation (an automorphism) of their input. Concretely they multiply a $d$-dimensional vector $\\mathbf{x}$ with a $dxd$-dimensional matrix $W$, where the $W$ has to be invertible, i.e., have nonzero determinant. \n",
    "$y=W \\mathbf{x}$ with inverse $x=W^{-1} \\mathbf{y}$. For multidimensional arrays like feature maps in a convolutional network, these linear transformations are usually done per-position, as so-called invertible 1x1 convolutions in the 2d case.\n",
    "\n",
    "Activation normalization layers perform an affine transformation with learned parameters, e.g., $y=x\\cdot s+t$, with $s$ and $t$ learned scaling and translation parameters. These have also been used to replace batch normalization and initialized data-dependently to maintain unit variance at the beginning of training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bded14d4-187c-4395-b8b4-95331126d87f",
   "metadata": {},
   "source": [
    "## Generative models via maximum likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df19f6c-3d40-486f-938a-14db8c0ae4b7",
   "metadata": {},
   "source": [
    "Invertible networks can also be trained as generative models via maximum likelihood. In maximum likelihood training, the network is optimized to maximize the probability densities of the training inputs. For that, the network maps the inputs into a latent space such that the probability densities are maximized under a predefined prior within this latent space, e.g., a Gaussian distribution. In addition, for these probability densities to form a valid probability distribution in the input space, one has to account for how much the network's mapping function squeezes and expands volume. We'll proceed to illustrate this in an example below.\n",
    "\n",
    "\n",
    "\n",
    "$p(x) = p_\\textrm{prior}(f(x)) \\cdot  | \\det \\left( \\frac{\\partial \\mathbf{f}}{\\partial \\mathbf{x}} \\right)|$\n",
    "\n",
    "\n",
    "* figure for that (map samples to latent space, shift achagne volume, latent dist just 3 bars, invert show how dist looks in input soace, what it integrates to \n",
    "* etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd8335f-c18d-4f8c-aec9-34151135be79",
   "metadata": {},
   "source": [
    "## Generative classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b2d33c-7201-44bc-8d36-f4248611b7da",
   "metadata": {},
   "source": [
    "Invertible networks trained as class-conditional generative models can also be used as classifiers. Class-conditional generative networks may be implemented in different ways, for example with a separate prior in latent space for each class. Given the class-conditional probability densities $p(x|c_i)$, one can obtain class probabilities via Bayes formula as $p(c_i|x)=\\frac{p(x|c_i)}{\\sum_jp(x|c_j)}$.\n",
    "\n",
    "Pure class-conditional generative training may not yield networks that perform well as classifiers. The reason is that at least in theory, the relative reductions in maximum likelihood loss one obtains from knowing the class label are very small for high-dimensional inputs, for example much smaller than typical differences between two runs of the same network [REF]. This is understandable from a compression perspective, so using that under Shannon's theorem more probable inputs need less bits to encode than less likely inputs, or more precisely $\\textrm{Number of bits needed}(x) = \\log_2 p(x)$. Even if you assume some one needs only 1 bit per dimension, a high-dimensional input like our 2688-dimensional EEG signal will need 2688 bits to encode. How many bits are needed for the class information? To distinguish between n classes, one needs only $\\log_2(n)$ bits, so in case of binary pathology classification, only 1 bit is needed, therefore the optimal class-conditional model will only be 1 bit better than the optimal class-independent model. In contrast, the loss difference between two training runs of the same network will typically be at least 1 to two orders of magnitude larger. In practice, the gains from using a class-conditional model, by e.g., using a separate prior per class in latent space, are usually larger, but it is not a priori clear if the reductions in loss from exploiting the class label are high enough to result in a good classification model.\n",
    "\n",
    "Various methods have been proposed to improve the performance of using generative classifiers. For example, people have fixed the per-class latent gaussian priors so that they retain the same distance throughout training [Ref Pavel] or added a classification loss term $L_class(x)=\\log_2 (...etc softmax) $ to the training [Ref VIB heidelberg]. In our work, we experimented with adding a classification loss term to the training, and also found using a learned temperature before the softmax helps the training. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95592866-22d8-4a2f-bdfb-d4cf22cd5dbf",
   "metadata": {},
   "source": [
    "## Invertible Network for EEG Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e8614e-a6a4-4f03-ae38-fa49c405792b",
   "metadata": {},
   "source": [
    "We designed an invertible network for EEG Decoding using invertible components used in the literature, primarily from the Glow architecture [REF]. Our architecture consists of three stages that operate on sequentially lower temporal resolutions. Similar to glow, the individual stages consists of several blocks of Activation Normalization, Invertible Linear Channel Transformations and Coupling Layers. Between each stage, we downsample by computing the mean and difference of two neighbouring timepoints and moving these into the channel dimension. Unlike Glow, we keep processing all dimensions throughout all stages, finding this architecture to reach competitive accuracy on pathology decoding.\n",
    "\n",
    "[diagram]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241b96f2-7862-4152-96ea-25b18a9d2bc0",
   "metadata": {},
   "source": [
    "Training and dataset details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4a8ee3-3fa9-402a-884b-e89fb820b1c9",
   "metadata": {},
   "source": [
    "Prototypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78153402-b152-4351-979f-29aecbad88b9",
   "metadata": {},
   "source": [
    "Per-Chan Prototypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cfb47e-494a-41c6-880f-fc948685191e",
   "metadata": {},
   "source": [
    "EEG CosNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5c1c18-7ec6-4bb6-bec7-1b216ed3919c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e70d734-5547-4f74-b646-52c58bd4b59c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401c7685-4ab4-44d8-a061-97cae7974b67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4b7fef-876a-411c-ae39-01c16f4f0faa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2ceb97-4da8-4117-863e-4e7b8d142000",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e85ed6c-79ec-4cb6-9970-8e788c3ae865",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45bd947-13d4-4710-9bf9-f228d04bcb5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6031363-0124-4521-9cd5-3f63ad962c79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870c7ebe-189c-4c4e-a462-8c314e6fdb6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac89b892-57bd-4912-a491-eb7bfe11e846",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}