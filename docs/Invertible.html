

<!DOCTYPE html>


<html lang="en" data-theme="light">

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Invertible Networks &#8212; Deep Learning for Brain-Signal Decoding from Electroencephalography (EEG)</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "light";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="_static/.ipynb_checkpoints/custom-checkpoint.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script async="async" kind="hypothesis" src="https://hypothes.is/embed.js"></script>
    <script kind="utterances">

    var commentsRunWhenDOMLoaded = cb => {
    if (document.readyState != 'loading') {
        cb()
    } else if (document.addEventListener) {
        document.addEventListener('DOMContentLoaded', cb)
    } else {
        document.attachEvent('onreadystatechange', function() {
        if (document.readyState == 'complete') cb()
        })
    }
}

var addUtterances = () => {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src = "https://utteranc.es/client.js";
    script.async = "async";

    script.setAttribute("repo", "robintibor/eeg-deep-learning-phd-thesis");
    script.setAttribute("issue-term", "pathname");
    script.setAttribute("theme", "github-light");
    script.setAttribute("label", "💬 comment");
    script.setAttribute("crossorigin", "anonymous");

    sections = document.querySelectorAll("div.section");
    if (sections !== null) {
        section = sections[sections.length-1];
        section.appendChild(script);
    }
}
commentsRunWhenDOMLoaded(addUtterances);
</script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'Invertible';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Decoding Movement-Related Brain Activity" href="MovementDecoding.html" />
    <link rel="prev" title="Perturbation Visualization" href="PerturbationVisualization.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="light">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="Abstract.html">
  
  
  
  
    
    
    
    <img src="_static/braindecode-logo.svg" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="_static/braindecode-logo.svg" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="Introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="PriorWork.html">Prior Work</a></li>
<li class="toctree-l1"><a class="reference internal" href="FBCSPAndFBCSPNet.html">Filter Bank Common Spatial Patterns and Filterbank Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="DeepArchitectures.html">Neural Network Architectures for EEG-Decoding</a></li>
<li class="toctree-l1"><a class="reference internal" href="CroppedTraining.html">Cropped Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="PerturbationVisualization.html">Perturbation Visualization</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Invertible Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="MovementDecoding.html">Decoding Movement-Related Brain Activity</a></li>
<li class="toctree-l1"><a class="reference internal" href="TaskDecoding.html">Generalization to Other Tasks</a></li>
<li class="toctree-l1"><a class="reference internal" href="Pathology.html">Decoding Pathology</a></li>
<li class="toctree-l1"><a class="reference internal" href="UnderstandingPathology.html">Understanding Pathology Decoding With Invertible Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="FutureWork.html">Future Work</a></li>
<li class="toctree-l1"><a class="reference internal" href="References.html">References</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/Invertible.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Invertible Networks</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#invertible-layers">Invertible Layers</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#generative-models-via-maximum-likelihood">Generative models via maximum likelihood</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#de-quantization">(De)quantization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#volume-change">Volume Change</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#generative-classifiers">Generative classifiers</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#invertible-network-for-eeg-decoding">Invertible Network for EEG Decoding</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#class-prototypes">Class Prototypes</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#per-electrode-prototypes">Per-Electrode Prototypes</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#eeg-cosnet">EEG CosNet</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  
<style>
  .ss-layout-default-AB_CD { grid-template-areas: 'A B' 'C D'; }
  .ss-layout-default-AB { grid-template-areas: 'A B'; }
</style>
<div class="tex2jax_ignore mathjax_ignore section" id="invertible-networks">
<span id="id1"></span><h1>Invertible Networks<a class="headerlink" href="#invertible-networks" title="Permalink to this heading">#</a></h1>
<p>Invertible networks are networks that are invertible by design, i.e., any network output can be mapped back to a corresponding input [refs] bijectively. The ability to invert any output back to the input enables different interpretability methods and furthermore allows training invertible networks as generative models via maximum likelihood.</p>
<p>This chapter starts by explaining invertible layers that are used to design invertible networks, proceeds to detail training methodologies for invertible networks as generative models or classifiers, and goes on to outline interpretability techniques that help reveal the learned features crucial for their classification tasks.
<span class="math notranslate nohighlight">\(\require{color}\)</span>
<span class="math notranslate nohighlight">\(\definecolor{commentcolor}{RGB} {70,130,180}\)</span></p>
<div class="section" id="invertible-layers">
<h2>Invertible Layers<a class="headerlink" href="#invertible-layers" title="Permalink to this heading">#</a></h2>
<p>Invertible networks use layers constructed specifically to maintain invertibility, thereby rendering the entire network structure invertible. Often-used invertible layers are coupling layers, invertible linear layers and activation normalization layers.</p>
<p><strong>Coupling layers</strong> split a multidimensional input <span class="math notranslate nohighlight">\(x\)</span> into two parts  <span class="math notranslate nohighlight">\(x_1\)</span> and <span class="math notranslate nohighlight">\(x_2\)</span> with disjoint dimensions and then use <span class="math notranslate nohighlight">\(x_2\)</span> to compute an invertible transformation for <span class="math notranslate nohighlight">\(x_1\)</span>. Concretely, for an additive coupling layer, the forward computation is:</p>
<p><span class="math notranslate nohighlight">\(
\begin{align*}
    y_1 &amp;= x_1 + f(x_2) &amp;&amp; \color{commentcolor}{\text{Compute } y_1 \text{ from } x_1 \text{ and arbitrary function f of } x_2} \\
    y_2 &amp;= x_2 &amp;&amp; \color{commentcolor}{\text{Leave } x_2 \text{ unchanged}} \\
\end{align*}
\)</span></p>
<p>The inverse computation is:</p>
<p><span class="math notranslate nohighlight">\(
\begin{align*}
    x_1 &amp;= y_1 - f(y_2) &amp;&amp; \color{commentcolor}{\text{Invert to } x_1 \text{ using unchanged } y_2=x_2} \\
    x_2 &amp;= y_2 &amp;&amp;  \color{commentcolor}{x_2 \text{ was unchanged}}\\
\end{align*}
\)</span></p>
<p>For the splitting of the dimensions in a timeseries, there are multiple ways, such as using the even time indices as <span class="math notranslate nohighlight">\(x_1\)</span> and all the odd time indices as <span class="math notranslate nohighlight">\(x_2\)</span> or using difference and mean between two neighbouring samples (akin to one stage of a Haar Wavelet). The function <span class="math notranslate nohighlight">\(f\)</span> is usually implemented by a neural network, in our cases it will be small convolutional networks. Instead of addition any other invertible function can be used, affine transformation are commonly used, where <span class="math notranslate nohighlight">\(f\)</span> produces translation and scaling coefficients <span class="math notranslate nohighlight">\(f_t\)</span> and <span class="math notranslate nohighlight">\(f_s\)</span>:</p>
<p><span class="math notranslate nohighlight">\(
\begin{align*}
    y_1 &amp;= x_1 \cdot f_s(x_2) + f_t(x_2) &amp;&amp; \text{ } y_2=x_2 &amp;&amp; \color{commentcolor}{\text{Affine Forward }} \\
    \\
    x_1 &amp;= \frac{(y_1  - f_t(y_2))}{f_s(y_2)} &amp;&amp; \text{ } x_2=y_2 &amp;&amp; \color{commentcolor}{\text{Affine Inverse}} \\
\end{align*}
\)</span></p>
<p><strong>Invertible linear layers</strong> compute an invertible linear transformation (an automorphism) of their input. Concretely they multiply a <span class="math notranslate nohighlight">\(d\)</span>-dimensional vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> with a <span class="math notranslate nohighlight">\(dxd\)</span>-dimensional matrix <span class="math notranslate nohighlight">\(W\)</span>, where <span class="math notranslate nohighlight">\(W\)</span> has to be invertible, i.e., have nonzero determinant.</p>
<p><span class="math notranslate nohighlight">\(
\begin{align*}
    y&amp;=W \mathbf{x} &amp;&amp; \color{commentcolor}{\text{Linear Forward }} \\
    x&amp;=W^{-1} \mathbf{y} &amp;&amp; \color{commentcolor}{\text{Linear Inverse}} \\
\end{align*}
\)</span></p>
<p>For multidimensional arrays like feature maps in a convolutional network, these linear transformations are usually done per-position, as so-called invertible 1x1 convolutions in the 2d case.</p>
<p><strong>Activation normalization layers</strong> perform an affine transformation with learned parameters with <span class="math notranslate nohighlight">\(s\)</span> and <span class="math notranslate nohighlight">\(t\)</span> learned scaling and translation parameters (independent of the input <span class="math notranslate nohighlight">\(x\)</span>):</p>
<p><span class="math notranslate nohighlight">\(
\begin{align*}
    y&amp;=x \cdot{s} + t &amp;&amp; \color{commentcolor}{\text{ActNorm Forward }} \\
    x&amp;=\frac{y - t}{s} &amp;&amp; \color{commentcolor}{\text{ActNorm Inverse}} \\
\end{align*}
\)</span></p>
<p>These have also been used to replace batch normalization and are often initialized data-dependently to have standard-normalized activations at the beginning of training.</p>
</div>
<div class="section" id="generative-models-via-maximum-likelihood">
<h2>Generative models via maximum likelihood<a class="headerlink" href="#generative-models-via-maximum-likelihood" title="Permalink to this heading">#</a></h2>
<p>Invertible networks can also be trained as generative models via maximum likelihood. In maximum likelihood training, the network is optimized to maximize the probabilities of the training inputs. Invertible networks assign probabilities to training inputs by mapping them to a latent space and computing their probabilities under a predefined prior in that latent space. However, for real-valued inputs, one has to account for quantization and volume change to ensure this results in a proper probability distribution in the input space. Quantization  refers to the fact that training data often consists of quantized measureuements of underlying continuous data, e.g. digital images can only represent a distinct set of color values. Volume change refers to how the invertible networks’ mapping function expands or squeezes volume from input space to latent space.</p>
<div class="section" id="de-quantization">
<h3>(De)quantization<a class="headerlink" href="#de-quantization" title="Permalink to this heading">#</a></h3>
<div class="figure align-default" id="dequantization-fig">
<img alt="_images/dequantization.png" src="_images/dequantization.png" />
<p class="caption"><span class="caption-number">Fig. 13 </span><span class="caption-text"><strong>Density-maximizing distributions without and with dequantization.</strong> Examples show result of fitting  quantized values like discrete integer color values with a continuous probability distribution. Example training distributions have 3 data points at <span class="math notranslate nohighlight">\(x_1=1\)</span>, <span class="math notranslate nohighlight">\(x_2=2\)</span> and <span class="math notranslate nohighlight">\(x_3=5\)</span>. On the left, fitting quantized values directly leads to a pathological solution as the learned distribution <span class="math notranslate nohighlight">\(p\)</span> can assign arbitrarily high probability densities on the data points. On the right, adding uniform noise <span class="math notranslate nohighlight">\(U(0,1)\)</span> to the datapoints leads to a distribution that also recovers the correct discrete distribution, that means integrating over the probability densities in the volume of each point leads to <span class="math notranslate nohighlight">\(P(x_i)=\frac{1}{3}\)</span>.</span><a class="headerlink" href="#dequantization-fig" title="Permalink to this image">#</a></p>
</div>
<p>Often, training data for neural networks consists of quantized measurements like discrete integer color values from 0 to 255, which are mapped to real-world floating point numbers for training. Naively maximizing the probability densities of these quantized values with a continuous probability distribution would lead to pathological behavior as the quantized training data points do not cover any volume. Hence it would be possible for the learned distribution to assign infinitely high probability densities to individual data points, see <a class="reference internal" href="#dequantization-fig"><span class="std std-numref">Fig. 13</span></a> on the left side.</p>
<p>Todo (this ref): As an example, given a gaussian mixture distribution with two components, one component may cover all the training points with nonzero densities while the other one could assign infinitely high densities to one single point [mackay ref?].</p>
<p>Hence, one needs to “dequantize” the data such that each datapoint occupies volume in the input space. The simplest way here is to add uniform noise to each data point with a volume corresponding to the gap between two data points. For example, if the 256 color values are mapped to 256 floating values between 0 and 1, one may add uniform noise  <span class="math notranslate nohighlight">\(u\sim(0,\frac{1}{256})\)</span> to the inputs. If a new noise sample is drawn for each new forward pass of the network, then optimizing the resulting continuous distribution lower bounds optimizing the original discrete distribution [ref]. TODO: correct a bit maybe, like what is lower bounding what and ref also. maybe also formula
Since in our case, we are not primarily interested in the exact performance as generative model in number of bits, we simply add gaussian noise with a fixed small standard deviation <span class="math notranslate nohighlight">\(N(0,0.005I)\)</span> during training.</p>
</div>
<div class="section" id="volume-change">
<h3>Volume Change<a class="headerlink" href="#volume-change" title="Permalink to this heading">#</a></h3>
<div class="figure align-default" id="change-of-volume-fig">
<img alt="_images/change-of-volume.png" src="_images/change-of-volume.png" />
<p class="caption"><span class="caption-number">Fig. 14 </span><span class="caption-text"><strong>Computing probability densities accounting for volume changes by a function.</strong> Input <span class="math notranslate nohighlight">\(x\)</span> with probability distribution <span class="math notranslate nohighlight">\(p_\text{x}(x)\)</span> on the left is scaled by 0.5 to <span class="math notranslate nohighlight">\(z=f(x)=0.5x\)</span> with probability distribution <span class="math notranslate nohighlight">\(p_\text{z}(z)\)</span> on the right. Naively integrating <span class="math notranslate nohighlight">\(p_\text{z}(f(x))\)</span> over x would lead to a non-valid probability distribution with <span class="math notranslate nohighlight">\(\int_x p_\mathrm{z}(f(x)) \, dx=2\)</span>. To get the prober probability densities in input space from <span class="math notranslate nohighlight">\(p_\text{z}(z)\)</span>, one has to multiply with the volume changes, in this case the scaling factor of the mapping <span class="math notranslate nohighlight">\(f(x)\)</span> from x to z, giving <span class="math notranslate nohighlight">\(p_\text{x}(x)=p_\mathrm{z}(f(x)) \cdot \frac{df}{dx}=p_\mathrm{z}(f(x))\cdot 0.5\)</span> which correctly integrates to 1.</span><a class="headerlink" href="#change-of-volume-fig" title="Permalink to this image">#</a></p>
</div>
<p>TODO: probably change to p_x and p_z, make more consistent by moving also right side formula out maybe make font above arrow a bit bigger</p>
<p>In addition, for these probability densities to form a valid probability distribution in the input space, one has to account for how much the network’s mapping function squeezes and expands volume Otherwise, the network can increase densities by squeezing all the inputs closely together in latent space, see also <a class="reference internal" href="#change-of-volume-fig"><span class="std std-numref">Fig. 14</span></a> for a onedimensional example.
To correctly account for the volume change during the forward pass of <span class="math notranslate nohighlight">\(f\)</span> one needs to multiply the probability density with the volume change of <span class="math notranslate nohighlight">\(f\)</span>, descreasing the densities if the volume is squeezed from input to latent space and increasing it if the volume is expanded. As the volume change at a given point <span class="math notranslate nohighlight">\(x\)</span> is given by the absolute determinant of the jacobian of f at that point  <span class="math notranslate nohighlight">\(\det \left( \frac{\partial \mathbf{f}}{\partial \mathbf{x}} \right)\)</span>, the overall formula looks like this:</p>
<p><span class="math notranslate nohighlight">\(p(x) = p_\textrm{prior}(f(x)) \cdot  | \det \left( \frac{\partial \mathbf{f}}{\partial \mathbf{x}} \right)|\)</span></p>
<p>Usually, one optimizes the log-densities, leading to:</p>
<p><span class="math notranslate nohighlight">\(\log p(x) = \log p_\textrm{prior}(f(x)) \cdot  + \log |\det \left( \frac{\partial \mathbf{f}}{\partial \mathbf{x}} \right)|\)</span></p>
</div>
</div>
<div class="section" id="generative-classifiers">
<h2>Generative classifiers<a class="headerlink" href="#generative-classifiers" title="Permalink to this heading">#</a></h2>
<p>Invertible networks trained as class-conditional generative models can also be used as classifiers. Class-conditional generative networks may be implemented in different ways, for example with a separate prior in latent space for each class. Given the class-conditional probability densities <span class="math notranslate nohighlight">\(p(x|c_i)\)</span>, one can obtain class probabilities via Bayes formula as <span class="math notranslate nohighlight">\(p(c_i|x)=\frac{p(x|c_i)}{\sum_jp(x|c_j)}\)</span>.</p>
<p>Pure class-conditional generative training may not yield networks that perform well as classifiers. One proposed reason is the relatively small reduction in generative maximum likelihood loss obtainable from providing the class label to the network for high-dimensional inputs, for example much smaller than typical differences between two runs of the same network [REF]. How much one can reduce the loss through providing the class label can be derived from a compression perspective, so using that under Shannon’s theorem more probable inputs need less bits to encode than less probable inputs, or more precisely <span class="math notranslate nohighlight">\(\textrm{Number of bits needed}(x) = \log_2 p(x)\)</span>. How many of these bits are needed for the class label in case it is not given? To distinguish between n classes, one needs only <span class="math notranslate nohighlight">\(\log_2(n)\)</span> bits, so in case of binary pathology classification, only 1 bit is needed. However, the inputs themselves typically need at least 1 bit per dimension, so already, a 21 channel x 128 timepoints EEG-signal may need at least 2688 bits to encode. Therefore the optimal class-conditional model will only be 1 bit better than the optimal class-independent model and contribute very little to the overall encoding size. In contrast, the loss difference between two training runs of the same network will typically be at least 1 to two orders of magnitude larger. In practice, the gains from using a class-conditional model, by e.g., using a separate prior per class in latent space, are usually larger, but it is not a priori clear if the reductions in loss from exploiting the class label are high enough to result in a good classification model.</p>
<p>Various methods have been proposed to improve the performance of using generative classifiers. For example, people have fixed the per-class latent gaussian priors so that they retain the same distance throughout training [Ref Pavel] or added a classification loss term <span class="math notranslate nohighlight">\(L_\textrm{class}(x,c_i)=-\log p(c_i|x) = -\log \frac{p(x|ci)}{\sum_j p(x|cj)}=-\log \frac{e^{\log p(x|ci)}}{\sum_j e^{\log p(x|cj)}}\)</span> to the training loss [Ref VIB heidelberg]. In our work, we experimented with adding a classification loss term to the training, and also found using a learned temperature before the softmax helps the training, so leading to:</p>
<p><span class="math notranslate nohighlight">\(
\begin{align*}
   L_\textrm{class}(x,c_i,t)&amp;= -\log \frac{e^{\frac{\log p(x|ci)}{t}}}{\sum_j e^{\frac{\log p(x|cj)}{t}}} \\
\end{align*}
\)</span></p>
</div>
<div class="section" id="invertible-network-for-eeg-decoding">
<h2>Invertible Network for EEG Decoding<a class="headerlink" href="#invertible-network-for-eeg-decoding" title="Permalink to this heading">#</a></h2>
<div class="figure align-default" id="eeg-invnet-fig">
<img alt="_images/EEG-InvNet.png" src="_images/EEG-InvNet.png" />
<p class="caption"><span class="caption-number">Fig. 15 </span><span class="caption-text"><strong>EEG-InvNet architecture</strong>. Our EEG-InvNet architecture consists of three stages that operate at sequentially lower temporal resolutions. Input is two seconds of 21 electrodes at 64 Hz so 21x128 dimensions. These are downsampled using Haar Wavelets to 42x32 for the first, 84x16 for the second and 164x8 for the last stage. One stage consists of 4 blocks, each block has an activation normalization, an invertible linear and a coupling layer. The activation normalization and invertible linear layer act on the channel dimension, so perform the same operation across channels on timepoint in the feature map. The coupling layer uses two convolutions with an exponential linear unit activation inbetween.</span><a class="headerlink" href="#eeg-invnet-fig" title="Permalink to this image">#</a></p>
</div>
<p>We designed an invertible network for EEG Decoding using invertible components used in the literature, primarily from the Glow architecture [REF]. Our architecture consists of three stages that operate on sequentially lower temporal resolutions. Similar to Glow, the individual stages consists of several blocks of Activation Normalization, Invertible Linear Channel Transformations and Coupling Layers, see <a class="reference internal" href="#eeg-invnet-fig"><span class="std std-numref">Fig. 15</span></a>. Between each stage, we downsample by computing the mean and difference of two neighbouring timepoints and moving these into the channel dimension. Unlike Glow, we keep processing all dimensions throughout all stages, finding this architecture to reach competitive accuracy on pathology decoding. We use one gaussian distribution per class in the latent space. We experimented with affine and additive coupling layers, and report results for additive layers as the restricted expressiveness may make them easier to interpret.</p>
<p>Our training loss is simply a weighted sum of generative loss and classification loss:</p>
<p><span class="math notranslate nohighlight">\(
\begin{align*}
   L(x,c_i,t)&amp;= L_\textrm{class}(x,c_i,t) + L_\textrm{gen}(x,c_i) &amp;= -\log \frac{e^{\frac{\log p(x|ci)}{t}}}{\sum_j e^{\frac{\log p(x|cj)}{t}}} - \alpha \log p(x|ci)\\
\end{align*}
\)</span></p>
<p>where we choose the hyperparameter <span class="math notranslate nohighlight">\(\alpha\)</span> as the inverse of the number of dimensions <span class="math notranslate nohighlight">\(\alpha=\frac{1}{\textrm{Number of dimensions of x}}\)</span>.</p>
<p>TODO: add temperature sentence</p>
</div>
<div class="section" id="class-prototypes">
<h2>Class Prototypes<a class="headerlink" href="#class-prototypes" title="Permalink to this heading">#</a></h2>
<div class="figure align-default" id="eeg-prototypes-fig">
<img alt="_images/EEGInvNetClassPrototypes.png" src="_images/EEGInvNetClassPrototypes.png" />
<p class="caption"><span class="caption-number">Fig. 16 </span><span class="caption-text"><strong>EEG-InvNet class prototypes</strong>. Class prototypes are synthesized by inverting the means <span class="math notranslate nohighlight">\(z_\mathrm{healthy}\)</span> and <span class="math notranslate nohighlight">\(z_\mathrm{pathological}\)</span> of the per-class gaussian distributions.</span><a class="headerlink" href="#eeg-prototypes-fig" title="Permalink to this image">#</a></p>
</div>
<p>In our first visualization, we show the inputs resulting from inverting the means of the gaussian distributions for each class. These can be seen as prototypical examples of each class and may give hint about the the discriminative features that have been learned. As these are only single examples, they need to be interpreted cautiously. For example, individual features within the examples may have a variety of relationships with the actual prediction function. Consider if a prototype contains a large alpha-band oscillation at two electrodes, then these may be indepedendently predictive predictive of that class or only in combination or even only in some combination with other features. Nevertheless, the prototypes can already suggest potential discriminative features for further investigation.</p>
</div>
<div class="section" id="per-electrode-prototypes">
<h2>Per-Electrode Prototypes<a class="headerlink" href="#per-electrode-prototypes" title="Permalink to this heading">#</a></h2>
<div class="sphinx-subfigure figure align-default" id="invnet-marginal-chan-explanation-fig">
<div class="sphinx-subfigure-grid ss-layout-default-AB_CD outline" style="display: grid; gap: 10px; grid-gap: 10px;">
<div class="sphinx-subfigure-area" style="display: flex; flex-direction: column; justify-content: center; align-items: center; grid-area: A;">
<img alt="_images/marginal-chan-explanation_0.png" src="_images/marginal-chan-explanation_0.png" />
</div>
<div class="sphinx-subfigure-area" style="display: flex; flex-direction: column; justify-content: center; align-items: center; grid-area: B;">
<img alt="_images/marginal-chan-explanation_1.png" src="_images/marginal-chan-explanation_1.png" />
</div>
<div class="sphinx-subfigure-area" style="display: flex; flex-direction: column; justify-content: center; align-items: center; grid-area: C;">
<img alt="_images/marginal-chan-explanation_2.png" src="_images/marginal-chan-explanation_2.png" />
</div>
<div class="sphinx-subfigure-area" style="display: flex; flex-direction: column; justify-content: center; align-items: center; grid-area: D;">
<img alt="_images/marginal-chan-explanation_3.png" src="_images/marginal-chan-explanation_3.png" />
</div>
</div>
<p class="caption"><span class="caption-number">Fig. 17 </span><span class="caption-text"><strong>EEG-InvNet per-electrode class prototypes</strong>. For getting per-electrode prototypes, class-specific signals for one electrode are synthesized while signals at other electrodes are sampled from training data. In the example, prototypes for T3 for the healthy and pathological class are learned, four samples for remaining electrodes are shown. In practice, a much larger number of samples would be used. Class signal probabilities are marginalized over the non-optimized channels as explained in text.</span><a class="headerlink" href="#invnet-marginal-chan-explanation-fig" title="Permalink to this image">#</a></p>
</div>
<p>One way to get more interpretable prototypes is to synthesize them per electrode. Here, we synthesize a signal for a specific electrode <span class="math notranslate nohighlight">\(e_i\)</span> such that the class prediction is high for one class, independent of the signals at the other electrodes. So for electrode <span class="math notranslate nohighlight">\(e_k\)</span> and class <span class="math notranslate nohighlight">\(c_i\)</span>, we aim to optimize the signal <span class="math notranslate nohighlight">\(x^*_{e_i}\)</span> by maximizing the marginals and <span class="math notranslate nohighlight">\(p(x^*_{e_k}|c_i)=\int p(x|c_i;x_{e_k}=x^*_{e_k}) dx\)</span> and <span class="math notranslate nohighlight">\(p(c_i|x^*_{e_k})=\frac{p(x^*_{e_k}|c_i)}{\sum_j p(x^*_{e_k}|c_j)}\)</span>. To approximate this, we sample signals of the training distribution and replace the signal <span class="math notranslate nohighlight">\(x_{e_k}\)</span> of the electrode <span class="math notranslate nohighlight">\(e_k\)</span> we are synthesizing  by the optimized <span class="math notranslate nohighlight">\(x^*_{e_k}\)</span>. As we sample the remaining electrodes signals from the full distribution and not from the actual conditional distribution <span class="math notranslate nohighlight">\(p(x|c_i;x_{e_k}=x^*_{e_k})\)</span>, this is only an approximation, but already yields insightful visualizations. In practice, when computing the approximation of the integral <span class="math notranslate nohighlight">\(\int p(x|c_i;x_{e_k}=x^*_{e_k})\)</span> from our log probabilities, we found it helpful to divide the log probabilities by the learned temperature of the classifier.</p>
</div>
<div class="section" id="eeg-cosnet">
<h2>EEG CosNet<a class="headerlink" href="#eeg-cosnet" title="Permalink to this heading">#</a></h2>
<div class="sphinx-subfigure figure align-default" id="cos-net-example-fig">
<div class="sphinx-subfigure-grid ss-layout-default-AB outline" style="display: grid; gap: 0px; grid-gap: 0px;">
<div class="sphinx-subfigure-area" style="display: flex; flex-direction: column; justify-content: center; align-items: center; grid-area: A;">
<img alt="_images/cos-net-example-input.png" src="_images/cos-net-example-input.png" />
</div>
<div class="sphinx-subfigure-area" style="display: flex; flex-direction: column; justify-content: center; align-items: center; grid-area: B;">
<img alt="_images/cos-net-example-processing.png" src="_images/cos-net-example-processing.png" />
</div>
</div>
<p class="caption"><span class="caption-number">Fig. 18 </span><span class="caption-text">Example Processing of the EEG-CosNet. Example EEG input on the left, then processing steps on the right: spatial filtering, absolute cosine similarity with temporal filters, temporal averaging, then weighting with linear classifier weights for class prediction. Note the EEG-CosNet in this visualization only uses 8 filters, whereas later we will use 64 filters.</span><a class="headerlink" href="#cos-net-example-fig" title="Permalink to this image">#</a></p>
</div>
<p>Finally, we also implemented a small convolutional network EEG-CosNet that we designed to be directly interpretable. We tried to distill the trained EEG-InvNet into the EEG-CosNet by training the EEG-CosNet using the EEG-InvNet class probabilities as the targets for the classification loss <span class="math notranslate nohighlight">\(L_\textrm{class}\)</span>. Our EEG-CosNet consists of just three steps:</p>
<p><strong>Spatial Filtering</strong></p>
<p><span class="math notranslate nohighlight">\(
\begin{align*}
    h_1 &amp;= W_s^Tx &amp;&amp; \color{commentcolor}{\text{Apply learnable spatial filter weights } W_s \text{ to  inputs }} \\
\end{align*}
\)</span></p>
<p><strong>Absolute Temporal Cosine Similarity with Temporal Filters</strong></p>
<p><span class="math notranslate nohighlight">\(
\begin{align*}
    h_2 &amp;= |\mathrm{moving\_cos\_sim}(h_1, \mathrm{temporal\_filters})| &amp;&amp; \color{commentcolor}{\text{Moving absolute cosine similarity with temporal filters, one temporal filter per spatial filter }} \\
    h_3 &amp;=\frac{\sum_t (h_2)}{n_\mathrm{times}} &amp;&amp; \color{commentcolor}{\text{Average over timepoints in trial}} \\
\end{align*}
\)</span></p>
<p><strong>Classification</strong></p>
<p><span class="math notranslate nohighlight">\(
\begin{align*}
    h_4 &amp;= W_c^Th_3 &amp;&amp; \color{commentcolor}{\text{Apply learnable classification weights } W_c \text{ on these spatiotemporal features }} \\
    p(c_i|h_4) &amp;= \frac{e^{h_{4,i}}}{\sum_j e^h_{4,j}} &amp;&amp; \color{commentcolor}{\text{Take the softmax to produce per-class predictions }} \\
\end{align*}
\)</span></p>
<p>Steps 1 and 2 yield spatiotemporal patterns that can be visualized as waveforms and scalp plots, and that are weighted by the linear classifier for the respective classes. We chose cosine similarity to ensure that high output values correspond to spatially filtered signals that resemble the corresponding temporal filter. The spatial filter weights and linear classifier weights can be made even more interpretable through transforming the discriminative weights into generative patterns  by multiplying them with the covariance of the electrodes/averaged absolute cosine similarities after training, see <span id="id2">Haufe <em>et al.</em> [<a class="reference internal" href="References.html#id65" title="Stefan Haufe, Frank Meinecke, Kai Görgen, Sven Dähne, John-Dylan Haynes, Benjamin Blankertz, and Felix Bießmann. On the interpretation of weight vectors of linear models in multivariate neuroimaging. NeuroImage, 87:96–110, February 2014. URL: http://www.sciencedirect.com/science/article/pii/S1053811913010914 (visited on 2015-08-07), doi:10.1016/j.neuroimage.2013.10.067.">2014</a>]</span> for a discussion on this technique. We use 64 spatiotemporal filters with temporal length 64 corresponding to one second at 64 Hz.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="PerturbationVisualization.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Perturbation Visualization</p>
      </div>
    </a>
    <a class="right-next"
       href="MovementDecoding.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Decoding Movement-Related Brain Activity</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#invertible-layers">Invertible Layers</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#generative-models-via-maximum-likelihood">Generative models via maximum likelihood</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#de-quantization">(De)quantization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#volume-change">Volume Change</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#generative-classifiers">Generative classifiers</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#invertible-network-for-eeg-decoding">Invertible Network for EEG Decoding</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#class-prototypes">Class Prototypes</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#per-electrode-prototypes">Per-Electrode Prototypes</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#eeg-cosnet">EEG CosNet</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By The Jupyter Book community
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>