

<!DOCTYPE html>


<html lang="en" data-theme="light">

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Generalization to Other Tasks &#8212; Deep Learning for Brain-Signal Decoding from Electroencephalography (EEG)</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "light";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="_static/.ipynb_checkpoints/custom-checkpoint.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script async="async" kind="hypothesis" src="https://hypothes.is/embed.js"></script>
    <script kind="utterances">

    var commentsRunWhenDOMLoaded = cb => {
    if (document.readyState != 'loading') {
        cb()
    } else if (document.addEventListener) {
        document.addEventListener('DOMContentLoaded', cb)
    } else {
        document.attachEvent('onreadystatechange', function() {
        if (document.readyState == 'complete') cb()
        })
    }
}

var addUtterances = () => {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src = "https://utteranc.es/client.js";
    script.async = "async";

    script.setAttribute("repo", "robintibor/eeg-deep-learning-phd-thesis");
    script.setAttribute("issue-term", "pathname");
    script.setAttribute("theme", "github-light");
    script.setAttribute("label", "ðŸ’¬ comment");
    script.setAttribute("crossorigin", "anonymous");

    sections = document.querySelectorAll("div.section");
    if (sections !== null) {
        section = sections[sections.length-1];
        section.appendChild(script);
    }
}
commentsRunWhenDOMLoaded(addUtterances);
</script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'TaskDecoding';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Decoding Pathology" href="Pathology.html" />
    <link rel="prev" title="Decoding Movement-Related Brain Activity" href="MovementDecoding.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="light">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="Abstract.html">
  
  
  
  
    
    
    
    <img src="_static/braindecode-logo.svg" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="_static/braindecode-logo.svg" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="Introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="PriorWork.html">Prior Work</a></li>
<li class="toctree-l1"><a class="reference internal" href="FBCSPAndFBCSPNet.html">Filter Bank Common Spatial Patterns and Filterbank Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="DeepArchitectures.html">Neural Network Architectures for EEG-Decoding</a></li>
<li class="toctree-l1"><a class="reference internal" href="CroppedTraining.html">Cropped Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="PerturbationVisualization.html">Perturbation Visualization</a></li>
<li class="toctree-l1"><a class="reference internal" href="Invertible.html">Invertible Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="MovementDecoding.html">Decoding Movement-Related Brain Activity</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Generalization to Other Tasks</a></li>
<li class="toctree-l1"><a class="reference internal" href="Pathology.html">Decoding Pathology</a></li>
<li class="toctree-l1"><a class="reference internal" href="UnderstandingPathology.html">Understanding Pathology Decoding With Invertible Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="FutureWork.html">Future Work</a></li>
<li class="toctree-l1"><a class="reference internal" href="References.html">References</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/TaskDecoding.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Generalization to Other Tasks</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#decoding-different-mental-imageries">Decoding different mental imageries</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#decoding-error-related-signals">Decoding error-related signals</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#decoding-observation-of-robots-making-errors">Decoding Observation of Robots Making Errors</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#decoding-of-eriksen-flanker-task-errors-and-errors-during-online-gui-control">Decoding of Eriksen Flanker Task Errors and Errors during Online GUI Control</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#proof-of-concept-assistive-system">Proof-of-concept assistive system</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#intracranial-eeg-decoding">Intracranial EEG decoding</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#intracranial-eeg-decoding-of-eriksen-flanker-task">Intracranial EEG Decoding of Eriksen Flanker Task</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#transfer-learning-for-intracranial-error-decoding">Transfer Learning for Intracranial Error Decoding</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#microelectrocorticography-decoding-of-auditory-evoked-responses-in-sheep">Microelectrocorticography decoding of auditory evoked responses in sheep</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation-on-large-scale-task-diverse-dataset">Evaluation on large-scale task-diverse dataset</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <div class="tex2jax_ignore mathjax_ignore section" id="generalization-to-other-tasks">
<span id="task-related"></span><h1>Generalization to Other Tasks<a class="headerlink" href="#generalization-to-other-tasks" title="Permalink to this heading">#</a></h1>
<div class="admonition-our-architectures-generalize-well-to-a-wide-variety-of-decoding-tasks admonition">
<p class="admonition-title">Our architectures generalize well to a wide variety of decoding tasks</p>
<ul class="simple">
<li><p>Perform similar or better than common feature-based algorithms on mental imageries, error decoding, auditory evoked potentials</p></li>
<li><p>Also perform well on intracranial EEG</p></li>
<li><p>Deep networks performs a bit better than shallow network on average across tasks</p></li>
<li><p>EEGNet architecture developed by others also performs well</p></li>
<li><p>Networks can be used in an online BCI scenario</p></li>
</ul>
</div>
<p>After our initial work designing and evaluating convolutional neural networks for movement decoding from EEG, we evaluated the resulting networks on a wide variety of other EEG decoding tasks found that they generalize well to a large number of settings such as error-related decoding, online BCI control or auditory evoked potentials and also work on intracranial EEG. Text and content condensed from a number of publications, namely <span id="id1">Schirrmeister <em>et al.</em> [<a class="reference internal" href="References.html#id37" title="Robin Tibor Schirrmeister, Jost Tobias Springenberg, Lukas Dominique Josef Fiederer, Martin Glasstetter, Katharina Eggensperger, Michael Tangermann, Frank Hutter, Wolfram Burgard, and Tonio Ball. Deep learning with convolutional neural networks for eeg decoding and visualization. Human Brain Mapping, aug 2017. URL: http://dx.doi.org/10.1002/hbm.23730, doi:10.1002/hbm.23730.">2017</a>]</span>, <span id="id2">VÃ¶lker <em>et al.</em> [<a class="reference internal" href="References.html#id43" title="Martin VÃ¶lker, Robin T Schirrmeister, Lukas DJ Fiederer, Wolfram Burgard, and Tonio Ball. Deep transfer learning for error decoding from non-invasive eeg. In 2018 6th International Conference on Brain-Computer Interface (BCI), 1â€“6. IEEE, 2018.">2018</a>]</span>, <span id="id3">Burget <em>et al.</em> [<a class="reference internal" href="References.html#id44" title="Felix Burget, Lukas Dominique Josef Fiederer, Daniel Kuhner, Martin VÃ¶lker, Johannes Aldinger, Robin Tibor Schirrmeister, Chau Do, Joschka Boedecker, Bernhard Nebel, Tonio Ball, and others. Acting thoughts: towards a mobile robotic service assistant for users with limited communication skills. In 2017 European Conference on Mobile Robots (ECMR), 1â€“6. IEEE, 2017.">2017</a>]</span>, <span id="id4">Volker <em>et al.</em> [<a class="reference internal" href="References.html#id41" title="Martin Volker, Jiri Hammer, Robin T Schirrmeister, Joos Behncke, Lukas DJ Fiederer, Andreas Schulze-Bonhage, Petr Marusic, Wolfram Burgard, and Tonio Ball. Intracranial error detection via deep learning. In 2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC), 568â€“575. IEEE, 2018.">2018</a>]</span>, <span id="id5">Behncke <em>et al.</em> [<a class="reference internal" href="References.html#id40" title="Joos Behncke, Robin Tibor Schirrmeister, Martin Volker, Jiri Hammer, Petr Marusic, Andreas Schulze-Bonhage, Wolfram Burgard, and Tonio Ball. Cross-paradigm pretraining of convolutional networks improves intracranial eeg decoding. In 2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC), 1046â€“1053. IEEE, 2018.">2018</a>]</span>, <span id="id6">Wang <em>et al.</em> [<a class="reference internal" href="References.html#id39" title="X. Wang, C. A. Gkogkidis, R. T. Schirrmeister, F. A. Heilmeyer, M. Gierthmuehlen, F. Kohler, M. Schuettler, T. Stieglitz, and T. Ball. Deep learning for micro-electrocorticographic (Âµecog) data. In 2018 IEEE-EMBS Conference on Biomedical Engineering and Sciences (IECBES), volume, 63-68. 2018. doi:10.1109/IECBES.2018.8626607.">2018</a>]</span> and <span id="id7">Heilmeyer <em>et al.</em> [<a class="reference internal" href="References.html#id38" title="Felix A Heilmeyer, Robin T Schirrmeister, Lukas DJ Fiederer, Martin Volker, Joos Behncke, and Tonio Ball. A large-scale evaluation framework for eeg deep learning architectures. In 2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC), 1039â€“1045. IEEE, 2018.">2018</a>]</span>. In all of these works except <span id="id8">Schirrmeister <em>et al.</em> [<a class="reference internal" href="References.html#id37" title="Robin Tibor Schirrmeister, Jost Tobias Springenberg, Lukas Dominique Josef Fiederer, Martin Glasstetter, Katharina Eggensperger, Michael Tangermann, Frank Hutter, Wolfram Burgard, and Tonio Ball. Deep learning with convolutional neural networks for eeg decoding and visualization. Human Brain Mapping, aug 2017. URL: http://dx.doi.org/10.1002/hbm.23730, doi:10.1002/hbm.23730.">2017</a>]</span>, I was not the main contributor, I assisted in adapting the code and training for the various settings and helped in the writing process.</p>
<div class="section" id="decoding-different-mental-imageries">
<h2>Decoding different mental imageries<a class="headerlink" href="#decoding-different-mental-imageries" title="Permalink to this heading">#</a></h2>
<table class="colwidths-auto table" id="mixed-imagery-dataset-results">
<caption><span class="caption-number">Table 11 </span><span class="caption-text">Accuracies on the Mixed-Imagery dataset. ConvNet accuracies show the difference to the FBCSP accuracy.</span><a class="headerlink" href="#mixed-imagery-dataset-results" title="Permalink to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>FBCSP</p></th>
<th class="head"><p>Deep ConvNet</p></th>
<th class="head"><p>Shallow ConvNet</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>71.2</p></td>
<td><p>+1.0</p></td>
<td><p>-3.5</p></td>
</tr>
<tr class="row-odd"><td><p><span id="id9">[<a class="reference internal" href="References.html#id37" title="Robin Tibor Schirrmeister, Jost Tobias Springenberg, Lukas Dominique Josef Fiederer, Martin Glasstetter, Katharina Eggensperger, Michael Tangermann, Frank Hutter, Wolfram Burgard, and Tonio Ball. Deep learning with convolutional neural networks for eeg decoding and visualization. Human Brain Mapping, aug 2017. URL: http://dx.doi.org/10.1002/hbm.23730, doi:10.1002/hbm.23730.">Schirrmeister <em>et al.</em>, 2017</a>]</span></p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<p>The Mixed Imagery Dataset (MID) was obtained from 4 healthy subjects (3 female, all right-handed, age
26.75Â±5.9 (meanÂ±std)) with a varying number of trials (S1: 675, S2: 2172, S3: 698, S4: 464) of imagined
movements (right hand and feet), mental rotation and mental word generation. All details were the same as
for the High Gamma Dataset, except: a 64-electrode subset of electrodes was used for recording, recordings
were not performed in the electromagnetically shielded cabin, thus possibly better approximating conditions
of real-world BCI usage, and trials varied in duration between 1 to 7 seconds. The dataset was analyzed
by cutting out time windows of 2 seconds with 1.5 second overlap from all trials longer than 2 seconds (S1:
6074 windows, S2: 21339, S3: 6197, S4: 4220), and both methods were evaluated using the accuracy of the
predictions for all the 2-second windows for the last two runs of roughly 130 trials (S1: 129, S2: 160, S3:
124, S4: 123).</p>
<p>For the mixed imagery dataset, we find the deep ConvNet to perform slightly better and the shallow ConvNet to perform slightly worse than the FBCSP algorithm, as can be seen in <a class="reference internal" href="#mixed-imagery-dataset-results"><span class="std std-numref">Table 11</span></a>.</p>
</div>
<div class="section" id="decoding-error-related-signals">
<h2>Decoding error-related signals<a class="headerlink" href="#decoding-error-related-signals" title="Permalink to this heading">#</a></h2>
<div class="section" id="decoding-observation-of-robots-making-errors">
<h3>Decoding Observation of Robots Making Errors<a class="headerlink" href="#decoding-observation-of-robots-making-errors" title="Permalink to this heading">#</a></h3>
<table class="colwidths-auto table" id="robot-ball-results">
<caption><span class="caption-number">Table 12 </span><span class="caption-text">Accuracies for decoding watching of successful or unsuccessful robot-liquid pouring or ball-lifting.</span><a class="headerlink" href="#robot-ball-results" title="Permalink to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>robot task</p></th>
<th class="head"><p>time interval</p></th>
<th class="head"><p>Deep ConvNet</p></th>
<th class="head"><p>rLDA</p></th>
<th class="head"><p>FBCSP</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Pouring Liquid</p></td>
<td><p>2-5s</p></td>
<td><p>78.2 Â± 8.4</p></td>
<td><p>67.5 Â± 8.5</p></td>
<td><p>60.1 Â± 3.7</p></td>
</tr>
<tr class="row-odd"><td><p>Pouring Liquid</p></td>
<td><p>3.3-7.5s</p></td>
<td><p>71.9 Â± 7.6</p></td>
<td><p>63.0 Â± 9.3</p></td>
<td><p>66.5 Â± 5.7</p></td>
</tr>
<tr class="row-even"><td><p>Lifting Ball</p></td>
<td><p>4.8-6.3s</p></td>
<td><p>59.6 Â± 6.4</p></td>
<td><p>58.1 Â± 6.6</p></td>
<td><p>52.4 Â± 2.8</p></td>
</tr>
<tr class="row-odd"><td><p>Lifting Ball</p></td>
<td><p>4-7s</p></td>
<td><p>64.6 Â± 6.1</p></td>
<td><p>58.5 Â± 8.2</p></td>
<td><p>53.1 Â± 2.5</p></td>
</tr>
<tr class="row-even"><td><p><span id="id10">[<a class="reference internal" href="References.html#id42" title="Joos Behncke, Robin T Schirrmeister, Wolfram Burgard, and Tonio Ball. The signature of robot action success in eeg signals of a human observer: decoding and visualization using deep convolutional neural networks. In 2018 6th international conference on brain-computer interface (BCI), 1â€“6. IEEE, 2018.">Behncke <em>et al.</em>, 2018</a>]</span></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<p>In this study, we aimed to classify whether a person had watched a video of a successful or an unsuccessful attempt of a robot performing one of two tasks (lifting a ball or pouring liquid) based on EEG recorded during the video observation. We compared the performance of our deep ConvNet to that of regularized linear discriminant analysis (rLDA) and FBCSP on this task. Our results, presented in <a class="reference internal" href="#robot-ball-results"><span class="std std-numref">Table 12</span></a>, demonstrate that the deep ConvNet outperformed the other methods for both tasks and both decoding intervals.</p>
</div>
<div class="section" id="decoding-of-eriksen-flanker-task-errors-and-errors-during-online-gui-control">
<span id="flanker-and-gui-section"></span><h3>Decoding of Eriksen Flanker Task Errors and Errors during Online GUI Control<a class="headerlink" href="#decoding-of-eriksen-flanker-task-errors-and-errors-during-online-gui-control" title="Permalink to this heading">#</a></h3>
<div class="sd-container-fluid sd-sphinx-override sd-mb-4 docutils">
<div class="sd-row sd-g-2 sd-g-xs-2 sd-g-sm-2 sd-g-md-2 sd-g-lg-2 docutils">
<div class="sd-col sd-d-flex-column docutils">
<div class="figure align-default" id="within-subject-flanker-gui-fig">
<a class="reference internal image-reference" href="_images/within-subject-flanker-gui.png"><img alt="_images/within-subject-flanker-gui.png" src="_images/within-subject-flanker-gui.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 31 </span><span class="caption-text">Comparison of within-subject decoding by rLDA and deep ConvNets. Error bars show the SEM. A) Eriksen flanker task (mean of 31 subjects), last 20% of subject data as test set. Deep ConvNets were 7.12% better than rLDA, pval = 6.24 *10-20 (paired t-test). B) Online GUI control (mean of 4 subjects), last session of each subject as test data. Figure from <span id="id11">[<a class="reference internal" href="References.html#id43" title="Martin VÃ¶lker, Robin T Schirrmeister, Lukas DJ Fiederer, Wolfram Burgard, and Tonio Ball. Deep transfer learning for error decoding from non-invasive eeg. In 2018 6th International Conference on Brain-Computer Interface (BCI), 1â€“6. IEEE, 2018.">VÃ¶lker <em>et al.</em>, 2018</a>]</span>:</span><a class="headerlink" href="#within-subject-flanker-gui-fig" title="Permalink to this image">#</a></p>
</div>
</div>
<div class="sd-col sd-d-flex-column docutils">
<div class="figure align-default" id="cross-subject-flanker-gui-fig">
<a class="reference internal image-reference" href="_images/cross-subject-flanker-gui.png"><img alt="_images/cross-subject-flanker-gui.png" src="_images/cross-subject-flanker-gui.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 32 </span><span class="caption-text">Mean normalized decoding accuracy on unknown subjects. Error bars show the SEM. A) Eriksen flanker task, trained on 30 subjects, tested on 1 subject.  Deep ConvNets were 5.05% better than rLDA, p = 3.16 *10-4 (paired t-test). B) Online GUI control. Trained on 3 subjects, tested on the  respective remaining subject. Figure from <span id="id12">VÃ¶lker <em>et al.</em> [<a class="reference internal" href="References.html#id43" title="Martin VÃ¶lker, Robin T Schirrmeister, Lukas DJ Fiederer, Wolfram Burgard, and Tonio Ball. Deep transfer learning for error decoding from non-invasive eeg. In 2018 6th International Conference on Brain-Computer Interface (BCI), 1â€“6. IEEE, 2018.">2018</a>]</span>.</span><a class="headerlink" href="#cross-subject-flanker-gui-fig" title="Permalink to this image">#</a></p>
</div>
</div>
</div>
</div>
<p>In two additional error-related decoding experiments, we evaluated an Eriksen flanker task and errors during an the online control of a graphical user interface through a brain-computer-interface. In the Eriksen flanker task, the subjects were asked to press the left or right button on a gamepad depending on whether an â€˜Lâ€™ or an â€˜Râ€™ was the middle character of a 5-letter string displayed on the screen. For the online graphical user interface (GUI) control, the subjects were given an aim to reach using the GUI, also see <a class="reference internal" href="#online-bci"><span class="std std-ref">Proof-of-concept assistive system</span></a>. They had to think of one of the classes of the aforementioned Mixed Imagery Dataset to choose one of four possible GUI actions. The correct GUI action was always determined by the specificed aim given to the subject, hence an erroneous action could be detected. The decoding task in this paper was to distinguish whether the BCI-selected action was correct or erroneous. Results in <a class="reference internal" href="#within-subject-flanker-gui-fig"><span class="std std-numref">Fig. 31</span></a> and <a class="reference internal" href="#cross-subject-flanker-gui-fig"><span class="std std-numref">Fig. 32</span></a> show that deep ConvNets outperform rLDA in all settings except cross-subject error-decoding for online GUI control, where the low number of subjects (4) may prevent the ConvNets to learn enough to outperform rLDA.</p>
</div>
</div>
<div class="section" id="proof-of-concept-assistive-system">
<span id="online-bci"></span><h2>Proof-of-concept assistive system<a class="headerlink" href="#proof-of-concept-assistive-system" title="Permalink to this heading">#</a></h2>
<div class="figure align-default" id="robot-bci-overview-fig">
<img alt="_images/robot-bci-overview.png" src="_images/robot-bci-overview.png" />
<p class="caption"><span class="caption-number">Fig. 33 </span><span class="caption-text">Overview of the proof-of-concept assistive system from <span id="id13">[<a class="reference internal" href="References.html#id44" title="Felix Burget, Lukas Dominique Josef Fiederer, Daniel Kuhner, Martin VÃ¶lker, Johannes Aldinger, Robin Tibor Schirrmeister, Chau Do, Joschka Boedecker, Bernhard Nebel, Tonio Ball, and others. Acting thoughts: towards a mobile robotic service assistant for users with limited communication skills. In 2017 European Conference on Mobile Robots (ECMR), 1â€“6. IEEE, 2017.">Burget <em>et al.</em>, 2017</a>]</span> using the deep ConvNet in the BCI component. Robotic arm could be given high-level commands via the BCI, high-level commands were extracted from a knowledge base. The commands were then autonomously planned and executed by the robotic arm. Figure from <span id="id14">Burget <em>et al.</em> [<a class="reference internal" href="References.html#id44" title="Felix Burget, Lukas Dominique Josef Fiederer, Daniel Kuhner, Martin VÃ¶lker, Johannes Aldinger, Robin Tibor Schirrmeister, Chau Do, Joschka Boedecker, Bernhard Nebel, Tonio Ball, and others. Acting thoughts: towards a mobile robotic service assistant for users with limited communication skills. In 2017 European Conference on Mobile Robots (ECMR), 1â€“6. IEEE, 2017.">2017</a>]</span></span><a class="headerlink" href="#robot-bci-overview-fig" title="Permalink to this image">#</a></p>
</div>
<table class="colwidths-auto table" id="bci-robot-results">
<caption><span class="caption-number">Table 13 </span><span class="caption-text">Results for BCI control of the GUI. Accuracy is fraction of correct commands, time is time per command, steps is steps needed to reach the aim, path optimality is ratio of miniminally needed  nubmer of steps to actually used number of steps when every step is optimal, and time/step is time per step.</span><a class="headerlink" href="#bci-robot-results" title="Permalink to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Subject</p></th>
<th class="head"><p>Runs</p></th>
<th class="head"><p>Accuracy* [%]</p></th>
<th class="head"><p>Time [s]</p></th>
<th class="head"><p>Steps</p></th>
<th class="head"><p>Path Optimality [%]</p></th>
<th class="head"><p>Time/Step [s]</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>S1</p></td>
<td><p>18</p></td>
<td><p>84.1 Â± 6.1</p></td>
<td><p>125 Â± 84</p></td>
<td><p>13.0 Â± 7.8</p></td>
<td><p>70.1 Â± 22.3</p></td>
<td><p>9 Â± 2</p></td>
</tr>
<tr class="row-odd"><td><p>S2</p></td>
<td><p>14</p></td>
<td><p>76.8 Â± 14.1</p></td>
<td><p>150 Â± 32</p></td>
<td><p>10.1 Â± 2.8</p></td>
<td><p>91.3 Â± 12.0</p></td>
<td><p>9 Â± 3</p></td>
</tr>
<tr class="row-even"><td><p>S3</p></td>
<td><p>17</p></td>
<td><p>82.0 Â± 7.4</p></td>
<td><p>200 Â± 159</p></td>
<td><p>17.6 Â± 11.4</p></td>
<td><p>65.7 Â± 28.9</p></td>
<td><p>11 Â± 4</p></td>
</tr>
<tr class="row-odd"><td><p>S4</p></td>
<td><p>3</p></td>
<td><p>63.8 Â± 15.6</p></td>
<td><p>176 Â± 102</p></td>
<td><p>26.3 Â± 11.2</p></td>
<td><p>34.5 Â± 1.2</p></td>
<td><p>6 Â± 2</p></td>
</tr>
<tr class="row-even"><td><p>Average</p></td>
<td><p>13</p></td>
<td><p>76.7 Â± 9.1</p></td>
<td><p>148 Â± 50</p></td>
<td><p>16.7 Â± 7.1</p></td>
<td><p>65.4 Â± 23.4</p></td>
<td><p>9 Â± 2</p></td>
</tr>
<tr class="row-odd"><td><p><span id="id15">[<a class="reference internal" href="References.html#id44" title="Felix Burget, Lukas Dominique Josef Fiederer, Daniel Kuhner, Martin VÃ¶lker, Johannes Aldinger, Robin Tibor Schirrmeister, Chau Do, Joschka Boedecker, Bernhard Nebel, Tonio Ball, and others. Acting thoughts: towards a mobile robotic service assistant for users with limited communication skills. In 2017 European Conference on Mobile Robots (ECMR), 1â€“6. IEEE, 2017.">Burget <em>et al.</em>, 2017</a>]</span></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<p>We also evaluated the use of our deep ConvNet as part of an assistive robot system where the brain-computer interface was sending high-level commands to a robotic arm. In this proof-of-concept system, the robotic arm could be instructed by the user via the BCI to fetch a cup and directly move the cup to the persons mouth to drink from it. An overview can be seen in <a class="reference internal" href="#robot-bci-overview-fig"><span class="std std-numref">Fig. 33</span></a>. Results from <a class="reference internal" href="#bci-robot-results"><span class="std std-numref">Table 13</span></a> show that 3 out of 4 subjects had a command accuracy of more than 75% and were able to reach the target using less than twice the steps of the minimal path through the GUI (path optimality &gt; 50%).</p>
</div>
<div class="section" id="intracranial-eeg-decoding">
<h2>Intracranial EEG decoding<a class="headerlink" href="#intracranial-eeg-decoding" title="Permalink to this heading">#</a></h2>
<div class="section" id="intracranial-eeg-decoding-of-eriksen-flanker-task">
<h3>Intracranial EEG Decoding of Eriksen Flanker Task<a class="headerlink" href="#intracranial-eeg-decoding-of-eriksen-flanker-task" title="Permalink to this heading">#</a></h3>
<table class="colwidths-auto table" id="intracranial-error-results-table">
<caption><span class="caption-number">Table 14 </span><span class="caption-text">Results for single-channel intracranial decoding of errors during an Eriksen flanker task. Balanced Accuracy is the mean of the accuracies for correct class ground truth labels and error class ground truth labels.</span><a class="headerlink" href="#intracranial-error-results-table" title="Permalink to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Classifier</p></th>
<th class="head"><p>Balanced Accuracy  [%]</p></th>
<th class="head"><p>Accuracy Correct Class [%]</p></th>
<th class="head"><p>Accuracy Error Class  [%]</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Deep4Net</p></td>
<td><p>59.28 Â± 0.50</p></td>
<td><p>69.37 Â± 0.44</p></td>
<td><p>49.19 Â± 0.56</p></td>
</tr>
<tr class="row-odd"><td><p>ShallowNet</p></td>
<td><p>58.42 Â± 0.32</p></td>
<td><p>74.83 Â± 0.25</p></td>
<td><p>42.01 Â± 0.40</p></td>
</tr>
<tr class="row-even"><td><p>EEGNet</p></td>
<td><p>57.73 Â± 0.52</p></td>
<td><p>57.78 Â± 0.48</p></td>
<td><p>57.68 Â± 0.56</p></td>
</tr>
<tr class="row-odd"><td><p>rLDA</p></td>
<td><p>53.76 Â± 0.32</p></td>
<td><p>76.12 Â± 0.26</p></td>
<td><p>31.40 Â± 0.38</p></td>
</tr>
<tr class="row-even"><td><p>ResNet</p></td>
<td><p>52.45 Â± 0.21</p></td>
<td><p>95.47 Â± 0.14</p></td>
<td><p>09.43 Â± 0.28</p></td>
</tr>
<tr class="row-odd"><td><p><span id="id16">[<a class="reference internal" href="References.html#id41" title="Martin Volker, Jiri Hammer, Robin T Schirrmeister, Joos Behncke, Lukas DJ Fiederer, Andreas Schulze-Bonhage, Petr Marusic, Wolfram Burgard, and Tonio Ball. Intracranial error detection via deep learning. In 2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC), 568â€“575. IEEE, 2018.">Volker <em>et al.</em>, 2018</a>]</span></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<div class="figure align-default" id="intracranial-error-results-fig">
<img alt="_images/IntracranialError.png" src="_images/IntracranialError.png" />
<p class="caption"><span class="caption-number">Fig. 34 </span><span class="caption-text">Results for all-channel intracranial decoding of errors during an Eriksen flanker task. Here, the classifiers were trained on all available channels per patient. A) Confusion matrices of the four models used for decoding. The matrices display the sum of all trials over the 24 recordings. On top of the matrices, the class-normalized accuracy (average over per-class accuracies) over all trials, i.e., <span class="math notranslate nohighlight">\(\mathrm{acc}_\mathrm{norm}\)</span>, and the mean of the single recordingsâ€™ normalized accuracy, i.e.,  <span class="math notranslate nohighlight">\(\mathrm{mean}(\mathrm{acc}_\mathrm{norm})\)</span> is displayed; please note that these two measures differ slightly, as the patients had a varying number of total trials and trials per class. B) Box plots for specificity, precision and F1 score. The box represents the interquartile range (IQR) of the data, the circle within the mean, the horizontal line depicts the median. The lower whiskers include all data points that have the minimal value of <span class="math notranslate nohighlight">\(25^\mathrm{th} \mathrm{percentile}-1.5 \cdot \mathrm{IQR}\)</span>, the upper whiskers include all points that are maximally <span class="math notranslate nohighlight">\(75^\mathrm{th} \mathrm{percentile}+1.5 \cdot \mathrm{IQR}\)</span>. Figure from <span id="id17">Volker <em>et al.</em> [<a class="reference internal" href="References.html#id41" title="Martin Volker, Jiri Hammer, Robin T Schirrmeister, Joos Behncke, Lukas DJ Fiederer, Andreas Schulze-Bonhage, Petr Marusic, Wolfram Burgard, and Tonio Ball. Intracranial error detection via deep learning. In 2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC), 568â€“575. IEEE, 2018.">2018</a>]</span>.</span><a class="headerlink" href="#intracranial-error-results-fig" title="Permalink to this image">#</a></p>
</div>
<p>We further evaluated whether the same networks developed for noninvasive EEG decoding can successfully learn to decode intracranial EEG. Therefore, in one work we used the same Eriksen flanker task as described in <a class="reference internal" href="#flanker-and-gui-section"><span class="std std-ref">Decoding of Eriksen Flanker Task Errors and Errors during Online GUI Control</span></a>, but recorded intracranial EEG from 23 patients who had pharmacoresistant epilepsy <span id="id18">[<a class="reference internal" href="References.html#id41" title="Martin Volker, Jiri Hammer, Robin T Schirrmeister, Joos Behncke, Lukas DJ Fiederer, Andreas Schulze-Bonhage, Petr Marusic, Wolfram Burgard, and Tonio Ball. Intracranial error detection via deep learning. In 2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC), 568â€“575. IEEE, 2018.">Volker <em>et al.</em>, 2018</a>]</span>.  Results for single-channel decoding <a class="reference internal" href="#intracranial-error-results-table"><span class="std std-numref">Table 14</span></a> show the deep and shallow ConvNet to clearly outperform rLDA (59.3%/58.4% vs. 53.8%) , whereas the residual ConvNet has low accuracy (52.5%). In contrast, results for all-channel decoding <a class="reference internal" href="#intracranial-error-results-fig"><span class="std std-numref">Fig. 34</span></a> show the residual ConvNet to perform well with the residual ConvNet and the deep ConvNet outperforming the shallow ConvNet (72.1% and 73.7% vs. 60.3% class-normalized accuracies (average over per-class accuracies)).</p>
</div>
<div class="section" id="transfer-learning-for-intracranial-error-decoding">
<h3>Transfer Learning for Intracranial Error Decoding<a class="headerlink" href="#transfer-learning-for-intracranial-error-decoding" title="Permalink to this heading">#</a></h3>
<div class="figure align-default" id="eriksen-flanker-car-driving-tasks-fig">
<img alt="_images/eriksen-flanker-car-driving-tasks.png" src="_images/eriksen-flanker-car-driving-tasks.png" />
<p class="caption"><span class="caption-number">Fig. 35 </span><span class="caption-text">Sketch of the Eriksen flanker task (A) and screenshot of the car driving task (B). Figure from <span id="id19">Behncke <em>et al.</em> [<a class="reference internal" href="References.html#id40" title="Joos Behncke, Robin Tibor Schirrmeister, Martin Volker, Jiri Hammer, Petr Marusic, Andreas Schulze-Bonhage, Wolfram Burgard, and Tonio Ball. Cross-paradigm pretraining of convolutional networks improves intracranial eeg decoding. In 2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC), 1046â€“1053. IEEE, 2018.">2018</a>]</span>.</span><a class="headerlink" href="#eriksen-flanker-car-driving-tasks-fig" title="Permalink to this image">#</a></p>
</div>
<div class="figure align-default" id="cross-training-eft-cdt-results-fig">
<img alt="_images/cross-training-eft-cdt-results.png" src="_images/cross-training-eft-cdt-results.png" />
<p class="caption"><span class="caption-number">Fig. 36 </span><span class="caption-text">Results for transfer learning on the Eriksen flanker task (EFT) and the car driving task (CDT) <span id="id20">[<a class="reference internal" href="References.html#id40" title="Joos Behncke, Robin Tibor Schirrmeister, Martin Volker, Jiri Hammer, Petr Marusic, Andreas Schulze-Bonhage, Wolfram Burgard, and Tonio Ball. Cross-paradigm pretraining of convolutional networks improves intracranial eeg decoding. In 2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC), 1046â€“1053. IEEE, 2018.">Behncke <em>et al.</em>, 2018</a>]</span>. All results are computed for a varying fraction of available data for the target decoding task (bottom row). <strong>A</strong> compares CDT accuracies after training only on CDT or pretraining on EFT and  finetuning on CDT. <strong>B</strong> compares EFT accuracies after only training on EFT or after  pretraining on CDT and finetuning on EFT. As a sanity check for the results in <strong>B</strong>, <strong>C</strong> compares EFT accuracies when pretraining on original CDT data and finetuning on EFT to pretraining on CDT data with shuffled labels (CDT*) and finetuning on EFT. Results show that pretraining on CDT helps EFT decoding when little EFT data is available. Figure from <span id="id21">Behncke <em>et al.</em> [<a class="reference internal" href="References.html#id40" title="Joos Behncke, Robin Tibor Schirrmeister, Martin Volker, Jiri Hammer, Petr Marusic, Andreas Schulze-Bonhage, Wolfram Burgard, and Tonio Ball. Cross-paradigm pretraining of convolutional networks improves intracranial eeg decoding. In 2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC), 1046â€“1053. IEEE, 2018.">2018</a>]</span>.</span><a class="headerlink" href="#cross-training-eft-cdt-results-fig" title="Permalink to this image">#</a></p>
</div>
<p>We further tested the potential of ConvNets to transfer knowledge learned from decoding intracranial signals in error-decoding paradigm to decoding signals in another a different error-decoding paradigm <span id="id22">[<a class="reference internal" href="References.html#id40" title="Joos Behncke, Robin Tibor Schirrmeister, Martin Volker, Jiri Hammer, Petr Marusic, Andreas Schulze-Bonhage, Wolfram Burgard, and Tonio Ball. Cross-paradigm pretraining of convolutional networks improves intracranial eeg decoding. In 2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC), 1046â€“1053. IEEE, 2018.">Behncke <em>et al.</em>, 2018</a>]</span>. The two error-decoding paradigms were the aforementioned Eriksen flanker task (EFT) and a car driving task (CDT), where subjects had to use a steering wheel to steer a car in a computer game and avoid hitting obstacles, where hitting an obstacle was considered an error event (see <a class="reference internal" href="#eriksen-flanker-car-driving-tasks-fig"><span class="std std-numref">Fig. 35</span></a>). Results in <a class="reference internal" href="#cross-training-eft-cdt-results-fig"><span class="std std-numref">Fig. 36</span></a> show that pretraining on CDT helps EFT decoding when few EDT data is available.</p>
</div>
<div class="section" id="microelectrocorticography-decoding-of-auditory-evoked-responses-in-sheep">
<h3>Microelectrocorticography decoding of auditory evoked responses in sheep<a class="headerlink" href="#microelectrocorticography-decoding-of-auditory-evoked-responses-in-sheep" title="Permalink to this heading">#</a></h3>
<div class="figure align-default" id="sheep-sounds-fig">
<img alt="_images/sheep-sounds.jpg" src="_images/sheep-sounds.jpg" />
<p class="caption"><span class="caption-number">Fig. 37 </span><span class="caption-text">Overview over decoding tasks for auditory evoked responses in a sheep <span id="id23">[<a class="reference internal" href="References.html#id39" title="X. Wang, C. A. Gkogkidis, R. T. Schirrmeister, F. A. Heilmeyer, M. Gierthmuehlen, F. Kohler, M. Schuettler, T. Stieglitz, and T. Ball. Deep learning for micro-electrocorticographic (Âµecog) data. In 2018 IEEE-EMBS Conference on Biomedical Engineering and Sciences (IECBES), volume, 63-68. 2018. doi:10.1109/IECBES.2018.8626607.">Wang <em>et al.</em>, 2018</a>]</span>. First task (top) was to distingish 3 seconds when the sound was playing from the second before and the second after. Second task (bottom) was to distinguish the first, second and third second during theplaying of the sound. Signals are averaged responses from one electrode during different days, with black and grey being signals while the sheep was awake and red ones while the sheep was under general anesthesia. Figure from <span id="id24">Wang <em>et al.</em> [<a class="reference internal" href="References.html#id39" title="X. Wang, C. A. Gkogkidis, R. T. Schirrmeister, F. A. Heilmeyer, M. Gierthmuehlen, F. Kohler, M. Schuettler, T. Stieglitz, and T. Ball. Deep learning for micro-electrocorticographic (Âµecog) data. In 2018 IEEE-EMBS Conference on Biomedical Engineering and Sciences (IECBES), volume, 63-68. 2018. doi:10.1109/IECBES.2018.8626607.">2018</a>]</span>.</span><a class="headerlink" href="#sheep-sounds-fig" title="Permalink to this image">#</a></p>
</div>
<div class="figure align-default" id="sheep-accuracies-fig">
<img alt="_images/sheep-accuracies.png" src="_images/sheep-accuracies.png" />
<p class="caption"><span class="caption-number">Fig. 38 </span><span class="caption-text">Results of decoding auditory evoked responses from sheep with rlDA and FBSCP or the deep ConvNet. Open circles represent accuracies for individual experiment days and closed circles represent the average over these accuracies. Figure from <span id="id25">Wang <em>et al.</em> [<a class="reference internal" href="References.html#id39" title="X. Wang, C. A. Gkogkidis, R. T. Schirrmeister, F. A. Heilmeyer, M. Gierthmuehlen, F. Kohler, M. Schuettler, T. Stieglitz, and T. Ball. Deep learning for micro-electrocorticographic (Âµecog) data. In 2018 IEEE-EMBS Conference on Biomedical Engineering and Sciences (IECBES), volume, 63-68. 2018. doi:10.1109/IECBES.2018.8626607.">2018</a>]</span>.</span><a class="headerlink" href="#sheep-accuracies-fig" title="Permalink to this image">#</a></p>
</div>
<p>In this study, we evaluated the ConvNets for decoding auditory evoked responses played to a sheep that was chronically implanted with  a Î¼ECoG-based neural interfacing device <span id="id26">[<a class="reference internal" href="References.html#id39" title="X. Wang, C. A. Gkogkidis, R. T. Schirrmeister, F. A. Heilmeyer, M. Gierthmuehlen, F. Kohler, M. Schuettler, T. Stieglitz, and T. Ball. Deep learning for micro-electrocorticographic (Âµecog) data. In 2018 IEEE-EMBS Conference on Biomedical Engineering and Sciences (IECBES), volume, 63-68. 2018. doi:10.1109/IECBES.2018.8626607.">Wang <em>et al.</em>, 2018</a>]</span>. 3-seconds-long sounds were presented to the sheep and two decoding tasks were defined from those 3 seconds as well as the second immediately before and after the playing of the sound. The first decoding task was to distinguish the 3 seconds when the sound was playing from the second  immediately before and the second immediately after the sound. The second task was distinguishing the first, second and third second of the playing of the sound to discriminate early, intermediate and late auditory evoked response (see <a class="reference internal" href="#sheep-sounds-fig"><span class="std std-numref">Fig. 37</span></a>). Results in <a class="reference internal" href="#sheep-accuracies-fig"><span class="std std-numref">Fig. 38</span></a> show that the  deep ConvNet can perform as good as FBSCP and rLDA, and perform well on both tasks, whereas rLDA performs competitively only on the first and FBSCP only on the second task.</p>
</div>
</div>
<div class="section" id="evaluation-on-large-scale-task-diverse-dataset">
<h2>Evaluation on large-scale task-diverse dataset<a class="headerlink" href="#evaluation-on-large-scale-task-diverse-dataset" title="Permalink to this heading">#</a></h2>
<table class="colwidths-auto table" id="large-framework-overview-table">
<caption><span class="caption-number">Table 15 </span><span class="caption-text">Datasets for the large-scale evaluation framework.</span><a class="headerlink" href="#large-framework-overview-table" title="Permalink to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Name (Acronym)</p></th>
<th class="head"><p>#Classes</p></th>
<th class="head"><p>Task Type</p></th>
<th class="head"><p>#Subjects</p></th>
<th class="head"><p>Trials per Subject</p></th>
<th class="head"><p>Class balance</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>High-Gamma Dataset (Motor)</p></td>
<td><p>4</p></td>
<td><p>Motor task</p></td>
<td><p>20</p></td>
<td><p>1000</p></td>
<td><p>balanced</p></td>
</tr>
<tr class="row-odd"><td><p>KUKA Pouring Observation (KPO)</p></td>
<td><p>2</p></td>
<td><p>Error observation</p></td>
<td><p>5</p></td>
<td><p>720-800</p></td>
<td><p>balanced</p></td>
</tr>
<tr class="row-even"><td><p>Robot-Grasping Observation (RGO)</p></td>
<td><p>2</p></td>
<td><p>Error observation</p></td>
<td><p>12</p></td>
<td><p>720-800</p></td>
<td><p>balanced</p></td>
</tr>
<tr class="row-odd"><td><p>Error-Related Negativity (ERN)</p></td>
<td><p>2</p></td>
<td><p>Eriksen flanker task</p></td>
<td><p>31</p></td>
<td><p>1000</p></td>
<td><p>1/2 up to 1/15</p></td>
</tr>
<tr class="row-even"><td><p>Semantic Categories</p></td>
<td><p>3</p></td>
<td><p>Speech imagery</p></td>
<td><p>16</p></td>
<td><p>750</p></td>
<td><p>balanced</p></td>
</tr>
<tr class="row-odd"><td><p>Real vs. Pseudo Words</p></td>
<td><p>2</p></td>
<td><p>Speech imagery</p></td>
<td><p>16</p></td>
<td><p>1000</p></td>
<td><p>3/1</p></td>
</tr>
<tr class="row-even"><td><p><span id="id27">[<a class="reference internal" href="References.html#id38" title="Felix A Heilmeyer, Robin T Schirrmeister, Lukas DJ Fiederer, Martin Volker, Joos Behncke, and Tonio Ball. A large-scale evaluation framework for eeg deep learning architectures. In 2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC), 1039â€“1045. IEEE, 2018.">Heilmeyer <em>et al.</em>, 2018</a>]</span></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<div class="figure align-default" id="large-framework-per-dataset-results-fig">
<img alt="_images/large-framework-per-dataset-results.png" src="_images/large-framework-per-dataset-results.png" />
<p class="caption"><span class="caption-number">Fig. 39 </span><span class="caption-text">Per-dataset results for the large-scale evaluation of deep ConvNet, shallow ConvNet and two versions of EEGNet. Boxplots show the distribution over per-subject accuracies for the individual decoding tasks. ern, kpo and rgo are the error-related datasets, ern: Error-related negativity Eriksen flanker task, KPO: KUKA Pouring Observation paradigm, rgo: robot-grasping observation paradigm. motor is the high-gamma dataset with 6 additional subjects that were excluded for data quality reasons from <span id="id28">[<a class="reference internal" href="References.html#id37" title="Robin Tibor Schirrmeister, Jost Tobias Springenberg, Lukas Dominique Josef Fiederer, Martin Glasstetter, Katharina Eggensperger, Michael Tangermann, Frank Hutter, Wolfram Burgard, and Tonio Ball. Deep learning with convolutional neural networks for eeg decoding and visualization. Human Brain Mapping, aug 2017. URL: http://dx.doi.org/10.1002/hbm.23730, doi:10.1002/hbm.23730.">Schirrmeister <em>et al.</em>, 2017</a>]</span>. pseudovsreal and semantic are two semantic processing datasets to classify silent repetitions of  pseudowords vs. realwords (pseudovsreal) or different semantic categories (semantic) . Figure from <span id="id29">Heilmeyer <em>et al.</em> [<a class="reference internal" href="References.html#id38" title="Felix A Heilmeyer, Robin T Schirrmeister, Lukas DJ Fiederer, Martin Volker, Joos Behncke, and Tonio Ball. A large-scale evaluation framework for eeg deep learning architectures. In 2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC), 1039â€“1045. IEEE, 2018.">2018</a>]</span>.</span><a class="headerlink" href="#large-framework-per-dataset-results-fig" title="Permalink to this image">#</a></p>
</div>
<div class="figure align-default" id="large-framework-averaged-results-fig">
<img alt="_images/large-framework-averaged-results.png" src="_images/large-framework-averaged-results.png" />
<p class="caption"><span class="caption-number">Fig. 40 </span><span class="caption-text">Dataset-averaged results for the large-scale evaluation of deep ConvNet, shallow ConvNet and two versions of EEGNet. Accuracies are normalized to the average of the accuracies of all models. Figure from <span id="id30">Heilmeyer <em>et al.</em> [<a class="reference internal" href="References.html#id38" title="Felix A Heilmeyer, Robin T Schirrmeister, Lukas DJ Fiederer, Martin Volker, Joos Behncke, and Tonio Ball. A large-scale evaluation framework for eeg deep learning architectures. In 2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC), 1039â€“1045. IEEE, 2018.">2018</a>]</span>.</span><a class="headerlink" href="#large-framework-averaged-results-fig" title="Permalink to this image">#</a></p>
</div>
<table class="colwidths-auto table" id="large-framework-results-table">
<caption><span class="caption-number">Table 16 </span><span class="caption-text">Dataset-averaged results for the large-scale evaluation of deep ConvNet, shallow ConvNet and two versions of EEGNet. Accuracies are normalized to the average of the accuracies of all models.</span><a class="headerlink" href="#large-framework-results-table" title="Permalink to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>Mean accuracy</p></th>
<th class="head"><p>Mean normalized accuracy</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Deep ConvNet</p></td>
<td><p>70.08% Â± 20.92%</p></td>
<td><p>1.00 Â± 0.05</p></td>
</tr>
<tr class="row-odd"><td><p>EEGNetv2</p></td>
<td><p>70.00% Â±18.86%</p></td>
<td><p>1.02 Â± 0.08</p></td>
</tr>
<tr class="row-even"><td><p>EEGNet</p></td>
<td><p>67.71% Â± 19.04%</p></td>
<td><p>0.98 Â± 0.06</p></td>
</tr>
<tr class="row-odd"><td><p>Shallow ConvNet</p></td>
<td><p>67.71% Â±19.04%</p></td>
<td><p>0.99 Â± 0.06</p></td>
</tr>
<tr class="row-even"><td><p><span id="id31">[<a class="reference internal" href="References.html#id38" title="Felix A Heilmeyer, Robin T Schirrmeister, Lukas DJ Fiederer, Martin Volker, Joos Behncke, and Tonio Ball. A large-scale evaluation framework for eeg deep learning architectures. In 2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC), 1039â€“1045. IEEE, 2018.">Heilmeyer <em>et al.</em>, 2018</a>]</span></p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<p>We also compared the deep and shallow ConvNet architectures as well as EEGNet on six classification tasks with more than 90000 trials in total (see <a class="reference internal" href="#large-framework-overview-table"><span class="std std-numref">Table 15</span></a>) <span id="id32">[<a class="reference internal" href="References.html#id38" title="Felix A Heilmeyer, Robin T Schirrmeister, Lukas DJ Fiederer, Martin Volker, Joos Behncke, and Tonio Ball. A large-scale evaluation framework for eeg deep learning architectures. In 2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC), 1039â€“1045. IEEE, 2018.">Heilmeyer <em>et al.</em>, 2018</a>]</span>. The datasets tasks were all recorded in our lab and included the high-gamma dataset, three error-related tasks described before (Eriksen flanker task, robot grasping and robot pouring observations) as well as two tasks on semantic processing. In the semantic processing dataset, the classification tasks were to distinguish different types of words that a subject silently repeated <span id="id33">[<a class="reference internal" href="References.html#id36" title="V. Rau. Eeg correlates of inner speech. Bachelor's Thesis, University of Freiburg, DOI, 2015.">Rau, 2015</a>]</span>. The first task was to distinguish existing real words from nonexisting pseudowords. The second classification task was to distingiush three semantic categories (food, animals, tools) the word may belong to. The evaluation code for all models always used the original code and hyperparameters from the original studies in order to ensure a fair comparison. Results show that the deep ConvNet and the more recent version of EEGNet (EEGNetv2) perform similarly well, with shallow and an older version of EEGNet performing slightly worse, see  <a class="reference internal" href="#large-framework-per-dataset-results-fig"><span class="std std-numref">Fig. 39</span></a>, <a class="reference internal" href="#large-framework-averaged-results-fig"><span class="std std-numref">Fig. 40</span></a>  and <a class="reference internal" href="#large-framework-results-table"><span class="std std-numref">Table 16</span></a>.</p>
<div class="tip admonition">
<p class="admonition-title">Open Questions</p>
<ul class="simple">
<li><p>How do these networks perform on non-trial-based tasks like pathology decoding?</p></li>
</ul>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="MovementDecoding.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Decoding Movement-Related Brain Activity</p>
      </div>
    </a>
    <a class="right-next"
       href="Pathology.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Decoding Pathology</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#decoding-different-mental-imageries">Decoding different mental imageries</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#decoding-error-related-signals">Decoding error-related signals</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#decoding-observation-of-robots-making-errors">Decoding Observation of Robots Making Errors</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#decoding-of-eriksen-flanker-task-errors-and-errors-during-online-gui-control">Decoding of Eriksen Flanker Task Errors and Errors during Online GUI Control</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#proof-of-concept-assistive-system">Proof-of-concept assistive system</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#intracranial-eeg-decoding">Intracranial EEG decoding</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#intracranial-eeg-decoding-of-eriksen-flanker-task">Intracranial EEG Decoding of Eriksen Flanker Task</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#transfer-learning-for-intracranial-error-decoding">Transfer Learning for Intracranial Error Decoding</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#microelectrocorticography-decoding-of-auditory-evoked-responses-in-sheep">Microelectrocorticography decoding of auditory evoked responses in sheep</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation-on-large-scale-task-diverse-dataset">Evaluation on large-scale task-diverse dataset</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By The Jupyter Book community
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      Â© Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>